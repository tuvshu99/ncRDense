{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import  Dense, Flatten, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, merge, Input,Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import  Bidirectional\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Test_Data_750.h5',\n",
       " '/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Train_Data_750.h5']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8 = glob.glob(\"/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot*.h5\")\n",
    "my_files8.sort()\n",
    "my_files8[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_test_0.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_test_1.h5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files7 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/*.h5\")\n",
    "my_files7.sort()\n",
    "my_files7[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_train_0.h5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files7[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file8(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files8[fold_no*2+1],'r')\n",
    "    hf_Test = h5.File(my_files8[fold_no*2],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file7(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files7[10+fold_no],'r')\n",
    "    hf_Test = h5.File(my_files7[fold_no],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5S_rRNA',\n",
       " '5.8S_rRNA',\n",
       " 'tRNA',\n",
       " 'ribozymes',\n",
       " 'CD-box',\n",
       " 'miRNA',\n",
       " 'Intron_gpI',\n",
       " 'Intron_gpII',\n",
       " 'HACA-box',\n",
       " 'riboswitch',\n",
       " 'IRES',\n",
       " 'leader',\n",
       " 'scaRNA']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_names = [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]\n",
    "class_names = ['5S_rRNA', '5.8S_rRNA', 'tRNA', 'ribozymes', 'CD-box', 'miRNA', 'Intron_gpI', 'Intron_gpII', 'HACA-box', 'riboswitch', 'IRES', 'leader', 'scaRNA']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 10\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\\n============================\")\n",
    "    else:\n",
    "        #cm = np.asfarray(cm,float64)\n",
    "        print('Confusion matrix, without normalization\\n============================')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(5*2, 4*2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=FONT_SIZE)\n",
    "    plt.yticks(tick_marks, classes, fontsize=FONT_SIZE)\n",
    "    fmt = '.3f' if normalize else '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    fontsize=FONT_SIZE,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=FONT_SIZE)\n",
    "    plt.xlabel('Predicted label', fontsize=FONT_SIZE)\n",
    "    plt.savefig('Conf_mat_avg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dme(f, k, xo):\n",
    "    x1o=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (xo)\n",
    "    x1o = BatchNormalization()(x1o)\n",
    "    x1o=Dropout(0.2)(x1o)\n",
    "    x1o=Activation('relu')(x1o)\n",
    "    #x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC1Dme(k, xo):\n",
    "    xf=Dense(k,)(xo)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_1(xin, f, k):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_file8(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, Y_train1, X_test1, Y_test1 = get_file7(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(X_train[0], X_train1[0,:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5688, 750, 7), (632, 750, 7))\n"
     ]
    }
   ],
   "source": [
    "print(X_train1.shape,X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense2(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(512, 20, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 6, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "            \n",
    "    xf=Flatten()(x1)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dense2()\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 1.6795 - acc: 0.4789 - val_loss: 1.2692 - val_acc: 0.6472\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 6s 998us/step - loss: 1.1188 - acc: 0.6791 - val_loss: 1.0895 - val_acc: 0.7294\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.8792 - acc: 0.7664 - val_loss: 0.9722 - val_acc: 0.7326\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 6s 999us/step - loss: 0.7063 - acc: 0.8268 - val_loss: 0.8546 - val_acc: 0.8070\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.5864 - acc: 0.8652 - val_loss: 0.7720 - val_acc: 0.8022\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4954 - acc: 0.8936 - val_loss: 0.7385 - val_acc: 0.8133\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4103 - acc: 0.9235 - val_loss: 0.7049 - val_acc: 0.8149\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.3415 - acc: 0.9357 - val_loss: 0.7147 - val_acc: 0.8101\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2859 - acc: 0.9564 - val_loss: 0.6062 - val_acc: 0.8418\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2460 - acc: 0.9610 - val_loss: 0.6092 - val_acc: 0.8449\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1988 - acc: 0.9731 - val_loss: 0.5640 - val_acc: 0.8449\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1722 - acc: 0.9784 - val_loss: 0.5457 - val_acc: 0.8434\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1429 - acc: 0.9842 - val_loss: 0.5554 - val_acc: 0.8465\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1217 - acc: 0.9873 - val_loss: 0.5220 - val_acc: 0.8497\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1150 - acc: 0.9880 - val_loss: 0.4875 - val_acc: 0.8608\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0889 - acc: 0.9933 - val_loss: 0.4809 - val_acc: 0.8671\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0817 - acc: 0.9928 - val_loss: 0.4719 - val_acc: 0.8703\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0751 - acc: 0.9933 - val_loss: 0.5274 - val_acc: 0.8576\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0695 - acc: 0.9938 - val_loss: 0.5475 - val_acc: 0.8497\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0571 - acc: 0.9963 - val_loss: 0.4737 - val_acc: 0.8544\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0532 - acc: 0.9968 - val_loss: 0.5049 - val_acc: 0.8481\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0487 - acc: 0.9961 - val_loss: 0.5236 - val_acc: 0.8513\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0449 - acc: 0.9972 - val_loss: 0.4852 - val_acc: 0.8655\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0380 - acc: 0.9974 - val_loss: 0.4990 - val_acc: 0.8528\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0378 - acc: 0.9968 - val_loss: 0.5406 - val_acc: 0.8513\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0404 - acc: 0.9958 - val_loss: 0.5294 - val_acc: 0.8497\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0376 - acc: 0.9951 - val_loss: 0.4786 - val_acc: 0.8560\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0344 - acc: 0.9967 - val_loss: 0.5059 - val_acc: 0.8513\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0322 - acc: 0.9961 - val_loss: 0.5561 - val_acc: 0.8402\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0267 - acc: 0.9977 - val_loss: 0.4741 - val_acc: 0.8703\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0254 - acc: 0.9982 - val_loss: 0.4694 - val_acc: 0.8639\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0256 - acc: 0.9968 - val_loss: 0.4868 - val_acc: 0.8671\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0240 - acc: 0.9972 - val_loss: 0.5162 - val_acc: 0.8528\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0218 - acc: 0.9979 - val_loss: 0.5274 - val_acc: 0.8481\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0220 - acc: 0.9968 - val_loss: 0.4929 - val_acc: 0.8528\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0222 - acc: 0.9974 - val_loss: 0.5266 - val_acc: 0.8576\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0201 - acc: 0.9974 - val_loss: 0.5311 - val_acc: 0.8513\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0233 - acc: 0.9965 - val_loss: 0.4991 - val_acc: 0.8576\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0175 - acc: 0.9982 - val_loss: 0.5561 - val_acc: 0.8513\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0173 - acc: 0.9979 - val_loss: 0.5038 - val_acc: 0.8544\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0163 - acc: 0.9981 - val_loss: 0.5374 - val_acc: 0.8576\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0158 - acc: 0.9982 - val_loss: 0.4848 - val_acc: 0.8544\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0166 - acc: 0.9977 - val_loss: 0.4732 - val_acc: 0.8639\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0177 - acc: 0.9972 - val_loss: 0.4934 - val_acc: 0.8623\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0143 - acc: 0.9986 - val_loss: 0.5310 - val_acc: 0.8592\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0133 - acc: 0.9982 - val_loss: 0.4876 - val_acc: 0.8671\n",
      "0.8639240506329114\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train1[:,:,0:4],X_train1[:,:,4:7]], Y_train, validation_data=([X_test1[:,:,0:4],X_test1[:,:,4:7]], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test1[:,:,0:4],X_test1[:,:,4:7]])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense2(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(512, 20, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 6, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x2 = Conv1Dme(512, 20, inputs2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = Conv1Dme(128, 6, x2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2],axis=-1)\n",
    "    \n",
    "    xf = Conv1Dme(128, 6, xf)\n",
    "    xf=MaxPooling1D(pool_size=2, strides=2)(xf)\n",
    "    \n",
    "    #x1 = dense_block_1(x1, 128, 4)\n",
    "    #x1 = dense_block_1(x1, 256, 5)\n",
    "    #x1 = dense_block_1(x1, 256, 5)\n",
    "            \n",
    "    xf=Flatten()(xf)\n",
    "    #xf2=Flatten()(x2)\n",
    "    \n",
    "    \n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 750, 3)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 750, 512)     41472       input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 750, 512)     31232       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 750, 512)     2048        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 750, 512)     2048        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 750, 512)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 750, 512)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 750, 512)     0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 750, 512)     0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 375, 512)     0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 375, 512)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 375, 128)     393344      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 375, 128)     393344      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 375, 128)     512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 375, 128)     512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 375, 128)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 375, 128)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 375, 128)     0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 375, 128)     0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 187, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 187, 128)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 187, 256)     0           max_pooling1d_30[0][0]           \n",
      "                                                                 max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 187, 128)     196736      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 187, 128)     512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 187, 128)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 187, 128)     0           dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 93, 128)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 11904)        0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 256)          3047680     flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 256)          1024        dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 256)          0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 256)          0           dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 64)           256         dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 64)           0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 64)           0           dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 13)           845         activation_55[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,128,013\n",
      "Trainable params: 4,124,557\n",
      "Non-trainable params: 3,456\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 1.8500 - acc: 0.4156 - val_loss: 1.5485 - val_acc: 0.5949\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 1.3169 - acc: 0.6199 - val_loss: 1.3417 - val_acc: 0.6503\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 1.0822 - acc: 0.6930 - val_loss: 1.1181 - val_acc: 0.7231\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.9171 - acc: 0.7514 - val_loss: 1.0664 - val_acc: 0.7453\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.8055 - acc: 0.7827 - val_loss: 0.9450 - val_acc: 0.7785\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.7162 - acc: 0.8140 - val_loss: 0.8981 - val_acc: 0.7848\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.6194 - acc: 0.8397 - val_loss: 0.7604 - val_acc: 0.8165\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.5505 - acc: 0.8634 - val_loss: 0.7727 - val_acc: 0.7943\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.4889 - acc: 0.8855 - val_loss: 0.6997 - val_acc: 0.8212\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.4314 - acc: 0.9024 - val_loss: 0.6302 - val_acc: 0.8275\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.3742 - acc: 0.9160 - val_loss: 0.5820 - val_acc: 0.8497\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.3230 - acc: 0.9297 - val_loss: 0.6224 - val_acc: 0.8339\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.2945 - acc: 0.9386 - val_loss: 0.6528 - val_acc: 0.8196\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.2533 - acc: 0.9497 - val_loss: 0.5244 - val_acc: 0.8465\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.2250 - acc: 0.9566 - val_loss: 0.5993 - val_acc: 0.8180\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.1998 - acc: 0.9608 - val_loss: 0.4535 - val_acc: 0.8671\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.1850 - acc: 0.9647 - val_loss: 0.4788 - val_acc: 0.8544\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.1543 - acc: 0.9752 - val_loss: 0.4280 - val_acc: 0.8750\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.1513 - acc: 0.9736 - val_loss: 0.4058 - val_acc: 0.8861\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.1238 - acc: 0.9791 - val_loss: 0.4919 - val_acc: 0.8560\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.1070 - acc: 0.9852 - val_loss: 0.3897 - val_acc: 0.8861\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.1029 - acc: 0.9828 - val_loss: 0.4146 - val_acc: 0.8782\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0916 - acc: 0.9858 - val_loss: 0.4295 - val_acc: 0.8703\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0855 - acc: 0.9852 - val_loss: 0.4420 - val_acc: 0.8671\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0777 - acc: 0.9884 - val_loss: 0.3712 - val_acc: 0.8845\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0751 - acc: 0.9887 - val_loss: 0.4087 - val_acc: 0.8892\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0664 - acc: 0.9905 - val_loss: 0.4291 - val_acc: 0.8813\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0585 - acc: 0.9905 - val_loss: 0.4360 - val_acc: 0.8718\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0557 - acc: 0.9921 - val_loss: 0.3725 - val_acc: 0.9003\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0563 - acc: 0.9912 - val_loss: 0.3951 - val_acc: 0.8877\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0472 - acc: 0.9926 - val_loss: 0.4213 - val_acc: 0.8797\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0446 - acc: 0.9930 - val_loss: 0.3773 - val_acc: 0.8924\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0379 - acc: 0.9944 - val_loss: 0.4284 - val_acc: 0.8766\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0394 - acc: 0.9944 - val_loss: 0.3633 - val_acc: 0.8956\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0343 - acc: 0.9954 - val_loss: 0.3252 - val_acc: 0.9146\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0362 - acc: 0.9961 - val_loss: 0.4490 - val_acc: 0.8845\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0324 - acc: 0.9951 - val_loss: 0.4277 - val_acc: 0.8861\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0304 - acc: 0.9954 - val_loss: 0.4069 - val_acc: 0.8908\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0311 - acc: 0.9967 - val_loss: 0.3631 - val_acc: 0.9035\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0300 - acc: 0.9944 - val_loss: 0.4305 - val_acc: 0.8877\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0334 - acc: 0.9930 - val_loss: 0.4507 - val_acc: 0.8655\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0345 - acc: 0.9923 - val_loss: 0.3790 - val_acc: 0.9066\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0226 - acc: 0.9967 - val_loss: 0.3373 - val_acc: 0.9130\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0273 - acc: 0.9947 - val_loss: 0.4010 - val_acc: 0.8861\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 9s 2ms/step - loss: 0.0202 - acc: 0.9974 - val_loss: 0.3685 - val_acc: 0.9082\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0249 - acc: 0.9949 - val_loss: 0.3871 - val_acc: 0.8861\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.3434 - val_acc: 0.9035\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0194 - acc: 0.9967 - val_loss: 0.3715 - val_acc: 0.8908\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0195 - acc: 0.9975 - val_loss: 0.3787 - val_acc: 0.9035\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 0.0198 - acc: 0.9975 - val_loss: 0.4709 - val_acc: 0.8687\n",
      "0.9145569620253164\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train1[:,:,0:4],X_train1[:,:,4:7]], Y_train, validation_data=([X_test1[:,:,0:4],X_test1[:,:,4:7]], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test1[:,:,0:4],X_test1[:,:,4:7]])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense2(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    #inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    #x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(512, 20, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 6, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    #x1 = dense_block_1(x1, 128, 4)\n",
    "    #x1 = dense_block_1(x1, 256, 5)\n",
    "    #x1 = dense_block_1(x1, 256, 5)\n",
    "            \n",
    "    xf=Flatten()(x1)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=inputs1, outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.7604 - acc: 0.4436 - val_loss: 1.3278 - val_acc: 0.6123\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 1.1767 - acc: 0.6660 - val_loss: 1.1626 - val_acc: 0.6741\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 6s 971us/step - loss: 0.9457 - acc: 0.7423 - val_loss: 0.9554 - val_acc: 0.7421\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 6s 982us/step - loss: 0.7615 - acc: 0.8129 - val_loss: 0.8281 - val_acc: 0.7769\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 6s 980us/step - loss: 0.6310 - acc: 0.8565 - val_loss: 0.7767 - val_acc: 0.7991\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.5230 - acc: 0.8845 - val_loss: 0.7963 - val_acc: 0.8085\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 6s 982us/step - loss: 0.4410 - acc: 0.9102 - val_loss: 0.6627 - val_acc: 0.8354\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 6s 979us/step - loss: 0.3602 - acc: 0.9350 - val_loss: 0.6956 - val_acc: 0.8085\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 6s 975us/step - loss: 0.3027 - acc: 0.9476 - val_loss: 0.5889 - val_acc: 0.8434\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 6s 981us/step - loss: 0.2546 - acc: 0.9615 - val_loss: 0.5709 - val_acc: 0.8481\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 6s 977us/step - loss: 0.2150 - acc: 0.9698 - val_loss: 0.5583 - val_acc: 0.8465\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 6s 977us/step - loss: 0.1884 - acc: 0.9749 - val_loss: 0.5608 - val_acc: 0.8339\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 6s 982us/step - loss: 0.1546 - acc: 0.9812 - val_loss: 0.5980 - val_acc: 0.8196\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 6s 979us/step - loss: 0.1390 - acc: 0.9840 - val_loss: 0.5262 - val_acc: 0.8592\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 6s 981us/step - loss: 0.1190 - acc: 0.9854 - val_loss: 0.5100 - val_acc: 0.8528\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.1031 - acc: 0.9895 - val_loss: 0.5374 - val_acc: 0.8560\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 6s 981us/step - loss: 0.0918 - acc: 0.9895 - val_loss: 0.5386 - val_acc: 0.8354\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 6s 980us/step - loss: 0.0840 - acc: 0.9914 - val_loss: 0.5323 - val_acc: 0.8449\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 5s 967us/step - loss: 0.0740 - acc: 0.9917 - val_loss: 0.4830 - val_acc: 0.8623\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 6s 972us/step - loss: 0.0599 - acc: 0.9956 - val_loss: 0.4799 - val_acc: 0.8623\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 6s 984us/step - loss: 0.0530 - acc: 0.9953 - val_loss: 0.5417 - val_acc: 0.8449\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 6s 970us/step - loss: 0.0553 - acc: 0.9928 - val_loss: 0.4759 - val_acc: 0.8639\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 6s 983us/step - loss: 0.0485 - acc: 0.9963 - val_loss: 0.4986 - val_acc: 0.8528\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 6s 968us/step - loss: 0.0421 - acc: 0.9972 - val_loss: 0.5384 - val_acc: 0.8497\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 6s 974us/step - loss: 0.0387 - acc: 0.9975 - val_loss: 0.4702 - val_acc: 0.8718\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 6s 978us/step - loss: 0.0446 - acc: 0.9938 - val_loss: 0.6062 - val_acc: 0.8259\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 6s 981us/step - loss: 0.0394 - acc: 0.9949 - val_loss: 0.5563 - val_acc: 0.8434\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 6s 978us/step - loss: 0.0354 - acc: 0.9963 - val_loss: 0.5039 - val_acc: 0.8608\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 6s 980us/step - loss: 0.0314 - acc: 0.9960 - val_loss: 0.6127 - val_acc: 0.8418\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 6s 973us/step - loss: 0.0290 - acc: 0.9970 - val_loss: 0.5650 - val_acc: 0.8544\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 6s 983us/step - loss: 0.0282 - acc: 0.9965 - val_loss: 0.5251 - val_acc: 0.8576\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.0288 - acc: 0.9968 - val_loss: 0.5204 - val_acc: 0.8655\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 6s 969us/step - loss: 0.0265 - acc: 0.9963 - val_loss: 0.5464 - val_acc: 0.8592\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 5s 962us/step - loss: 0.0215 - acc: 0.9975 - val_loss: 0.5156 - val_acc: 0.8655\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.0226 - acc: 0.9972 - val_loss: 0.4654 - val_acc: 0.8766\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 6s 972us/step - loss: 0.0219 - acc: 0.9979 - val_loss: 0.5456 - val_acc: 0.8528\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 6s 975us/step - loss: 0.0243 - acc: 0.9961 - val_loss: 0.5310 - val_acc: 0.8592\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 6s 981us/step - loss: 0.0175 - acc: 0.9988 - val_loss: 0.5264 - val_acc: 0.8528\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 6s 977us/step - loss: 0.0169 - acc: 0.9977 - val_loss: 0.4916 - val_acc: 0.8718\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.0174 - acc: 0.9979 - val_loss: 0.5068 - val_acc: 0.8639\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 6s 978us/step - loss: 0.0173 - acc: 0.9975 - val_loss: 0.5717 - val_acc: 0.8528\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 6s 975us/step - loss: 0.0151 - acc: 0.9982 - val_loss: 0.5598 - val_acc: 0.8576\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 6s 970us/step - loss: 0.0158 - acc: 0.9984 - val_loss: 0.5221 - val_acc: 0.8434\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 5s 954us/step - loss: 0.0181 - acc: 0.9967 - val_loss: 0.5896 - val_acc: 0.8370\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 6s 976us/step - loss: 0.0142 - acc: 0.9986 - val_loss: 0.5463 - val_acc: 0.8655\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 6s 968us/step - loss: 0.0145 - acc: 0.9977 - val_loss: 0.5683 - val_acc: 0.8592\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 6s 975us/step - loss: 0.0131 - acc: 0.9982 - val_loss: 0.5468 - val_acc: 0.8655\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 6s 980us/step - loss: 0.0133 - acc: 0.9977 - val_loss: 0.5410 - val_acc: 0.8449\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 6s 971us/step - loss: 0.0110 - acc: 0.9991 - val_loss: 0.5743 - val_acc: 0.8465\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 6s 977us/step - loss: 0.0178 - acc: 0.9965 - val_loss: 0.6267 - val_acc: 0.8449\n",
      "0.8765822784810127\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "#print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(X_train1[:,:,0:4], Y_train, validation_data=(X_test1[:,:,0:4], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict(X_test1[:,:,0:4])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train = []\n",
    "for j in range(len(X_train1)):\n",
    "    newvecs = []\n",
    "    for i in range(len(X_train1[0,:,:])):\n",
    "        newvec1 = np.multiply(X_train1[j,i,0:4],X_train1[j,i,4])\n",
    "        newvec2 = np.multiply(X_train1[j,i,0:4],X_train1[j,i,5])\n",
    "        newvec3 = np.multiply(X_train1[j,i,0:4],X_train1[j,i,6])\n",
    "        newvec = np.concatenate((newvec1,newvec2,newvec3),axis=0)\n",
    "        newvecs.append(newvec)\n",
    "    newvecs = np.array(newvecs)\n",
    "    vec_train.append(newvecs)\n",
    "vec_train = np.array(vec_train)\n",
    "        #print(newvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test = []\n",
    "for j in range(len(X_test1)):\n",
    "    newvecs = []\n",
    "    for i in range(len(X_test1[0,:,:])):\n",
    "        newvec1 = np.multiply(X_test1[j,i,0:4],X_test1[j,i,4])\n",
    "        newvec2 = np.multiply(X_test1[j,i,0:4],X_test1[j,i,5])\n",
    "        newvec3 = np.multiply(X_test1[j,i,0:4],X_test1[j,i,6])\n",
    "        newvec = np.concatenate((newvec1,newvec2,newvec3),axis=0)\n",
    "        newvecs.append(newvec)\n",
    "    newvecs = np.array(newvecs)\n",
    "    vec_test.append(newvecs)\n",
    "vec_test = np.array(vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 0, 0, 0]), array([1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(X_train1[0,0,0:4],X_train1[0,0,4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newvecs = np.array(newvecs)\n",
    "newvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 750, 12)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vec1(): # Model\n",
    "    inputs1 = Input(shape=(750, 12))\n",
    "    \n",
    "    x1 = Conv1Dme(512, 20, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = Conv1Dme(256, 8, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "            \n",
    "    xf=Flatten()(x1)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=inputs1, outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 14s 2ms/step - loss: 1.8994 - acc: 0.3877 - val_loss: 1.5780 - val_acc: 0.5380\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 1.3782 - acc: 0.5803 - val_loss: 1.3518 - val_acc: 0.6361\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 1.1451 - acc: 0.6637 - val_loss: 1.2322 - val_acc: 0.6756\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.9814 - acc: 0.7222 - val_loss: 1.1736 - val_acc: 0.6693\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.8613 - acc: 0.7588 - val_loss: 1.0763 - val_acc: 0.6835\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.7652 - acc: 0.7952 - val_loss: 1.0019 - val_acc: 0.7231\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.6768 - acc: 0.8194 - val_loss: 0.9352 - val_acc: 0.7310\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.6116 - acc: 0.8453 - val_loss: 0.9931 - val_acc: 0.6883\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.5508 - acc: 0.8629 - val_loss: 0.9247 - val_acc: 0.7120\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.4816 - acc: 0.8866 - val_loss: 0.9540 - val_acc: 0.7073\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.4240 - acc: 0.9026 - val_loss: 0.7760 - val_acc: 0.7737\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.3628 - acc: 0.9197 - val_loss: 0.8190 - val_acc: 0.7532\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.3234 - acc: 0.9320 - val_loss: 0.7687 - val_acc: 0.7737\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.2846 - acc: 0.9400 - val_loss: 0.6751 - val_acc: 0.7991\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.2511 - acc: 0.9529 - val_loss: 0.8594 - val_acc: 0.7294\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.2256 - acc: 0.9559 - val_loss: 0.6623 - val_acc: 0.7991\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1943 - acc: 0.9654 - val_loss: 0.5860 - val_acc: 0.8275\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.1721 - acc: 0.9703 - val_loss: 0.5733 - val_acc: 0.8180\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.1623 - acc: 0.9715 - val_loss: 0.6138 - val_acc: 0.8212\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.1314 - acc: 0.9787 - val_loss: 0.9193 - val_acc: 0.7104\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.1212 - acc: 0.9805 - val_loss: 0.5661 - val_acc: 0.8291\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.1038 - acc: 0.9863 - val_loss: 0.5716 - val_acc: 0.8259\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0922 - acc: 0.9875 - val_loss: 0.5934 - val_acc: 0.8165\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0911 - acc: 0.9856 - val_loss: 0.5644 - val_acc: 0.8354\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0804 - acc: 0.9880 - val_loss: 0.5437 - val_acc: 0.8275\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0710 - acc: 0.9923 - val_loss: 0.5241 - val_acc: 0.8354\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0683 - acc: 0.9898 - val_loss: 0.5077 - val_acc: 0.8465\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0634 - acc: 0.9919 - val_loss: 0.5843 - val_acc: 0.8386\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0589 - acc: 0.9917 - val_loss: 0.5649 - val_acc: 0.8259\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0601 - acc: 0.9907 - val_loss: 0.5798 - val_acc: 0.8117\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0528 - acc: 0.9917 - val_loss: 0.6306 - val_acc: 0.8038\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0458 - acc: 0.9942 - val_loss: 0.6254 - val_acc: 0.8070\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0473 - acc: 0.9926 - val_loss: 0.5643 - val_acc: 0.8418\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0421 - acc: 0.9935 - val_loss: 0.5859 - val_acc: 0.8212\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0438 - acc: 0.9917 - val_loss: 0.7937 - val_acc: 0.7722\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0452 - acc: 0.9924 - val_loss: 0.6385 - val_acc: 0.8006\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0383 - acc: 0.9945 - val_loss: 0.6103 - val_acc: 0.8101\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0323 - acc: 0.9958 - val_loss: 0.5199 - val_acc: 0.8402\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0322 - acc: 0.9940 - val_loss: 0.5598 - val_acc: 0.8212\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 7s 1ms/step - loss: 0.0272 - acc: 0.9960 - val_loss: 0.5939 - val_acc: 0.8101\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0304 - acc: 0.9938 - val_loss: 0.5683 - val_acc: 0.8212\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0272 - acc: 0.9956 - val_loss: 0.7293 - val_acc: 0.7927\n",
      "0.8465189873417721\n"
     ]
    }
   ],
   "source": [
    "model = model_vec1()\n",
    "#print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(vec_train, Y_train, validation_data=(vec_test, Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict(vec_test)\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense2(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 4)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 3, inputs2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_1(x2, 128, 3)\n",
    "    x2 = dense_block_1(x2, 256, 4)\n",
    "    x2 = dense_block_1(x2, 256, 5)\n",
    "    #x2 = dense_block_1(x2, 128, 4)\n",
    "    #x2 = dense_block_1(x2, 256, 5)\n",
    "    #x2 = dense_block_1(x2, 256, 5)\n",
    "            \n",
    "    xf1=Flatten()(x1)\n",
    "    xf2=Flatten()(x2)\n",
    "    \n",
    "    xf=keras.layers.concatenate([xf1,xf2],axis=-1)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_54 (InputLayer)           (None, 750, 3)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_328 (Conv1D)             (None, 750, 64)      832         input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_341 (Conv1D)             (None, 750, 64)      640         input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 750, 64)      256         conv1d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 750, 64)      256         conv1d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_388 (Dropout)           (None, 750, 64)      0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_401 (Dropout)           (None, 750, 64)      0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 750, 64)      0           dropout_388[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 750, 64)      0           dropout_401[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_148 (MaxPooling1D (None, 375, 64)      0           activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_152 (MaxPooling1D (None, 375, 64)      0           activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_329 (Conv1D)             (None, 375, 128)     24704       max_pooling1d_148[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_342 (Conv1D)             (None, 375, 128)     24704       max_pooling1d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 375, 128)     512         conv1d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 375, 128)     512         conv1d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_389 (Dropout)           (None, 375, 128)     0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_402 (Dropout)           (None, 375, 128)     0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 375, 128)     0           dropout_389[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 375, 128)     0           dropout_402[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_330 (Conv1D)             (None, 375, 128)     49280       activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_343 (Conv1D)             (None, 375, 128)     49280       activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 375, 128)     512         conv1d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 375, 128)     512         conv1d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_390 (Dropout)           (None, 375, 128)     0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_403 (Dropout)           (None, 375, 128)     0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 375, 128)     0           dropout_390[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 375, 128)     0           dropout_403[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_141 (Concatenate)   (None, 375, 256)     0           activation_389[0][0]             \n",
      "                                                                 activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_147 (Concatenate)   (None, 375, 256)     0           activation_402[0][0]             \n",
      "                                                                 activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_331 (Conv1D)             (None, 375, 128)     98432       concatenate_141[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_344 (Conv1D)             (None, 375, 128)     98432       concatenate_147[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 375, 128)     512         conv1d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 375, 128)     512         conv1d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_391 (Dropout)           (None, 375, 128)     0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_404 (Dropout)           (None, 375, 128)     0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 375, 128)     0           dropout_391[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 375, 128)     0           dropout_404[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_142 (Concatenate)   (None, 375, 256)     0           activation_389[0][0]             \n",
      "                                                                 activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_148 (Concatenate)   (None, 375, 256)     0           activation_402[0][0]             \n",
      "                                                                 activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_332 (Conv1D)             (None, 375, 128)     98432       concatenate_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_345 (Conv1D)             (None, 375, 128)     98432       concatenate_148[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 375, 128)     512         conv1d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 375, 128)     512         conv1d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_392 (Dropout)           (None, 375, 128)     0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_405 (Dropout)           (None, 375, 128)     0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 375, 128)     0           dropout_392[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 375, 128)     0           dropout_405[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_149 (MaxPooling1D (None, 187, 128)     0           activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_153 (MaxPooling1D (None, 187, 128)     0           activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_333 (Conv1D)             (None, 187, 256)     131328      max_pooling1d_149[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_346 (Conv1D)             (None, 187, 256)     131328      max_pooling1d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 187, 256)     1024        conv1d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 187, 256)     1024        conv1d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_393 (Dropout)           (None, 187, 256)     0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_406 (Dropout)           (None, 187, 256)     0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 187, 256)     0           dropout_393[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 187, 256)     0           dropout_406[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_334 (Conv1D)             (None, 187, 256)     262400      activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_347 (Conv1D)             (None, 187, 256)     262400      activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 187, 256)     1024        conv1d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 187, 256)     1024        conv1d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_394 (Dropout)           (None, 187, 256)     0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_407 (Dropout)           (None, 187, 256)     0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 187, 256)     0           dropout_394[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 187, 256)     0           dropout_407[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_143 (Concatenate)   (None, 187, 512)     0           activation_393[0][0]             \n",
      "                                                                 activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_149 (Concatenate)   (None, 187, 512)     0           activation_406[0][0]             \n",
      "                                                                 activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_335 (Conv1D)             (None, 187, 256)     524544      concatenate_143[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_348 (Conv1D)             (None, 187, 256)     524544      concatenate_149[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 187, 256)     1024        conv1d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 187, 256)     1024        conv1d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_395 (Dropout)           (None, 187, 256)     0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_408 (Dropout)           (None, 187, 256)     0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 187, 256)     0           dropout_395[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 187, 256)     0           dropout_408[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_144 (Concatenate)   (None, 187, 512)     0           activation_393[0][0]             \n",
      "                                                                 activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_150 (Concatenate)   (None, 187, 512)     0           activation_406[0][0]             \n",
      "                                                                 activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_336 (Conv1D)             (None, 187, 256)     524544      concatenate_144[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_349 (Conv1D)             (None, 187, 256)     524544      concatenate_150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 187, 256)     1024        conv1d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 187, 256)     1024        conv1d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_396 (Dropout)           (None, 187, 256)     0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_409 (Dropout)           (None, 187, 256)     0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 187, 256)     0           dropout_396[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 187, 256)     0           dropout_409[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_150 (MaxPooling1D (None, 93, 256)      0           activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_154 (MaxPooling1D (None, 93, 256)      0           activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_337 (Conv1D)             (None, 93, 256)      327936      max_pooling1d_150[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_350 (Conv1D)             (None, 93, 256)      327936      max_pooling1d_154[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 93, 256)      1024        conv1d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 93, 256)      1024        conv1d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_397 (Dropout)           (None, 93, 256)      0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_410 (Dropout)           (None, 93, 256)      0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 93, 256)      0           dropout_397[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 93, 256)      0           dropout_410[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_338 (Conv1D)             (None, 93, 256)      327936      activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_351 (Conv1D)             (None, 93, 256)      327936      activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 93, 256)      1024        conv1d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 93, 256)      1024        conv1d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_398 (Dropout)           (None, 93, 256)      0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_411 (Dropout)           (None, 93, 256)      0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 93, 256)      0           dropout_398[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 93, 256)      0           dropout_411[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_145 (Concatenate)   (None, 93, 512)      0           activation_397[0][0]             \n",
      "                                                                 activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_151 (Concatenate)   (None, 93, 512)      0           activation_410[0][0]             \n",
      "                                                                 activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_339 (Conv1D)             (None, 93, 256)      655616      concatenate_145[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_352 (Conv1D)             (None, 93, 256)      655616      concatenate_151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 93, 256)      1024        conv1d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 93, 256)      1024        conv1d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_399 (Dropout)           (None, 93, 256)      0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_412 (Dropout)           (None, 93, 256)      0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 93, 256)      0           dropout_399[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 93, 256)      0           dropout_412[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_146 (Concatenate)   (None, 93, 512)      0           activation_397[0][0]             \n",
      "                                                                 activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_152 (Concatenate)   (None, 93, 512)      0           activation_410[0][0]             \n",
      "                                                                 activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_340 (Conv1D)             (None, 93, 256)      655616      concatenate_146[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_353 (Conv1D)             (None, 93, 256)      655616      concatenate_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 93, 256)      1024        conv1d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 93, 256)      1024        conv1d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_400 (Dropout)           (None, 93, 256)      0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_413 (Dropout)           (None, 93, 256)      0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 93, 256)      0           dropout_400[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 93, 256)      0           dropout_413[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_151 (MaxPooling1D (None, 46, 256)      0           activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_155 (MaxPooling1D (None, 46, 256)      0           activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_43 (Flatten)            (None, 11776)        0           max_pooling1d_151[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_44 (Flatten)            (None, 11776)        0           max_pooling1d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_153 (Concatenate)   (None, 23552)        0           flatten_43[0][0]                 \n",
      "                                                                 flatten_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 256)          6029568     concatenate_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 256)          1024        dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_414 (Dropout)           (None, 256)          0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 256)          0           dropout_414[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 64)           16448       activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 64)           256         dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_415 (Dropout)           (None, 64)           0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 64)           0           dropout_415[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 13)           845         activation_415[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 13,432,141\n",
      "Trainable params: 13,421,005\n",
      "Non-trainable params: 11,136\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 52s 9ms/step - loss: 2.1597 - acc: 0.2737 - val_loss: 2.8924 - val_acc: 0.0791\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 1.7240 - acc: 0.4353 - val_loss: 2.5925 - val_acc: 0.2152\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 1.4601 - acc: 0.5346 - val_loss: 2.5099 - val_acc: 0.1693\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 1.2478 - acc: 0.6171 - val_loss: 2.1430 - val_acc: 0.2547\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 1.1028 - acc: 0.6709 - val_loss: 2.1855 - val_acc: 0.2500\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 1.0090 - acc: 0.7045 - val_loss: 2.0752 - val_acc: 0.2832\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.8972 - acc: 0.7407 - val_loss: 1.7131 - val_acc: 0.4304\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.8212 - acc: 0.7648 - val_loss: 1.4398 - val_acc: 0.5253\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.7443 - acc: 0.7892 - val_loss: 1.6356 - val_acc: 0.4557\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.6818 - acc: 0.8092 - val_loss: 1.2408 - val_acc: 0.6108\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.6304 - acc: 0.8233 - val_loss: 1.0382 - val_acc: 0.6867\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.5879 - acc: 0.8332 - val_loss: 1.2520 - val_acc: 0.6028\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.5230 - acc: 0.8564 - val_loss: 1.0734 - val_acc: 0.6630\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.4915 - acc: 0.8666 - val_loss: 1.0309 - val_acc: 0.6709\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.4432 - acc: 0.8848 - val_loss: 1.0023 - val_acc: 0.6994\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.4314 - acc: 0.8820 - val_loss: 0.9182 - val_acc: 0.7168\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.3885 - acc: 0.8972 - val_loss: 0.7504 - val_acc: 0.7706\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.3509 - acc: 0.9077 - val_loss: 0.7754 - val_acc: 0.7801\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.3240 - acc: 0.9172 - val_loss: 0.8058 - val_acc: 0.7516\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.3068 - acc: 0.9235 - val_loss: 0.6767 - val_acc: 0.7943\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.2811 - acc: 0.9284 - val_loss: 0.6467 - val_acc: 0.8038\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.2602 - acc: 0.9362 - val_loss: 0.6321 - val_acc: 0.8212\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.2444 - acc: 0.9358 - val_loss: 0.5935 - val_acc: 0.8196\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.2125 - acc: 0.9497 - val_loss: 0.6426 - val_acc: 0.8117\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1989 - acc: 0.9541 - val_loss: 0.6408 - val_acc: 0.8085\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.1745 - acc: 0.9589 - val_loss: 0.5221 - val_acc: 0.8560\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1720 - acc: 0.9603 - val_loss: 0.6186 - val_acc: 0.8307\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.1623 - acc: 0.9631 - val_loss: 0.5033 - val_acc: 0.8608\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1480 - acc: 0.9647 - val_loss: 0.5636 - val_acc: 0.8339\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.1445 - acc: 0.9631 - val_loss: 0.5273 - val_acc: 0.8275\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1390 - acc: 0.9650 - val_loss: 0.5507 - val_acc: 0.8434\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.1245 - acc: 0.9708 - val_loss: 0.5594 - val_acc: 0.8418\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1168 - acc: 0.9719 - val_loss: 0.5637 - val_acc: 0.8370\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.1040 - acc: 0.9780 - val_loss: 0.5322 - val_acc: 0.8449\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0986 - acc: 0.9784 - val_loss: 0.5691 - val_acc: 0.8402\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0950 - acc: 0.9777 - val_loss: 0.5506 - val_acc: 0.8434\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0953 - acc: 0.9775 - val_loss: 0.5729 - val_acc: 0.8307\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0836 - acc: 0.9835 - val_loss: 0.5311 - val_acc: 0.8544\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0804 - acc: 0.9822 - val_loss: 0.5717 - val_acc: 0.8386\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0746 - acc: 0.9847 - val_loss: 0.4776 - val_acc: 0.8718\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0807 - acc: 0.9819 - val_loss: 0.5546 - val_acc: 0.8402\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0742 - acc: 0.9833 - val_loss: 0.5045 - val_acc: 0.8671\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0718 - acc: 0.9844 - val_loss: 0.4649 - val_acc: 0.8782\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0584 - acc: 0.9879 - val_loss: 0.5280 - val_acc: 0.8449\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0557 - acc: 0.9879 - val_loss: 0.5333 - val_acc: 0.8528\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0545 - acc: 0.9887 - val_loss: 0.4168 - val_acc: 0.8877\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0536 - acc: 0.9884 - val_loss: 0.4544 - val_acc: 0.8639\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0593 - acc: 0.9866 - val_loss: 0.5299 - val_acc: 0.8465\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0620 - acc: 0.9856 - val_loss: 0.4188 - val_acc: 0.8845\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0529 - acc: 0.9886 - val_loss: 0.4800 - val_acc: 0.8639\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0454 - acc: 0.9903 - val_loss: 0.4780 - val_acc: 0.8655\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0452 - acc: 0.9891 - val_loss: 0.4604 - val_acc: 0.8750\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0445 - acc: 0.9887 - val_loss: 0.4656 - val_acc: 0.8687\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0407 - acc: 0.9921 - val_loss: 0.4065 - val_acc: 0.8845\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0438 - acc: 0.9919 - val_loss: 0.3973 - val_acc: 0.8924\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0389 - acc: 0.9919 - val_loss: 0.6021 - val_acc: 0.8465\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0406 - acc: 0.9896 - val_loss: 0.4969 - val_acc: 0.8718\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0407 - acc: 0.9912 - val_loss: 0.5127 - val_acc: 0.8766\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0347 - acc: 0.9933 - val_loss: 0.4166 - val_acc: 0.8924\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0433 - acc: 0.9887 - val_loss: 0.4520 - val_acc: 0.8797\n",
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0406 - acc: 0.9903 - val_loss: 0.4986 - val_acc: 0.8592\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0413 - acc: 0.9912 - val_loss: 0.5067 - val_acc: 0.8671\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 20s 4ms/step - loss: 0.0308 - acc: 0.9930 - val_loss: 0.4700 - val_acc: 0.8623\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0320 - acc: 0.9928 - val_loss: 0.4489 - val_acc: 0.8797\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0349 - acc: 0.9909 - val_loss: 0.6154 - val_acc: 0.8497\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0334 - acc: 0.9926 - val_loss: 0.5133 - val_acc: 0.8892\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0328 - acc: 0.9917 - val_loss: 0.4205 - val_acc: 0.8845\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0268 - acc: 0.9935 - val_loss: 0.4221 - val_acc: 0.8908\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0338 - acc: 0.9910 - val_loss: 0.4636 - val_acc: 0.8877\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 21s 4ms/step - loss: 0.0295 - acc: 0.9921 - val_loss: 0.4121 - val_acc: 0.8892\n",
      "0.8924050632911392\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train1[:,:,0:4],X_train1[:,:,4:7]], Y_train, validation_data=([X_test1[:,:,0:4],X_test1[:,:,4:7]], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test1[:,:,0:4],X_test1[:,:,4:7]])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense3(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 4)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 3, inputs2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_1(x2, 128, 3)\n",
    "    x2 = dense_block_1(x2, 256, 4)\n",
    "    x2 = dense_block_1(x2, 256, 5)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2],axis=-1)\n",
    "    \n",
    "    xf = dense_block_1(xf, 256, 3)\n",
    "    xf = Flatten()(xf)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 67s 12ms/step - loss: 2.3389 - acc: 0.2189 - val_loss: 3.0489 - val_acc: 0.0791\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 2.0380 - acc: 0.3066 - val_loss: 3.1516 - val_acc: 0.0918\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.8973 - acc: 0.3699 - val_loss: 3.3362 - val_acc: 0.0791\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.7483 - acc: 0.4311 - val_loss: 3.6514 - val_acc: 0.0791\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.5719 - acc: 0.5058 - val_loss: 3.7309 - val_acc: 0.0791\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.4503 - acc: 0.5452 - val_loss: 3.7068 - val_acc: 0.0791\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.3530 - acc: 0.5788 - val_loss: 3.6936 - val_acc: 0.0791\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.2548 - acc: 0.6104 - val_loss: 3.0898 - val_acc: 0.1282\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.1746 - acc: 0.6380 - val_loss: 2.9941 - val_acc: 0.1899\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.1006 - acc: 0.6621 - val_loss: 2.5830 - val_acc: 0.2468\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.0334 - acc: 0.6791 - val_loss: 2.3785 - val_acc: 0.2880\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.9606 - acc: 0.7039 - val_loss: 2.0612 - val_acc: 0.3782\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.8918 - acc: 0.7393 - val_loss: 2.0918 - val_acc: 0.3671\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.8448 - acc: 0.7477 - val_loss: 1.9899 - val_acc: 0.3987\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7866 - acc: 0.7702 - val_loss: 1.9214 - val_acc: 0.3782\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7457 - acc: 0.7825 - val_loss: 1.4556 - val_acc: 0.5158\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7064 - acc: 0.7920 - val_loss: 1.6209 - val_acc: 0.4604\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.6595 - acc: 0.8049 - val_loss: 1.1908 - val_acc: 0.6266\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.6296 - acc: 0.8135 - val_loss: 1.2696 - val_acc: 0.6028\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5943 - acc: 0.8226 - val_loss: 1.4736 - val_acc: 0.5237\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5700 - acc: 0.8312 - val_loss: 1.1287 - val_acc: 0.6345\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5377 - acc: 0.8411 - val_loss: 0.9514 - val_acc: 0.7057\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5062 - acc: 0.8548 - val_loss: 0.6091 - val_acc: 0.8212\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4719 - acc: 0.8604 - val_loss: 1.1507 - val_acc: 0.6392\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4478 - acc: 0.8664 - val_loss: 0.8255 - val_acc: 0.7373\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4249 - acc: 0.8757 - val_loss: 0.6945 - val_acc: 0.7896\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3949 - acc: 0.8866 - val_loss: 0.6898 - val_acc: 0.7785\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3782 - acc: 0.8873 - val_loss: 0.8871 - val_acc: 0.7453\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3516 - acc: 0.8964 - val_loss: 0.6973 - val_acc: 0.7911\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3262 - acc: 0.9040 - val_loss: 0.6444 - val_acc: 0.7927\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3373 - acc: 0.9031 - val_loss: 0.5516 - val_acc: 0.8402\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3003 - acc: 0.9139 - val_loss: 0.5496 - val_acc: 0.8259\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2914 - acc: 0.9133 - val_loss: 0.4686 - val_acc: 0.8544\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2762 - acc: 0.9191 - val_loss: 0.7408 - val_acc: 0.7674\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2478 - acc: 0.9276 - val_loss: 0.4324 - val_acc: 0.8608\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2506 - acc: 0.9286 - val_loss: 0.5783 - val_acc: 0.8180\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2347 - acc: 0.9320 - val_loss: 0.4101 - val_acc: 0.8766\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2251 - acc: 0.9355 - val_loss: 0.5917 - val_acc: 0.8228\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2113 - acc: 0.9383 - val_loss: 0.4633 - val_acc: 0.8449\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1977 - acc: 0.9427 - val_loss: 0.4692 - val_acc: 0.8449\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1891 - acc: 0.9466 - val_loss: 0.3865 - val_acc: 0.8797\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1663 - acc: 0.9539 - val_loss: 0.3256 - val_acc: 0.9082\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1785 - acc: 0.9487 - val_loss: 0.3845 - val_acc: 0.8892\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1540 - acc: 0.9553 - val_loss: 0.4140 - val_acc: 0.8639\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1445 - acc: 0.9564 - val_loss: 0.2908 - val_acc: 0.9130\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1398 - acc: 0.9620 - val_loss: 0.3199 - val_acc: 0.9003\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1497 - acc: 0.9592 - val_loss: 0.5555 - val_acc: 0.8354\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1343 - acc: 0.9617 - val_loss: 0.4322 - val_acc: 0.8687\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1211 - acc: 0.9666 - val_loss: 0.4197 - val_acc: 0.8687\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1080 - acc: 0.9715 - val_loss: 0.3910 - val_acc: 0.8845\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1088 - acc: 0.9708 - val_loss: 0.3078 - val_acc: 0.9130\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1147 - acc: 0.9657 - val_loss: 0.2979 - val_acc: 0.9114\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1099 - acc: 0.9708 - val_loss: 0.3033 - val_acc: 0.9003\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1097 - acc: 0.9685 - val_loss: 0.3953 - val_acc: 0.8861\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0997 - acc: 0.9743 - val_loss: 0.4030 - val_acc: 0.8639\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0982 - acc: 0.9720 - val_loss: 0.2933 - val_acc: 0.9066\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0884 - acc: 0.9750 - val_loss: 0.4662 - val_acc: 0.8560\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0968 - acc: 0.9712 - val_loss: 0.2997 - val_acc: 0.9019\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0766 - acc: 0.9786 - val_loss: 0.3180 - val_acc: 0.9035\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0883 - acc: 0.9775 - val_loss: 0.3186 - val_acc: 0.9003\n",
      "0.9129746835443038\n"
     ]
    }
   ],
   "source": [
    "model = model_dense3()\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train1[:,:,0:4],X_train1[:,:,4:7]], Y_train, validation_data=([X_test1[:,:,0:4],X_test1[:,:,4:7]], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test1[:,:,0:4],X_test1[:,:,4:7]])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_4(xin, f, k):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense4(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_4(x1, 128, 4)\n",
    "    x1 = dense_block_4(x1, 256, 5)\n",
    "    x1 = dense_block_4(x1, 256, 5)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 3, inputs2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_4(x2, 128, 4)\n",
    "    x2 = dense_block_4(x2, 256, 5)\n",
    "    x2 = dense_block_4(x2, 256, 5)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2],axis=-1)\n",
    "    \n",
    "    xf = dense_block_4(xf, 256, 4)\n",
    "    xf = Flatten()(xf)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9208860759493671, 0.9178846153846154, 0.924348554678574, 0.9191159287707931, 0.9145298200274602)\n",
      "(0.9272151898734177, 0.9188461538461538, 0.9291180206490424, 0.9212421825991927, 0.9213252215304497)\n",
      "(0.9224683544303798, 0.9194230769230768, 0.9263536455696335, 0.9215026593774243, 0.9162229745094559)\n",
      "(0.9113924050632911, 0.9060576923076924, 0.911446838380263, 0.906767810877328, 0.9042386424052794)\n",
      "(0.9050632911392406, 0.9042307692307693, 0.905660910995597, 0.9020540255302592, 0.897551618998997)\n",
      "(0.9367088607594937, 0.9315384615384615, 0.9369622273630032, 0.9331182012500321, 0.9314853590133775)\n",
      "(0.9193037974683544, 0.9111538461538462, 0.9202301327190539, 0.9129073356763678, 0.9127822587563296)\n",
      "(0.9193037974683544, 0.9154807692307692, 0.9225622872481003, 0.9168512725131162, 0.9127800713795204)\n",
      "(0.9224683544303798, 0.9185576923076924, 0.9267359646935243, 0.9208986397408669, 0.9162976647732292)\n",
      "(0.9161392405063291, 0.9054807692307691, 0.919673735878183, 0.907204683919741, 0.9095882602428886)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file7(i)\n",
    "    model = model_dense4()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],X_train[:,:,4:7]], Y_train, validation_data=([X_test[:,:,0:4],X_test[:,:,4:7]], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.save_weights(\"Checkpoints/Dense_RNAfold_1002_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_test[:,:,0:4],X_test[:,:,4:7]])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_file7(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense5(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_4(x1, 128, 4)\n",
    "    x1 = dense_block_4(x1, 256, 5)\n",
    "    #x1 = dense_block_4(x1, 256, 5)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 3, inputs2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_4(x2, 128, 4)\n",
    "    x2 = dense_block_4(x2, 256, 5)\n",
    "    #x2 = dense_block_4(x2, 256, 5)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2],axis=-1)\n",
    "    \n",
    "    xf = dense_block_4(xf, 256, 4)\n",
    "    xf = Flatten()(xf)\n",
    "\n",
    "    xf=FC1Dme(256,xf)\n",
    "    xf=FC1Dme(64,xf)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9353846153846154, 0.9353846153846155, 0.9376353492180876, 0.9355291637839754, 0.9301571263499605)\n",
      "(0.9276923076923077, 0.9276923076923077, 0.9316479057578468, 0.9273583617107536, 0.922058033325592)\n",
      "(0.9373076923076923, 0.9373076923076921, 0.9395116022421947, 0.9377707462533988, 0.9321929919716658)\n",
      "(0.9338461538461539, 0.9338461538461538, 0.9362606000710788, 0.9339499644551006, 0.9285105707764049)\n",
      "(0.925, 0.9250000000000002, 0.9292198910641495, 0.9255773913465568, 0.9190484446349905)\n",
      "(0.9284615384615384, 0.9284615384615387, 0.9301348267778005, 0.9285386396260225, 0.9226333774782174)\n",
      "(0.9088461538461539, 0.908846153846154, 0.9282430700593006, 0.9124683873037504, 0.902951583289637)\n",
      "(0.9338461538461539, 0.9338461538461539, 0.9357461737909765, 0.9338247657839154, 0.9284880942468589)\n",
      "(0.9307692307692308, 0.9307692307692308, 0.9336310101226287, 0.9305855194159665, 0.9252755477547618)\n",
      "(0.9265384615384615, 0.9265384615384615, 0.9323303053768869, 0.9274475655613176, 0.9208493018396655)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file7(i)\n",
    "    model = model_dense5()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],X_train[:,:,4:7]], Y_train, validation_data=([X_test[:,:,0:4],X_test[:,:,4:7]], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.save_weights(\"Checkpoints/Dense_RNAfold_1003_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],X_val[:,:,4:7]])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9287692307692307\n",
      "0.9287692307692307\n",
      "0.9334360734480951\n",
      "0.9293050505240756\n",
      "0.9232165071667755\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 750, 4)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[:,:,0:4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_val = h5.File(my_files7[20],'r')\n",
    "X_val = hf_val['Test_Data']     # Get test set\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_val['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)    #  Process the label of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_vals.h5'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files7[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
