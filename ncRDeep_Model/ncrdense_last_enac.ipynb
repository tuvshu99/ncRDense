{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import  Dense, Flatten, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, merge, Input,Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import  Bidirectional\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Test_Data_750.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Train_Data_750.h5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot*.h5\")\n",
    "my_files8.sort()\n",
    "my_files8[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/Test_enac_0.txt',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/Test_enac_1.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files6 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/T*\")\n",
    "my_files6.sort()\n",
    "my_files6[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enac(fold_no):\n",
    "    enac_test = my_files6[fold_no]\n",
    "    enac_train = my_files6[10+fold_no]\n",
    "    enac_test_lines = np.loadtxt(enac_test, delimiter=\"\\t\", unpack=False)\n",
    "    enac_train_lines = np.loadtxt(enac_train, delimiter=\"\\t\", unpack=False)\n",
    "    enac_test_lines = np.delete(enac_test_lines, 0, 1)\n",
    "    enac_train_lines = np.delete(enac_train_lines, 0, 1)\n",
    "    enac_test_lines = enac_test_lines.reshape(632,750,4)\n",
    "    enac_train_lines = enac_train_lines.reshape(5688,750,4)\n",
    "    return enac_train_lines, enac_test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enac_val = my_files6[20]\n",
    "enac_val_lines = np.loadtxt(enac_val, delimiter=\"\\t\", unpack=False)\n",
    "enac_val_lines = np.delete(enac_val_lines, 0, 1)\n",
    "enac_val_lines.shape\n",
    "enac_val = enac_val_lines.reshape(2600,750,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 750, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enac_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_Test = h5.File('/home/chenming/ncrna/ncRDeep2/Data_Processing/testdata.h5','r')\n",
    "X_val = hf_Test['Test_Data']      # Get train label\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_Test['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file8(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files8[fold_no*2+1],'r')\n",
    "    hf_Test = h5.File(my_files8[fold_no*2],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1d18ebacefc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_files8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_files8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train_Data'\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# Get test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m       \u001b[0;31m# Get test label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "my_files8[20]\n",
    "hf_val = h5.File(my_files8[20],'r')\n",
    "X_val = hf_val['Train_Data']     # Get test set\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_val['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)    #  Process the label of test\n",
    "X_val[:,:,0:4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5S_rRNA',\n",
       " '5.8S_rRNA',\n",
       " 'tRNA',\n",
       " 'ribozymes',\n",
       " 'CD-box',\n",
       " 'miRNA',\n",
       " 'Intron_gpI',\n",
       " 'Intron_gpII',\n",
       " 'HACA-box',\n",
       " 'riboswitch',\n",
       " 'IRES',\n",
       " 'leader',\n",
       " 'scaRNA']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['5S_rRNA', '5.8S_rRNA', 'tRNA', 'ribozymes', 'CD-box', 'miRNA', 'Intron_gpI', 'Intron_gpII', 'HACA-box', 'riboswitch', 'IRES', 'leader', 'scaRNA']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 10\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\\n============================\")\n",
    "    else:\n",
    "        #cm = np.asfarray(cm,float64)\n",
    "        print('Confusion matrix, without normalization\\n============================')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(5*2, 4*2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=FONT_SIZE)\n",
    "    plt.yticks(tick_marks, classes, fontsize=FONT_SIZE)\n",
    "    fmt = '.3f' if normalize else '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    fontsize=FONT_SIZE,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=FONT_SIZE)\n",
    "    plt.xlabel('Predicted label', fontsize=FONT_SIZE)\n",
    "    plt.savefig('Conf_mat_avg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dme(f, k, x):\n",
    "    x1=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1=Dropout(0.2)(x1)\n",
    "    x1=Activation('relu')(x1)\n",
    "    #x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_1(xin, f, k, p):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "    p1 = p\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=p1, strides=p1)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block_1(xin, n):\n",
    "    xf=Dense(n,)(xin)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense5(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 5, inputs1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 5, 4)\n",
    "    x1 = dense_block_1(x1, 128, 5, 4)\n",
    "    x1 = Conv1Dme(64, 5, x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 5, inputs2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_1(x2, 128, 5, 4)\n",
    "    x2 = dense_block_1(x2, 128, 5, 4)\n",
    "    x2 = Conv1Dme(64, 5, x2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2],axis=-1)\n",
    "    \n",
    "    #xf = dense_block_1(xf, 256, 5, 2)\n",
    "    xf = Conv1Dme(64, 3, xf)\n",
    "    xf = MaxPooling1D(pool_size=2, strides=2)(xf)\n",
    "    xf = Flatten()(xf)\n",
    "\n",
    "    xf=fc_block_1(xf, 256)\n",
    "    xf=fc_block_1(xf, 64)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0005),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9403846153846154, 0.9403846153846153, 0.9470223766324697, 0.9417431079360076, 0.9357982622223758)\n",
      "(0.938076923076923, 0.9380769230769231, 0.9433345146640337, 0.9392673585651484, 0.9331954705514665)\n",
      "(0.9415384615384615, 0.9415384615384615, 0.9476044332008831, 0.9423137183034729, 0.9370967185361719)\n",
      "(0.9392307692307692, 0.9392307692307692, 0.9426477000124489, 0.9396965355534695, 0.9343934043710717)\n",
      "(0.9492307692307692, 0.9492307692307692, 0.9518989621006939, 0.9497562765564779, 0.9451443575904533)\n",
      "(0.9411538461538461, 0.9411538461538461, 0.944419622538172, 0.941569287789642, 0.936464931360613)\n",
      "(0.9515384615384616, 0.9515384615384613, 0.9552588935364454, 0.9522639018698169, 0.9477156906151344)\n",
      "(0.9342307692307692, 0.9342307692307692, 0.9387943686206471, 0.9351326710811796, 0.9290028303666918)\n",
      "(0.9388461538461539, 0.9388461538461539, 0.943492546935141, 0.9395476826624107, 0.93404178394628)\n",
      "(0.9461538461538461, 0.9461538461538461, 0.9494365036815038, 0.9471476685679089, 0.9417937570275752)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    X_train_enac, X_test_enac = get_enac(i)\n",
    "    model = model_dense5()\n",
    "    #es = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "    #history = model.fit([X_train[:,:,0:4],X_train_enac], Y_train, validation_data=([X_test[:,:,0:4],X_test_enac], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.load_weights(\"Checkpoints/Dense_enac_revise2_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],enac_val])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    #history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics.ranking.auc>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function auc at 0x7f9483f08950>\n"
     ]
    }
   ],
   "source": [
    "if auc>0.93:\n",
    "    print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420384615384615\n",
      "0.9420384615384615\n",
      "0.9463909921922438\n",
      "0.9428438208885535\n",
      "0.9374647206587834\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "t27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
