{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import  Dense, Flatten, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, merge, Input,Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import  Bidirectional\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/chenming/ncrna/ncRDeep2/Data_Processing/EIIP_data/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/EIIP_data/Test_0.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/EIIP_data/Test_1.h5']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/EIIP_data/T*.h5\")\n",
    "my_files8.sort()\n",
    "my_files8[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file8(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files8[10+fold_no],'r')\n",
    "    hf_Test = h5.File(my_files8[fold_no],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 750, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8[20]\n",
    "hf_val = h5.File(my_files8[20],'r')\n",
    "X_val = hf_val['Train_Data']     # Get test set\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_val['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)    #  Process the label of test\n",
    "X_val[:,:,0:4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 1.    , 0.    , 0.    , 0.0806])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 10\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\\n============================\")\n",
    "    else:\n",
    "        #cm = np.asfarray(cm,float64)\n",
    "        print('Confusion matrix, without normalization\\n============================')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(5*2, 4*2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=FONT_SIZE)\n",
    "    plt.yticks(tick_marks, classes, fontsize=FONT_SIZE)\n",
    "    fmt = '.3f' if normalize else '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    fontsize=FONT_SIZE,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=FONT_SIZE)\n",
    "    plt.xlabel('Predicted label', fontsize=FONT_SIZE)\n",
    "    plt.savefig('Conf_mat_avg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5S_rRNA',\n",
       " '5.8S_rRNA',\n",
       " 'tRNA',\n",
       " 'ribozymes',\n",
       " 'CD-box',\n",
       " 'miRNA',\n",
       " 'Intron_gpI',\n",
       " 'Intron_gpII',\n",
       " 'HACA-box',\n",
       " 'riboswitch',\n",
       " 'IRES',\n",
       " 'leader',\n",
       " 'scaRNA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_names = [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]\n",
    "class_names = ['5S_rRNA', '5.8S_rRNA', 'tRNA', 'ribozymes', 'CD-box', 'miRNA', 'Intron_gpI', 'Intron_gpII', 'HACA-box', 'riboswitch', 'IRES', 'leader', 'scaRNA']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(f1,k1,f2,k2): # Model\n",
    "    inputs1 = Input(shape=(750, 5))\n",
    "    \n",
    "    x=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    #x=Conv1D(filters=32,kernel_size=2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Activation('relu')(x)\n",
    "    #x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    x2=Flatten()(x)\n",
    "\n",
    "    x3=Dense(128,)(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(32,)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(13, activation='softmax',  )(x3)\n",
    "\n",
    "    model = Model(inputs=inputs1, outputs=x3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_file8(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 750, 5)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 731, 512)          51712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 731, 512)          2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 731, 512)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 731, 512)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 365, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 361, 128)          327808    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 361, 128)          512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 361, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 361, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 180, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23040)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2949248   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13)                429       \n",
      "=================================================================\n",
      "Total params: 3,336,525\n",
      "Trainable params: 3,334,925\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_conv(512, 20, 128, 5)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 10s 2ms/step - loss: 2.1266 - acc: 0.2989 - val_loss: 1.7334 - val_acc: 0.4968\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 5s 895us/step - loss: 1.6488 - acc: 0.5077 - val_loss: 1.4903 - val_acc: 0.6345\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 5s 892us/step - loss: 1.4547 - acc: 0.5740 - val_loss: 1.3789 - val_acc: 0.6646\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 5s 870us/step - loss: 1.3278 - acc: 0.6414 - val_loss: 1.2598 - val_acc: 0.6946\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 5s 871us/step - loss: 1.2142 - acc: 0.6788 - val_loss: 1.1771 - val_acc: 0.7278\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 5s 875us/step - loss: 1.1188 - acc: 0.7126 - val_loss: 1.1282 - val_acc: 0.7278\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 5s 873us/step - loss: 1.0310 - acc: 0.7407 - val_loss: 1.0224 - val_acc: 0.7642\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 5s 868us/step - loss: 0.9268 - acc: 0.7813 - val_loss: 0.9883 - val_acc: 0.7927\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 5s 868us/step - loss: 0.8576 - acc: 0.8034 - val_loss: 0.9537 - val_acc: 0.7896\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 5s 863us/step - loss: 0.7890 - acc: 0.8187 - val_loss: 0.8716 - val_acc: 0.8038\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 5s 865us/step - loss: 0.7366 - acc: 0.8337 - val_loss: 0.8290 - val_acc: 0.8038\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 5s 871us/step - loss: 0.6781 - acc: 0.8572 - val_loss: 0.7473 - val_acc: 0.8323\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 5s 879us/step - loss: 0.5996 - acc: 0.8782 - val_loss: 0.7565 - val_acc: 0.7975\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 5s 866us/step - loss: 0.5577 - acc: 0.8912 - val_loss: 0.6857 - val_acc: 0.8386\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 5s 875us/step - loss: 0.5103 - acc: 0.9005 - val_loss: 0.6900 - val_acc: 0.8386\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 5s 866us/step - loss: 0.4663 - acc: 0.9191 - val_loss: 0.6411 - val_acc: 0.8402\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 5s 864us/step - loss: 0.4305 - acc: 0.9241 - val_loss: 0.6185 - val_acc: 0.8513\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 5s 867us/step - loss: 0.3911 - acc: 0.9300 - val_loss: 0.6199 - val_acc: 0.8259\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 5s 863us/step - loss: 0.3541 - acc: 0.9413 - val_loss: 0.5879 - val_acc: 0.8418\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 5s 862us/step - loss: 0.3356 - acc: 0.9425 - val_loss: 0.6191 - val_acc: 0.8291\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 5s 857us/step - loss: 0.3003 - acc: 0.9518 - val_loss: 0.5293 - val_acc: 0.8703\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 5s 880us/step - loss: 0.2768 - acc: 0.9587 - val_loss: 0.5945 - val_acc: 0.8354\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 5s 869us/step - loss: 0.2609 - acc: 0.9620 - val_loss: 0.5345 - val_acc: 0.8481\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 5s 868us/step - loss: 0.2333 - acc: 0.9682 - val_loss: 0.4880 - val_acc: 0.8718\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 5s 872us/step - loss: 0.2267 - acc: 0.9650 - val_loss: 0.6551 - val_acc: 0.7991\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 5s 888us/step - loss: 0.2170 - acc: 0.9645 - val_loss: 0.5290 - val_acc: 0.8560\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 5s 891us/step - loss: 0.1866 - acc: 0.9738 - val_loss: 0.4993 - val_acc: 0.8513\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 5s 887us/step - loss: 0.1879 - acc: 0.9720 - val_loss: 0.4971 - val_acc: 0.8481\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 5s 897us/step - loss: 0.1760 - acc: 0.9740 - val_loss: 0.4884 - val_acc: 0.8513\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 5s 881us/step - loss: 0.1532 - acc: 0.9812 - val_loss: 0.4497 - val_acc: 0.8766\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 5s 887us/step - loss: 0.1476 - acc: 0.9789 - val_loss: 0.5219 - val_acc: 0.8418\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 5s 865us/step - loss: 0.1404 - acc: 0.9777 - val_loss: 0.5168 - val_acc: 0.8513\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 5s 861us/step - loss: 0.1315 - acc: 0.9801 - val_loss: 0.4989 - val_acc: 0.8481\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 5s 874us/step - loss: 0.1220 - acc: 0.9851 - val_loss: 0.4896 - val_acc: 0.8513\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 5s 872us/step - loss: 0.1226 - acc: 0.9814 - val_loss: 0.4269 - val_acc: 0.8718\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 5s 860us/step - loss: 0.1148 - acc: 0.9822 - val_loss: 0.4487 - val_acc: 0.8623\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 5s 857us/step - loss: 0.1053 - acc: 0.9861 - val_loss: 0.4707 - val_acc: 0.8513\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 5s 861us/step - loss: 0.1078 - acc: 0.9849 - val_loss: 0.4496 - val_acc: 0.8687\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 5s 876us/step - loss: 0.0997 - acc: 0.9842 - val_loss: 0.4310 - val_acc: 0.8797\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 5s 897us/step - loss: 0.0946 - acc: 0.9865 - val_loss: 0.4728 - val_acc: 0.8655\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 5s 894us/step - loss: 0.0919 - acc: 0.9847 - val_loss: 0.4593 - val_acc: 0.8703\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 5s 871us/step - loss: 0.0916 - acc: 0.9863 - val_loss: 0.4592 - val_acc: 0.8703\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 5s 884us/step - loss: 0.0870 - acc: 0.9852 - val_loss: 0.4743 - val_acc: 0.8528\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 5s 866us/step - loss: 0.0800 - acc: 0.9882 - val_loss: 0.4198 - val_acc: 0.8797\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 5s 858us/step - loss: 0.0791 - acc: 0.9880 - val_loss: 0.4271 - val_acc: 0.8718\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 5s 858us/step - loss: 0.0728 - acc: 0.9898 - val_loss: 0.4550 - val_acc: 0.8623\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 5s 888us/step - loss: 0.0686 - acc: 0.9910 - val_loss: 0.4609 - val_acc: 0.8687\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 5s 875us/step - loss: 0.0753 - acc: 0.9875 - val_loss: 0.5326 - val_acc: 0.8465\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 5s 870us/step - loss: 0.0686 - acc: 0.9887 - val_loss: 0.4584 - val_acc: 0.8718\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 5s 868us/step - loss: 0.0674 - acc: 0.9870 - val_loss: 0.4492 - val_acc: 0.8608\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 5s 863us/step - loss: 0.0696 - acc: 0.9879 - val_loss: 0.4648 - val_acc: 0.8687\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 5s 875us/step - loss: 0.0663 - acc: 0.9868 - val_loss: 0.4381 - val_acc: 0.8703\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 5s 877us/step - loss: 0.0643 - acc: 0.9887 - val_loss: 0.4757 - val_acc: 0.8497\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 5s 888us/step - loss: 0.0656 - acc: 0.9882 - val_loss: 0.4850 - val_acc: 0.8481\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 5s 860us/step - loss: 0.0555 - acc: 0.9902 - val_loss: 0.4716 - val_acc: 0.8671\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 5s 867us/step - loss: 0.0567 - acc: 0.9907 - val_loss: 0.4616 - val_acc: 0.8671\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 5s 881us/step - loss: 0.0570 - acc: 0.9884 - val_loss: 0.4752 - val_acc: 0.8528\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 5s 882us/step - loss: 0.0503 - acc: 0.9910 - val_loss: 0.4723 - val_acc: 0.8576\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 5s 873us/step - loss: 0.0522 - acc: 0.9900 - val_loss: 0.5136 - val_acc: 0.8449\n",
      "0.879746835443038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict(X_test)\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(f1,k1,f2,k2): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 1))\n",
    "    \n",
    "    x=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    #x=Conv1D(filters=32,kernel_size=2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Activation('relu')(x)\n",
    "    #x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    xx=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs2)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "\n",
    "    xx=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (xx)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "    \n",
    "    x2=keras.layers.concatenate([xx,x],axis=1)\n",
    "    \n",
    "    x2=Conv1D(filters=32,kernel_size=2,strides=2,kernel_initializer=initializers.random_uniform()) (x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2=Dropout(0.2)(x2)\n",
    "    x2=Activation('relu')(x2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2=Flatten()(x2)\n",
    "\n",
    "    x3=Dense(256,)(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(64,)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(13, activation='softmax',  )(x3)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=x3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 750, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 731, 512)     10752       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 731, 512)     41472       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 731, 512)     2048        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 731, 512)     2048        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 731, 512)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 731, 512)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 731, 512)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 731, 512)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 365, 512)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 365, 512)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 360, 128)     393344      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 360, 128)     393344      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 360, 128)     512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 360, 128)     512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 360, 128)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 360, 128)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 360, 128)     0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 360, 128)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 180, 128)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 180, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 360, 128)     0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 180, 32)      8224        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 180, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 180, 32)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 180, 32)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 90, 32)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2880)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          737536      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 256)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           256         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 13)           845         activation_7[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,608,493\n",
      "Trainable params: 1,605,229\n",
      "Non-trainable params: 3,264\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_conv(512, 20, 128, 6)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8718354430379747, 0.8684615384615385, 0.877566615774163, 0.8711131144015395, 0.8613825134140778)\n",
      "(0.8813291139240507, 0.8776923076923079, 0.8793484760524662, 0.8763651695682996, 0.8717258423372284)\n",
      "(0.8908227848101266, 0.8903846153846153, 0.8920431214459553, 0.8904575451388214, 0.8817674987552615)\n",
      "(0.865506329113924, 0.8597115384615385, 0.8713141940249393, 0.8628896746484762, 0.8545413479811949)\n",
      "(0.8670886075949367, 0.8586538461538461, 0.8822189925545934, 0.8641970866868502, 0.8566825945973874)\n",
      "(0.8971518987341772, 0.8922115384615386, 0.8966164835852843, 0.8936297280507012, 0.8885999053875647)\n",
      "(0.8781645569620253, 0.8720192307692307, 0.8850538696659336, 0.8755283804045024, 0.8681979468719464)\n",
      "(0.8607594936708861, 0.8559615384615384, 0.8752376023104543, 0.8601196161981683, 0.8500613821301719)\n",
      "(0.8813291139240507, 0.8785576923076923, 0.8955063231837062, 0.8821468486221093, 0.8724910565502515)\n",
      "(0.8813291139240507, 0.8750961538461539, 0.8856886472699469, 0.8777048988888578, 0.8716499914210685)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    EIIP_train = X_train[:,:,4].reshape((5688,750,1))\n",
    "    EIIP_test = X_test[:,:,4].reshape((632,750,1))\n",
    "    model = model_conv(512, 20, 128, 6)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],EIIP_train], Y_train, validation_data=([X_test[:,:,0:4],EIIP_test], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test[:,:,0:4],EIIP_test])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8775316455696203\n",
      "0.872875\n",
      "0.8840594325867442\n",
      "0.8754152062608325\n",
      "0.8677100079446152\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 13s 2ms/step - loss: 2.4287 - acc: 0.2289 - val_loss: 1.9406 - val_acc: 0.4051\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.9461 - acc: 0.3820 - val_loss: 1.7659 - val_acc: 0.4699\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.7107 - acc: 0.4587 - val_loss: 1.6013 - val_acc: 0.5443\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.5585 - acc: 0.5125 - val_loss: 1.4876 - val_acc: 0.5886\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.4205 - acc: 0.5603 - val_loss: 1.3754 - val_acc: 0.6123\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.3230 - acc: 0.5923 - val_loss: 1.3124 - val_acc: 0.6408\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.2363 - acc: 0.6250 - val_loss: 1.2444 - val_acc: 0.6614\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.1536 - acc: 0.6517 - val_loss: 1.1995 - val_acc: 0.6693\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.0941 - acc: 0.6663 - val_loss: 1.1283 - val_acc: 0.6851\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.0468 - acc: 0.6827 - val_loss: 1.0869 - val_acc: 0.7025\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.9898 - acc: 0.7022 - val_loss: 1.0284 - val_acc: 0.7136\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.9386 - acc: 0.7185 - val_loss: 1.0117 - val_acc: 0.7057\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.9009 - acc: 0.7380 - val_loss: 0.9299 - val_acc: 0.7516\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.8463 - acc: 0.7456 - val_loss: 0.8994 - val_acc: 0.7547\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.7967 - acc: 0.7584 - val_loss: 0.8798 - val_acc: 0.7595\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.7683 - acc: 0.7736 - val_loss: 0.9294 - val_acc: 0.7168\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.7212 - acc: 0.7831 - val_loss: 0.8001 - val_acc: 0.7848\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.6901 - acc: 0.7973 - val_loss: 0.7351 - val_acc: 0.7911\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.6506 - acc: 0.8110 - val_loss: 0.7435 - val_acc: 0.7927\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.6168 - acc: 0.8191 - val_loss: 0.6908 - val_acc: 0.8149\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5851 - acc: 0.8356 - val_loss: 0.6805 - val_acc: 0.8022\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5525 - acc: 0.8411 - val_loss: 0.6143 - val_acc: 0.8165\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5217 - acc: 0.8502 - val_loss: 0.7791 - val_acc: 0.7532\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5054 - acc: 0.8578 - val_loss: 0.6134 - val_acc: 0.8133\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4653 - acc: 0.8641 - val_loss: 0.6371 - val_acc: 0.8101\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4457 - acc: 0.8743 - val_loss: 0.5547 - val_acc: 0.8434\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4247 - acc: 0.8812 - val_loss: 0.5413 - val_acc: 0.8386\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3959 - acc: 0.8891 - val_loss: 0.5453 - val_acc: 0.8339\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3854 - acc: 0.8889 - val_loss: 0.5471 - val_acc: 0.8323\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3650 - acc: 0.8952 - val_loss: 0.5017 - val_acc: 0.8528\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3452 - acc: 0.9012 - val_loss: 0.5090 - val_acc: 0.8623\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3222 - acc: 0.9128 - val_loss: 0.5199 - val_acc: 0.8418\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3199 - acc: 0.9119 - val_loss: 0.5180 - val_acc: 0.8370\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2966 - acc: 0.9193 - val_loss: 0.5704 - val_acc: 0.8133\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2903 - acc: 0.9184 - val_loss: 0.4974 - val_acc: 0.8528\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2575 - acc: 0.9304 - val_loss: 0.5082 - val_acc: 0.8449\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2520 - acc: 0.9344 - val_loss: 0.5762 - val_acc: 0.8165\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2319 - acc: 0.9383 - val_loss: 0.5043 - val_acc: 0.8449\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2212 - acc: 0.9404 - val_loss: 0.4506 - val_acc: 0.8671\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2090 - acc: 0.9469 - val_loss: 0.4847 - val_acc: 0.8608\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2093 - acc: 0.9411 - val_loss: 0.4929 - val_acc: 0.8465\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1975 - acc: 0.9474 - val_loss: 0.4700 - val_acc: 0.8481\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1896 - acc: 0.9504 - val_loss: 0.5120 - val_acc: 0.8560\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1877 - acc: 0.9490 - val_loss: 0.4994 - val_acc: 0.8449\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1778 - acc: 0.9490 - val_loss: 0.5138 - val_acc: 0.8418\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1623 - acc: 0.9571 - val_loss: 0.4468 - val_acc: 0.8623\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1627 - acc: 0.9546 - val_loss: 0.4881 - val_acc: 0.8544\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1463 - acc: 0.9608 - val_loss: 0.5261 - val_acc: 0.8418\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1418 - acc: 0.9638 - val_loss: 0.4822 - val_acc: 0.8513\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1270 - acc: 0.9664 - val_loss: 0.4614 - val_acc: 0.8623\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1280 - acc: 0.9675 - val_loss: 0.5716 - val_acc: 0.8196\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1287 - acc: 0.9648 - val_loss: 0.4793 - val_acc: 0.8623\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1193 - acc: 0.9680 - val_loss: 0.5087 - val_acc: 0.8513\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1099 - acc: 0.9712 - val_loss: 0.4347 - val_acc: 0.8782\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1146 - acc: 0.9692 - val_loss: 0.4250 - val_acc: 0.8655\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1045 - acc: 0.9738 - val_loss: 0.4371 - val_acc: 0.8687\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1164 - acc: 0.9669 - val_loss: 0.4799 - val_acc: 0.8608\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0978 - acc: 0.9747 - val_loss: 0.4957 - val_acc: 0.8655\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0929 - acc: 0.9766 - val_loss: 0.5118 - val_acc: 0.8481\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0927 - acc: 0.9750 - val_loss: 0.5248 - val_acc: 0.8449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0996 - acc: 0.9717 - val_loss: 0.4985 - val_acc: 0.8592\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0848 - acc: 0.9793 - val_loss: 0.4805 - val_acc: 0.8592\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0879 - acc: 0.9756 - val_loss: 0.4714 - val_acc: 0.8623\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0847 - acc: 0.9782 - val_loss: 0.4991 - val_acc: 0.8528\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0799 - acc: 0.9800 - val_loss: 0.5151 - val_acc: 0.8481\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0785 - acc: 0.9786 - val_loss: 0.4829 - val_acc: 0.8513\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0761 - acc: 0.9808 - val_loss: 0.4212 - val_acc: 0.8813\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0709 - acc: 0.9819 - val_loss: 0.4507 - val_acc: 0.8608\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0823 - acc: 0.9780 - val_loss: 0.4694 - val_acc: 0.8592\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0702 - acc: 0.9807 - val_loss: 0.3823 - val_acc: 0.8908\n",
      "Epoch 71/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0677 - acc: 0.9831 - val_loss: 0.5522 - val_acc: 0.8481\n",
      "Epoch 72/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0655 - acc: 0.9835 - val_loss: 0.4645 - val_acc: 0.8623\n",
      "Epoch 73/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0663 - acc: 0.9807 - val_loss: 0.4521 - val_acc: 0.8639\n",
      "Epoch 74/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0689 - acc: 0.9817 - val_loss: 0.4490 - val_acc: 0.8734\n",
      "Epoch 75/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0634 - acc: 0.9838 - val_loss: 0.5030 - val_acc: 0.8513\n",
      "Epoch 76/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0594 - acc: 0.9838 - val_loss: 0.3869 - val_acc: 0.8877\n",
      "Epoch 77/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0579 - acc: 0.9861 - val_loss: 0.3794 - val_acc: 0.8877\n",
      "Epoch 78/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0574 - acc: 0.9844 - val_loss: 0.3811 - val_acc: 0.8892\n",
      "Epoch 79/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0596 - acc: 0.9821 - val_loss: 0.4345 - val_acc: 0.8750\n",
      "Epoch 80/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0609 - acc: 0.9810 - val_loss: 0.4392 - val_acc: 0.8750\n",
      "Epoch 81/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0496 - acc: 0.9868 - val_loss: 0.4845 - val_acc: 0.8608\n",
      "Epoch 82/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0524 - acc: 0.9863 - val_loss: 0.4354 - val_acc: 0.8718\n",
      "Epoch 83/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0510 - acc: 0.9872 - val_loss: 0.3889 - val_acc: 0.8845\n",
      "Epoch 84/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0553 - acc: 0.9849 - val_loss: 0.4568 - val_acc: 0.8687\n",
      "Epoch 85/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0532 - acc: 0.9870 - val_loss: 0.3869 - val_acc: 0.8877\n",
      "Epoch 86/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0506 - acc: 0.9854 - val_loss: 0.4979 - val_acc: 0.8687\n",
      "Epoch 87/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0460 - acc: 0.9875 - val_loss: 0.4417 - val_acc: 0.8845\n",
      "Epoch 88/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0451 - acc: 0.9896 - val_loss: 0.4444 - val_acc: 0.8671\n",
      "Epoch 89/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0480 - acc: 0.9868 - val_loss: 0.4414 - val_acc: 0.8782\n",
      "Epoch 90/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0406 - acc: 0.9900 - val_loss: 0.4076 - val_acc: 0.8908\n",
      "Epoch 91/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0453 - acc: 0.9889 - val_loss: 0.3911 - val_acc: 0.8924\n",
      "Epoch 92/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0442 - acc: 0.9868 - val_loss: 0.4572 - val_acc: 0.8766\n",
      "0.8876582278481012\n"
     ]
    }
   ],
   "source": [
    "EIIP_train = X_train[:,:,4].reshape((5688,750,1))\n",
    "EIIP_test = X_test[:,:,4].reshape((632,750,1))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train[:,:,0:4],EIIP_train], Y_train, validation_data=([X_test[:,:,0:4],EIIP_test], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test[:,:,0:4],EIIP_test])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'ACGT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIIP_dict = {\n",
    "        'A': 0.1260,\n",
    "        'C': 0.1340,\n",
    "        'G': 0.0806,\n",
    "        'T': 0.1335,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trincleotides = [nn1 + nn2 + nn3 for nn1 in base for nn2 in base for nn3 in base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIIPxyz = {}\n",
    "EIIPnum = []\n",
    "i=0\n",
    "for triN in trincleotides:\n",
    "    EIIPxyz[triN] = EIIP_dict[triN[0]] + EIIP_dict[triN[1]] + EIIP_dict[triN[2]]\n",
    "    EIIPnum.append(EIIP_dict[triN[0]] + EIIP_dict[triN[1]] + EIIP_dict[triN[2]])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnc_train = '/home/chenming/ncrna/ncRDeep2/Data_Processing/TNC/train_0.txt'\n",
    "tnc_test = '/home/chenming/ncrna/ncRDeep2/Data_Processing/TNC/test_0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "tnc_test_lines = loadtxt(tnc_test, delimiter=\"\\t\", unpack=False)\n",
    "tnc_train_lines = loadtxt(tnc_train, delimiter=\"\\t\", unpack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnc_test_lines = np.delete(tnc_test_lines, 0, 1)\n",
    "tnc_train_lines = np.delete(tnc_train_lines, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5688, 64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnc_train_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00591716, 0.03550296, 0.01775148, 0.03550296, 0.01183432,\n",
       "       0.01183432, 0.0295858 , 0.00591716, 0.02366864, 0.01183432,\n",
       "       0.00591716, 0.01183432, 0.00591716, 0.0295858 , 0.02366864,\n",
       "       0.01775148, 0.01775148, 0.01775148, 0.02366864, 0.01775148,\n",
       "       0.        , 0.01775148, 0.01183432, 0.01183432, 0.0295858 ,\n",
       "       0.0295858 , 0.00591716, 0.01183432, 0.        , 0.01775148,\n",
       "       0.        , 0.0295858 , 0.0591716 , 0.        , 0.01183432,\n",
       "       0.02366864, 0.04733728, 0.00591716, 0.01775148, 0.00591716,\n",
       "       0.00591716, 0.01775148, 0.01183432, 0.        , 0.00591716,\n",
       "       0.00591716, 0.01775148, 0.00591716, 0.00591716, 0.00591716,\n",
       "       0.        , 0.        , 0.01775148, 0.00591716, 0.02366864,\n",
       "       0.02366864, 0.03550296, 0.01775148, 0.01183432, 0.01183432,\n",
       "       0.        , 0.01775148, 0.03550296, 0.01775148])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnc_train_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.378 , 0.386 , 0.3326, 0.3855, 0.386 , 0.394 , 0.3406, 0.3935,\n",
       "       0.3326, 0.3406, 0.2872, 0.3401, 0.3855, 0.3935, 0.3401, 0.393 ,\n",
       "       0.386 , 0.394 , 0.3406, 0.3935, 0.394 , 0.402 , 0.3486, 0.4015,\n",
       "       0.3406, 0.3486, 0.2952, 0.3481, 0.3935, 0.4015, 0.3481, 0.401 ,\n",
       "       0.3326, 0.3406, 0.2872, 0.3401, 0.3406, 0.3486, 0.2952, 0.3481,\n",
       "       0.2872, 0.2952, 0.2418, 0.2947, 0.3401, 0.3481, 0.2947, 0.3476,\n",
       "       0.3855, 0.3935, 0.3401, 0.393 , 0.3935, 0.4015, 0.3481, 0.401 ,\n",
       "       0.3401, 0.3481, 0.2947, 0.3476, 0.393 , 0.401 , 0.3476, 0.4005])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EIIPnum = np.asarray(EIIPnum)\n",
    "EIIPnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PseEIIP_test = []\n",
    "for i in range(len(tnc_test_lines)):\n",
    "    PseEIIP_test.append(np.multiply(tnc_train_lines[i],EIIPnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PseEIIP_test = np.asarray(PseEIIP_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PseEIIP_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PseEIIP_train = []\n",
    "for i in range(len(tnc_train_lines)):\n",
    "    PseEIIP_train.append(np.multiply(tnc_train_lines[i],EIIPnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PseEIIP_train = np.asarray(PseEIIP_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5688, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PseEIIP_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PseEIIP_test = PseEIIP_test.reshape(632,64,1)\n",
    "PseEIIP_train = PseEIIP_train.reshape(5688,64,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(f1,k1,f2,k2): # Model\n",
    "    inputs1 = Input(shape=(750, 5))\n",
    "    inputs2 = Input(shape=(64,1))\n",
    "    \n",
    "    x=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    #x=Conv1D(filters=32,kernel_size=2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Activation('relu')(x)\n",
    "    #x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    xx=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs2)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "\n",
    "    xx=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (xx)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "    \n",
    "    x2=keras.layers.concatenate([xx,x],axis=1)\n",
    "    \n",
    "    x2=Conv1D(filters=64,kernel_size=4,strides=1,kernel_initializer=initializers.random_uniform()) (x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2=Dropout(0.2)(x2)\n",
    "    x2=Activation('relu')(x2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2=Flatten()(x2)\n",
    "    #xx=Flatten()(inputs2)\n",
    "    \n",
    "    #x2=keras.layers.concatenate([x2,xx],axis=-1)\n",
    "\n",
    "    x3=Dense(128,)(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "    \n",
    "    \n",
    "\n",
    "    x3=Dense(32,)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(13, activation='softmax',  )(x3)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=x3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 64, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 750, 5)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 45, 512)      10752       input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 731, 512)     51712       input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 45, 512)      2048        conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 731, 512)     2048        conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 45, 512)      0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 731, 512)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 45, 512)      0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 731, 512)     0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 22, 512)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 365, 512)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 17, 128)      393344      max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 360, 128)     393344      max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 128)      512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 360, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 17, 128)      0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 360, 128)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 128)      0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 360, 128)     0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 8, 128)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 180, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 188, 128)     0           max_pooling1d_37[0][0]           \n",
      "                                                                 max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 185, 64)      32832       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 185, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 185, 64)      0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 185, 64)      0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 92, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 5888)         0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          753792      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 128)          512         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 128)          0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 128)          0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           4128        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32)           128         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 32)           0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32)           0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 13)           429         activation_52[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,646,349\n",
      "Trainable params: 1,643,341\n",
      "Non-trainable params: 3,008\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_conv(512, 20, 128, 6)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 12s 2ms/step - loss: 2.3735 - acc: 0.2312 - val_loss: 1.9547 - val_acc: 0.3877\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.9248 - acc: 0.3957 - val_loss: 1.8140 - val_acc: 0.5111\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.7159 - acc: 0.4808 - val_loss: 1.6354 - val_acc: 0.5633\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.5671 - acc: 0.5355 - val_loss: 1.5497 - val_acc: 0.6187\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.4379 - acc: 0.5863 - val_loss: 1.4725 - val_acc: 0.6709\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.3511 - acc: 0.6187 - val_loss: 1.3938 - val_acc: 0.6614\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.2650 - acc: 0.6549 - val_loss: 1.3405 - val_acc: 0.6741\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.2022 - acc: 0.6790 - val_loss: 1.2305 - val_acc: 0.7168\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.1299 - acc: 0.6916 - val_loss: 1.1994 - val_acc: 0.7310\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.0614 - acc: 0.7192 - val_loss: 1.0972 - val_acc: 0.7389\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 1.0023 - acc: 0.7344 - val_loss: 1.0764 - val_acc: 0.7247\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.9483 - acc: 0.7581 - val_loss: 1.0024 - val_acc: 0.7358\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.8908 - acc: 0.7681 - val_loss: 0.8862 - val_acc: 0.7927\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.8575 - acc: 0.7781 - val_loss: 0.8481 - val_acc: 0.7896\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.7999 - acc: 0.7938 - val_loss: 0.8437 - val_acc: 0.7959\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.7610 - acc: 0.8066 - val_loss: 0.8618 - val_acc: 0.7579\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.7123 - acc: 0.8212 - val_loss: 0.7381 - val_acc: 0.8054\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.6806 - acc: 0.8263 - val_loss: 0.7758 - val_acc: 0.7690\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.6314 - acc: 0.8427 - val_loss: 0.7364 - val_acc: 0.8070\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.5965 - acc: 0.8579 - val_loss: 0.6781 - val_acc: 0.8133\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.5490 - acc: 0.8637 - val_loss: 0.6119 - val_acc: 0.8418\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.5218 - acc: 0.8764 - val_loss: 0.5871 - val_acc: 0.8354\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4853 - acc: 0.8836 - val_loss: 0.6396 - val_acc: 0.8133\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4628 - acc: 0.8912 - val_loss: 0.5297 - val_acc: 0.8623\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4364 - acc: 0.9014 - val_loss: 0.5193 - val_acc: 0.8623\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4107 - acc: 0.9068 - val_loss: 0.5408 - val_acc: 0.8481\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.4097 - acc: 0.9052 - val_loss: 0.4840 - val_acc: 0.8592\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.3628 - acc: 0.9175 - val_loss: 0.5324 - val_acc: 0.8465\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.3474 - acc: 0.9214 - val_loss: 0.5061 - val_acc: 0.8449\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.3257 - acc: 0.9263 - val_loss: 0.4296 - val_acc: 0.8782\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.3134 - acc: 0.9318 - val_loss: 0.5290 - val_acc: 0.8291\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2946 - acc: 0.9313 - val_loss: 0.4341 - val_acc: 0.8703\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2783 - acc: 0.9371 - val_loss: 0.4218 - val_acc: 0.8766\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2599 - acc: 0.9395 - val_loss: 0.3807 - val_acc: 0.8877\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2465 - acc: 0.9473 - val_loss: 0.4591 - val_acc: 0.8576\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2238 - acc: 0.9531 - val_loss: 0.4434 - val_acc: 0.8608\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2193 - acc: 0.9543 - val_loss: 0.3907 - val_acc: 0.8718\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.2147 - acc: 0.9589 - val_loss: 0.3630 - val_acc: 0.8797\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1974 - acc: 0.9580 - val_loss: 0.3587 - val_acc: 0.8892\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1909 - acc: 0.9606 - val_loss: 0.4011 - val_acc: 0.8671\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1880 - acc: 0.9589 - val_loss: 0.4467 - val_acc: 0.8497\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1697 - acc: 0.9624 - val_loss: 0.3964 - val_acc: 0.8766\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1692 - acc: 0.9659 - val_loss: 0.4894 - val_acc: 0.8418\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1548 - acc: 0.9701 - val_loss: 0.3165 - val_acc: 0.9035\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1438 - acc: 0.9705 - val_loss: 0.3402 - val_acc: 0.8987\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1458 - acc: 0.9708 - val_loss: 0.4315 - val_acc: 0.8718\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1435 - acc: 0.9685 - val_loss: 0.3353 - val_acc: 0.8956\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1353 - acc: 0.9710 - val_loss: 0.3991 - val_acc: 0.8671\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1283 - acc: 0.9713 - val_loss: 0.3328 - val_acc: 0.8924\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1252 - acc: 0.9719 - val_loss: 0.3373 - val_acc: 0.8987\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1120 - acc: 0.9757 - val_loss: 0.3656 - val_acc: 0.8861\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1155 - acc: 0.9773 - val_loss: 0.4063 - val_acc: 0.8766\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1206 - acc: 0.9745 - val_loss: 0.2999 - val_acc: 0.9035\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1078 - acc: 0.9766 - val_loss: 0.3214 - val_acc: 0.9003\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1027 - acc: 0.9784 - val_loss: 0.3244 - val_acc: 0.8940\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1064 - acc: 0.9782 - val_loss: 0.3601 - val_acc: 0.8924\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0922 - acc: 0.9826 - val_loss: 0.3613 - val_acc: 0.8861\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1009 - acc: 0.9796 - val_loss: 0.4029 - val_acc: 0.8813\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.1009 - acc: 0.9750 - val_loss: 0.4238 - val_acc: 0.8703\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0948 - acc: 0.9800 - val_loss: 0.4183 - val_acc: 0.8703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0919 - acc: 0.9782 - val_loss: 0.3260 - val_acc: 0.8987\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0871 - acc: 0.9824 - val_loss: 0.3548 - val_acc: 0.8924\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0807 - acc: 0.9835 - val_loss: 0.4030 - val_acc: 0.8813\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0807 - acc: 0.9838 - val_loss: 0.2741 - val_acc: 0.9209\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0804 - acc: 0.9826 - val_loss: 0.3088 - val_acc: 0.8924\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0785 - acc: 0.9831 - val_loss: 0.3059 - val_acc: 0.8987\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0807 - acc: 0.9833 - val_loss: 0.3692 - val_acc: 0.8924\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0703 - acc: 0.9854 - val_loss: 0.2734 - val_acc: 0.9098\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0673 - acc: 0.9859 - val_loss: 0.3349 - val_acc: 0.8940\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0708 - acc: 0.9847 - val_loss: 0.2909 - val_acc: 0.9066\n",
      "Epoch 71/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0735 - acc: 0.9835 - val_loss: 0.4699 - val_acc: 0.8671\n",
      "Epoch 72/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0663 - acc: 0.9851 - val_loss: 0.2803 - val_acc: 0.9082\n",
      "Epoch 73/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0695 - acc: 0.9821 - val_loss: 0.3022 - val_acc: 0.9161\n",
      "Epoch 74/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0651 - acc: 0.9845 - val_loss: 0.2855 - val_acc: 0.9146\n",
      "Epoch 75/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0652 - acc: 0.9856 - val_loss: 0.3185 - val_acc: 0.8940\n",
      "Epoch 76/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0682 - acc: 0.9840 - val_loss: 0.3283 - val_acc: 0.8972\n",
      "Epoch 77/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0585 - acc: 0.9856 - val_loss: 0.2981 - val_acc: 0.9051\n",
      "Epoch 78/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0641 - acc: 0.9854 - val_loss: 0.2900 - val_acc: 0.9130\n",
      "Epoch 79/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0627 - acc: 0.9856 - val_loss: 0.3345 - val_acc: 0.8987\n",
      "Epoch 80/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0591 - acc: 0.9873 - val_loss: 0.4097 - val_acc: 0.8972\n",
      "Epoch 81/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0584 - acc: 0.9872 - val_loss: 0.3163 - val_acc: 0.9019\n",
      "Epoch 82/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0586 - acc: 0.9863 - val_loss: 0.3309 - val_acc: 0.9051\n",
      "Epoch 83/500\n",
      "5688/5688 [==============================] - 6s 1ms/step - loss: 0.0487 - acc: 0.9893 - val_loss: 0.3410 - val_acc: 0.9066\n",
      "0.9098101265822784\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train,PseEIIP_train], Y_train, validation_data=([X_test,PseEIIP_test], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test,PseEIIP_test])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 13)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict([X_test,PseEIIP_test])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8686708860759493, 0.8584615384615385, 0.9005817633171317, 0.8644950615536616, 0.859739932167994)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "print(auc,recall,precision,fscore,mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "============================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=False,\n",
    "                        title='Unnormalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dme(f, k, xo):\n",
    "    x1o=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (xo)\n",
    "    x1o = BatchNormalization()(x1o)\n",
    "    x1o=Dropout(0.2)(x1o)\n",
    "    x1o=Activation('relu')(x1o)\n",
    "    #x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_1(xin, f, k):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_2(xin, f, k):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x12 = Concatenate(axis=-1)([x1,x11])\n",
    "    x12 = Conv1Dme(f1, k1, x12)\n",
    "    x13 = Concatenate(axis=-1)([x1,x11,x12])\n",
    "    x14 = Conv1Dme(f1, k1, x13)\n",
    "    x14=MaxPooling1D(pool_size=2, strides=2)(x14)\n",
    "    \n",
    "    return x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense1(): # Model\n",
    "    inputs1 = Input(shape=(750, 5))\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = dense_block_2(x1, 128, 3)\n",
    "    x1 = dense_block_2(x1, 256, 5)\n",
    "    x1 = dense_block_2(x1, 512, 5)\n",
    "    \n",
    "       \n",
    "    xf=Flatten()(x1)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=inputs1, outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           (None, 750, 5)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 750, 64)      1024        input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 750, 64)      256         conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)           (None, 750, 64)      0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 750, 64)      0           dropout_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 375, 64)      0           activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 375, 128)     24704       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 375, 128)     512         conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, 375, 128)     0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 375, 128)     0           dropout_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 375, 128)     49280       activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 375, 128)     512         conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, 375, 128)     0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 375, 128)     0           dropout_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 375, 256)     0           activation_131[0][0]             \n",
      "                                                                 activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 375, 128)     98432       concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 375, 128)     512         conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 375, 128)     0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 375, 128)     0           dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 375, 384)     0           activation_131[0][0]             \n",
      "                                                                 activation_132[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 375, 128)     147584      concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 375, 128)     512         conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 375, 128)     0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 375, 128)     0           dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 187, 128)     0           activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 187, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 187, 256)     1024        conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, 187, 256)     0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 187, 256)     0           dropout_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 187, 256)     327936      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 187, 256)     1024        conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, 187, 256)     0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 187, 256)     0           dropout_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 187, 512)     0           activation_135[0][0]             \n",
      "                                                                 activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 187, 256)     655616      concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 187, 256)     1024        conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, 187, 256)     0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 187, 256)     0           dropout_137[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 187, 768)     0           activation_135[0][0]             \n",
      "                                                                 activation_136[0][0]             \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 187, 256)     983296      concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 187, 256)     1024        conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)           (None, 187, 256)     0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 187, 256)     0           dropout_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 93, 256)      0           activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 93, 512)      655872      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 93, 512)      2048        conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)           (None, 93, 512)      0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 93, 512)      0           dropout_139[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 93, 512)      1311232     activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 93, 512)      2048        conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, 93, 512)      0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 93, 512)      0           dropout_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 93, 1024)     0           activation_139[0][0]             \n",
      "                                                                 activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 93, 512)      2621952     concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 93, 512)      2048        conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, 93, 512)      0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 93, 512)      0           dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 93, 1536)     0           activation_139[0][0]             \n",
      "                                                                 activation_140[0][0]             \n",
      "                                                                 activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 93, 512)      3932672     concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 93, 512)      2048        conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_142 (Dropout)           (None, 93, 512)      0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 93, 512)      0           dropout_142[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 46, 512)      0           activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 23552)        0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256)          6029568     flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 256)          1024        dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_143 (Dropout)           (None, 256)          0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 256)          0           dropout_143[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 64)           16448       activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 64)           256         dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_144 (Dropout)           (None, 64)           0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 64)           0           dropout_144[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 13)           845         activation_144[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 17,036,429\n",
      "Trainable params: 17,028,493\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 29s 5ms/step - loss: 2.1351 - acc: 0.2885 - val_loss: 3.0787 - val_acc: 0.1250\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.8445 - acc: 0.3922 - val_loss: 3.1558 - val_acc: 0.0807\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.6047 - acc: 0.4866 - val_loss: 2.5739 - val_acc: 0.1408\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.4185 - acc: 0.5621 - val_loss: 2.6189 - val_acc: 0.1250\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.2490 - acc: 0.6183 - val_loss: 2.5512 - val_acc: 0.1741\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.1313 - acc: 0.6535 - val_loss: 2.2317 - val_acc: 0.2848\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 1.0189 - acc: 0.6885 - val_loss: 1.9653 - val_acc: 0.3813\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.9414 - acc: 0.7208 - val_loss: 1.6088 - val_acc: 0.5190\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.8659 - acc: 0.7482 - val_loss: 1.3867 - val_acc: 0.5775\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.7984 - acc: 0.7628 - val_loss: 1.4669 - val_acc: 0.5649\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.7438 - acc: 0.7857 - val_loss: 1.6291 - val_acc: 0.4937\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.6896 - acc: 0.8005 - val_loss: 1.4912 - val_acc: 0.5491\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.6375 - acc: 0.8187 - val_loss: 1.2771 - val_acc: 0.6013\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.5811 - acc: 0.8379 - val_loss: 1.2467 - val_acc: 0.6218\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.5575 - acc: 0.8419 - val_loss: 0.9431 - val_acc: 0.6994\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.5030 - acc: 0.8597 - val_loss: 1.0165 - val_acc: 0.6851\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.4775 - acc: 0.8731 - val_loss: 0.8610 - val_acc: 0.7168\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.4240 - acc: 0.8870 - val_loss: 0.7381 - val_acc: 0.7579\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.4034 - acc: 0.8917 - val_loss: 0.8307 - val_acc: 0.7247\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.3871 - acc: 0.8945 - val_loss: 0.7054 - val_acc: 0.7769\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.3412 - acc: 0.9105 - val_loss: 0.6860 - val_acc: 0.7706\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.3203 - acc: 0.9130 - val_loss: 0.8459 - val_acc: 0.7326\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.3035 - acc: 0.9175 - val_loss: 0.7277 - val_acc: 0.7642\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.2760 - acc: 0.9263 - val_loss: 0.5320 - val_acc: 0.8196\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2519 - acc: 0.9378 - val_loss: 0.6733 - val_acc: 0.7927\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.2288 - acc: 0.9408 - val_loss: 0.6682 - val_acc: 0.7927\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.2146 - acc: 0.9446 - val_loss: 0.5443 - val_acc: 0.8307\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.2020 - acc: 0.9467 - val_loss: 0.7314 - val_acc: 0.7722\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1925 - acc: 0.9497 - val_loss: 0.4404 - val_acc: 0.8608\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1629 - acc: 0.9633 - val_loss: 0.4664 - val_acc: 0.8528\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1498 - acc: 0.9673 - val_loss: 0.4370 - val_acc: 0.8687\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1548 - acc: 0.9647 - val_loss: 0.4931 - val_acc: 0.8386\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1430 - acc: 0.9662 - val_loss: 0.4386 - val_acc: 0.8671\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1344 - acc: 0.9675 - val_loss: 0.4143 - val_acc: 0.8655\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1218 - acc: 0.9701 - val_loss: 0.4272 - val_acc: 0.8592\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1048 - acc: 0.9761 - val_loss: 0.5109 - val_acc: 0.8434\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1098 - acc: 0.9743 - val_loss: 0.4572 - val_acc: 0.8623\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.1055 - acc: 0.9756 - val_loss: 0.4342 - val_acc: 0.8544\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0904 - acc: 0.9815 - val_loss: 0.4558 - val_acc: 0.8671\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0895 - acc: 0.9794 - val_loss: 0.4576 - val_acc: 0.8592\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0820 - acc: 0.9828 - val_loss: 0.4328 - val_acc: 0.8687\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0743 - acc: 0.9824 - val_loss: 0.4554 - val_acc: 0.8718\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0710 - acc: 0.9835 - val_loss: 0.4596 - val_acc: 0.8687\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0657 - acc: 0.9859 - val_loss: 0.4183 - val_acc: 0.8845\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0674 - acc: 0.9829 - val_loss: 0.4132 - val_acc: 0.8861\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0744 - acc: 0.9831 - val_loss: 0.3773 - val_acc: 0.8877\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0626 - acc: 0.9856 - val_loss: 0.3936 - val_acc: 0.8797\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0526 - acc: 0.9895 - val_loss: 0.4052 - val_acc: 0.8734\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0610 - acc: 0.9856 - val_loss: 0.4195 - val_acc: 0.8734\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0497 - acc: 0.9902 - val_loss: 0.4724 - val_acc: 0.8703\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0538 - acc: 0.9873 - val_loss: 0.3705 - val_acc: 0.8877\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0631 - acc: 0.9842 - val_loss: 0.4933 - val_acc: 0.8560\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0577 - acc: 0.9859 - val_loss: 0.3734 - val_acc: 0.8892\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0514 - acc: 0.9875 - val_loss: 0.4369 - val_acc: 0.8592\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0556 - acc: 0.9863 - val_loss: 0.4441 - val_acc: 0.8829\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0536 - acc: 0.9877 - val_loss: 0.3951 - val_acc: 0.8877\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0402 - acc: 0.9907 - val_loss: 0.3840 - val_acc: 0.8845\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0435 - acc: 0.9893 - val_loss: 0.3562 - val_acc: 0.8972\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0407 - acc: 0.9880 - val_loss: 0.4358 - val_acc: 0.8687\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0388 - acc: 0.9914 - val_loss: 0.4313 - val_acc: 0.8845\n",
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0391 - acc: 0.9914 - val_loss: 0.4820 - val_acc: 0.8703\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0477 - acc: 0.9866 - val_loss: 0.4150 - val_acc: 0.8877\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0387 - acc: 0.9896 - val_loss: 0.4238 - val_acc: 0.8861\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0404 - acc: 0.9903 - val_loss: 0.3912 - val_acc: 0.8892\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0344 - acc: 0.9940 - val_loss: 0.4649 - val_acc: 0.8797\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0265 - acc: 0.9945 - val_loss: 0.3798 - val_acc: 0.8908\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0269 - acc: 0.9937 - val_loss: 0.4430 - val_acc: 0.8813\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0305 - acc: 0.9942 - val_loss: 0.3555 - val_acc: 0.9003\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0301 - acc: 0.9931 - val_loss: 0.4529 - val_acc: 0.8813\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0379 - acc: 0.9896 - val_loss: 0.5011 - val_acc: 0.8544\n",
      "Epoch 71/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0396 - acc: 0.9887 - val_loss: 0.5920 - val_acc: 0.8402\n",
      "Epoch 72/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0350 - acc: 0.9900 - val_loss: 0.4441 - val_acc: 0.8655\n",
      "Epoch 73/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0301 - acc: 0.9926 - val_loss: 0.4338 - val_acc: 0.8750\n",
      "Epoch 74/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0344 - acc: 0.9905 - val_loss: 0.3918 - val_acc: 0.8877\n",
      "Epoch 75/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0272 - acc: 0.9933 - val_loss: 0.4016 - val_acc: 0.8924\n",
      "Epoch 76/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0257 - acc: 0.9937 - val_loss: 0.4159 - val_acc: 0.8861\n",
      "Epoch 77/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0286 - acc: 0.9930 - val_loss: 0.3952 - val_acc: 0.8924\n",
      "Epoch 78/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0245 - acc: 0.9940 - val_loss: 0.4375 - val_acc: 0.8782\n",
      "Epoch 79/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0242 - acc: 0.9944 - val_loss: 0.3828 - val_acc: 0.8845\n",
      "Epoch 80/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0132 - acc: 0.9974 - val_loss: 0.4153 - val_acc: 0.8813\n",
      "Epoch 81/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0166 - acc: 0.9970 - val_loss: 0.4015 - val_acc: 0.8845\n",
      "Epoch 82/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0290 - acc: 0.9907 - val_loss: 0.5170 - val_acc: 0.8481\n",
      "Epoch 83/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0177 - acc: 0.9961 - val_loss: 0.4926 - val_acc: 0.8639\n",
      "0.9003164556962026\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict(X_test)\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9003164556962026\n",
      "0.8987341772151899\n",
      "0.9145569620253164\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense1()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict(X_test)\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense3(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 1))\n",
    "    \n",
    "    #x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    #x11 = Conv1Dme(128, 3, x1)\n",
    "    #x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    #x12 = Conv1Dme(128, 5, x1)\n",
    "    #x12=MaxPooling1D(pool_size=2, strides=2)(x12)\n",
    "    \n",
    "    #x1=keras.layers.concatenate([x11,x12],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    xx1 = Conv1Dme(128, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 3)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    #xxx = Conv1Dme(64, 3, xxx)\n",
    "    #xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIIP_train = X_train[:,:,4].reshape((5688,750,1))\n",
    "EIIP_test = X_test[:,:,4].reshape((632,750,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829113924050633\n",
      "0.9145569620253164\n",
      "0.9018987341772152\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense3()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],EIIP_train], Y_train, validation_data=([X_test[:,:,0:4],EIIP_test], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test[:,:,0:4],EIIP_test])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense3(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 1))\n",
    "    \n",
    "    #x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    #x11 = Conv1Dme(128, 3, x1)\n",
    "    #x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    #x12 = Conv1Dme(128, 5, x1)\n",
    "    #x12=MaxPooling1D(pool_size=2, strides=2)(x12)\n",
    "    \n",
    "    #x1=keras.layers.concatenate([x11,x12],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    xx1 = Conv1Dme(128, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 3)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    #xxx = Conv1Dme(64, 3, xxx)\n",
    "    #xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "    xxx = dense_block_1(xxx, 256, 4)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9098101265822784\n",
      "0.9193037974683544\n",
      "0.8686708860759493\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense3()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],EIIP_train], Y_train, validation_data=([X_test[:,:,0:4],EIIP_test], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test[:,:,0:4],EIIP_test])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.915, 0.9150000000000001, 0.9220413240067107, 0.916665162393206, 0.9082662126421891)\n",
      "(0.9315384615384615, 0.9315384615384616, 0.9331593811353157, 0.931907203029917, 0.9259088632747694)\n",
      "(0.9426923076923077, 0.9426923076923077, 0.9452197873278994, 0.943437027171514, 0.9380140808795279)\n",
      "(0.921923076923077, 0.9219230769230768, 0.9281114762854817, 0.9234762081507257, 0.9157172591031774)\n",
      "(0.9061538461538462, 0.9061538461538462, 0.909741918548561, 0.9063826843827978, 0.8985863843932758)\n",
      "(0.9334615384615385, 0.9334615384615383, 0.9353930776839665, 0.9335886276839146, 0.9280569268941917)\n",
      "(0.9273076923076923, 0.9273076923076923, 0.9278602661246911, 0.9269365523341532, 0.921360894819795)\n",
      "(0.9392307692307692, 0.9392307692307693, 0.9391988279054521, 0.9390493260908713, 0.9341954115831056)\n",
      "(0.9230769230769231, 0.9230769230769231, 0.9288368943598702, 0.9244435274863572, 0.9169501715326294)\n",
      "(0.9265384615384615, 0.9265384615384616, 0.9311721685414887, 0.9273044801954154, 0.9206858295617899)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "EIIP_val = X_val[:,:,4].reshape((2600,750,1))\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    EIIP_train = X_train[:,:,4].reshape((5688,750,1))\n",
    "    EIIP_test = X_test[:,:,4].reshape((632,750,1))\n",
    "    model = model_dense3()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],EIIP_train], Y_train, validation_data=([X_test[:,:,0:4],EIIP_test], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_val[:,:,0:4],EIIP_val])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266923076923076\n",
      "0.9266923076923078\n",
      "0.9300735121919436\n",
      "0.9273190798918872\n",
      "0.9207742034684452\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "t27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
