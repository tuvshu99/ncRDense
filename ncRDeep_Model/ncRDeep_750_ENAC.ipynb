{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import  Dense, Flatten, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, merge, Input,Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import  Bidirectional\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Test_Data_750.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Train_Data_750.h5']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot*.h5\")\n",
    "my_files8.sort()\n",
    "my_files8[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file8(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files8[fold_no*2+1],'r')\n",
    "    hf_Test = h5.File(my_files8[fold_no*2],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_files7 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS*.h5\")\n",
    "my_files7.sort()\n",
    "hf_val = h5.File(my_files7[20],'r')\n",
    "X_val = hf_val['Test_Data']     # Get test set\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_val['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)    #  Process the label of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/Test_enac_0.txt',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/Test_enac_1.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files6 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/T*\")\n",
    "my_files6.sort()\n",
    "my_files6[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enac(fold_no):\n",
    "    enac_test = my_files6[fold_no]\n",
    "    enac_train = my_files6[10+fold_no]\n",
    "    enac_test_lines = loadtxt(enac_test, delimiter=\"\\t\", unpack=False)\n",
    "    enac_train_lines = loadtxt(enac_train, delimiter=\"\\t\", unpack=False)\n",
    "    enac_test_lines = np.delete(enac_test_lines, 0, 1)\n",
    "    enac_train_lines = np.delete(enac_train_lines, 0, 1)\n",
    "    enac_test_lines = enac_test_lines.reshape(632,750,4)\n",
    "    enac_train_lines = enac_train_lines.reshape(5688,750,4)\n",
    "    return enac_train_lines, enac_test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "enac_val = my_files6[20]\n",
    "enac_val_lines = loadtxt(enac_test, delimiter=\"\\t\", unpack=False)\n",
    "enac_val_lines = np.delete(enac_val_lines, 0, 1)\n",
    "enac_val_lines.shape\n",
    "enac_val = enac_val_lines.reshape(2600,750,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enac, X_test_enac = get_enac(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5688, 750, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 10\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\\n============================\")\n",
    "    else:\n",
    "        #cm = np.asfarray(cm,float64)\n",
    "        print('Confusion matrix, without normalization\\n============================')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(5*2, 4*2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=FONT_SIZE)\n",
    "    plt.yticks(tick_marks, classes, fontsize=FONT_SIZE)\n",
    "    fmt = '.3f' if normalize else '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    fontsize=FONT_SIZE,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=FONT_SIZE)\n",
    "    plt.xlabel('Predicted label', fontsize=FONT_SIZE)\n",
    "    plt.savefig('Conf_mat_avg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5S_rRNA',\n",
       " '5.8S_rRNA',\n",
       " 'tRNA',\n",
       " 'ribozymes',\n",
       " 'CD-box',\n",
       " 'miRNA',\n",
       " 'Intron_gpI',\n",
       " 'Intron_gpII',\n",
       " 'HACA-box',\n",
       " 'riboswitch',\n",
       " 'IRES',\n",
       " 'leader',\n",
       " 'scaRNA']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class_names = [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]\n",
    "class_names = ['5S_rRNA', '5.8S_rRNA', 'tRNA', 'ribozymes', 'CD-box', 'miRNA', 'Intron_gpI', 'Intron_gpII', 'HACA-box', 'riboswitch', 'IRES', 'leader', 'scaRNA']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enac_train = '/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/train_0.txt'\n",
    "enac_test = '/home/chenming/ncrna/ncRDeep2/Data_Processing/ENAC/test_0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open(enac_test, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(f1,k1,f2,k2): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    x=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    #x=Conv1D(filters=32,kernel_size=2,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Activation('relu')(x)\n",
    "    #x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    xx=Conv1D(filters=f1,kernel_size=k1,strides=1,kernel_initializer=initializers.random_uniform()) (inputs2)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "\n",
    "    xx=Conv1D(filters=f2,kernel_size=k2,strides=1,kernel_initializer=initializers.random_uniform()) (xx)\n",
    "    xx = BatchNormalization()(xx)\n",
    "    xx=Dropout(0.2)(xx)\n",
    "    xx=Activation('relu')(xx)\n",
    "    xx=MaxPooling1D(pool_size=2, strides=2)(xx)\n",
    "    \n",
    "    x2=keras.layers.concatenate([xx,x],axis=1)\n",
    "    \n",
    "    x2=Conv1D(filters=64,kernel_size=4,strides=1,kernel_initializer=initializers.random_uniform()) (x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2=Dropout(0.2)(x2)\n",
    "    x2=Activation('relu')(x2)\n",
    "    x2=MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "\n",
    "    x2=Flatten()(x2)\n",
    "\n",
    "    x3=Dense(128,)(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(32,)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3=Dropout(0.3)(x3)\n",
    "    x3=Activation('relu')(x3)\n",
    "\n",
    "    x3=Dense(13, activation='softmax',  )(x3)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=x3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_file8(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 731, 512)     41472       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 731, 512)     41472       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 731, 512)     2048        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 731, 512)     2048        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 731, 512)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 731, 512)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 731, 512)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 731, 512)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 365, 512)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 365, 512)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 360, 128)     393344      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 360, 128)     393344      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 360, 128)     512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 360, 128)     512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 360, 128)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 360, 128)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 360, 128)     0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 360, 128)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 180, 128)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 180, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 360, 128)     0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 357, 64)      32832       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 357, 64)      256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 357, 64)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 357, 64)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 178, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 11392)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1458304     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32)           128         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 13)           429         activation_7[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,371,341\n",
      "Trainable params: 2,368,333\n",
      "Non-trainable params: 3,008\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_conv(512, 20, 128, 6)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9034810126582279, 0.8966346153846154, 0.9039164292408315, 0.8986894502030763, 0.8955190461493051)\n",
      "(0.8860759493670886, 0.8797115384615384, 0.8957641130300755, 0.8845257390907386, 0.8769092095537339)\n",
      "(0.9018987341772152, 0.8976923076923076, 0.9090404496552227, 0.9008900027747365, 0.893879982024359)\n",
      "(0.8829113924050633, 0.8783653846153846, 0.8909556128301426, 0.8811519547902112, 0.8739020480971251)\n",
      "(0.9082278481012658, 0.9029807692307693, 0.9095549917093473, 0.9045340881013865, 0.9007955593616094)\n",
      "(0.9113924050632911, 0.9060576923076925, 0.915237388502259, 0.9083809245094567, 0.9042317943346105)\n",
      "(0.8955696202531646, 0.8880769230769232, 0.90776315193804, 0.8920425493947399, 0.887748150325864)\n",
      "(0.8955696202531646, 0.8846153846153846, 0.9141236424639787, 0.8895294536639907, 0.8879407022562357)\n",
      "(0.9003164556962026, 0.8909615384615385, 0.9051717809693425, 0.8940580861349214, 0.8922514773782745)\n",
      "(0.9113924050632911, 0.9051923076923075, 0.9144921666198262, 0.9077987558940139, 0.9041190023678378)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    X_train_enac, X_test_enac = get_enac(i)\n",
    "    model = model_conv(512, 20, 128, 6)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],X_train_enac], Y_train, validation_data=([X_test[:,:,0:4],X_test_enac], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    #model.save_weights(\"Checkpoints/Dense_RNAfold_1003conv_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_test[:,:,0:4],X_test_enac])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8996835443037975\n",
      "0.8930288461538461\n",
      "0.9066019726959066\n",
      "0.8961601004557271\n",
      "0.8917296971848954\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 2.2454 - acc: 0.2683 - val_loss: 1.8645 - val_acc: 0.4509\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.8303 - acc: 0.4350 - val_loss: 1.7502 - val_acc: 0.5301\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.6258 - acc: 0.5156 - val_loss: 1.6530 - val_acc: 0.5981\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.5087 - acc: 0.5693 - val_loss: 1.6353 - val_acc: 0.5902\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.3932 - acc: 0.6079 - val_loss: 1.5094 - val_acc: 0.6598\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.3104 - acc: 0.6368 - val_loss: 1.4386 - val_acc: 0.6693\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.2201 - acc: 0.6749 - val_loss: 1.3416 - val_acc: 0.6946\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.1483 - acc: 0.6930 - val_loss: 1.2100 - val_acc: 0.7136\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 1.0754 - acc: 0.7247 - val_loss: 1.2234 - val_acc: 0.6994\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.9991 - acc: 0.7412 - val_loss: 1.1014 - val_acc: 0.7532\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.9430 - acc: 0.7635 - val_loss: 1.0529 - val_acc: 0.7547\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.8751 - acc: 0.7806 - val_loss: 0.9977 - val_acc: 0.7627\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.8164 - acc: 0.7973 - val_loss: 0.9035 - val_acc: 0.7943\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.7695 - acc: 0.8128 - val_loss: 0.8519 - val_acc: 0.7927\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.7158 - acc: 0.8288 - val_loss: 0.8652 - val_acc: 0.8054\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.6751 - acc: 0.8430 - val_loss: 0.7811 - val_acc: 0.8212\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.6100 - acc: 0.8646 - val_loss: 0.7238 - val_acc: 0.8228\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5681 - acc: 0.8703 - val_loss: 0.7329 - val_acc: 0.8133\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.5383 - acc: 0.8759 - val_loss: 0.7135 - val_acc: 0.8307\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4919 - acc: 0.8924 - val_loss: 0.6517 - val_acc: 0.8449\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4591 - acc: 0.9003 - val_loss: 0.6052 - val_acc: 0.8544\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4416 - acc: 0.9007 - val_loss: 0.6342 - val_acc: 0.8386\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.4131 - acc: 0.9082 - val_loss: 0.6660 - val_acc: 0.8196\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3851 - acc: 0.9175 - val_loss: 0.5733 - val_acc: 0.8560\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3658 - acc: 0.9170 - val_loss: 0.5414 - val_acc: 0.8623\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3313 - acc: 0.9272 - val_loss: 0.4867 - val_acc: 0.8592\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3233 - acc: 0.9302 - val_loss: 0.5083 - val_acc: 0.8528\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.3005 - acc: 0.9360 - val_loss: 0.4966 - val_acc: 0.8671\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2843 - acc: 0.9397 - val_loss: 0.4765 - val_acc: 0.8813\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2589 - acc: 0.9467 - val_loss: 0.4549 - val_acc: 0.8671\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2392 - acc: 0.9527 - val_loss: 0.4793 - val_acc: 0.8623\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2302 - acc: 0.9524 - val_loss: 0.4468 - val_acc: 0.8734\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2173 - acc: 0.9573 - val_loss: 0.4470 - val_acc: 0.8655\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2043 - acc: 0.9592 - val_loss: 0.4532 - val_acc: 0.8718\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.2051 - acc: 0.9568 - val_loss: 0.4256 - val_acc: 0.8750\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1861 - acc: 0.9664 - val_loss: 0.4307 - val_acc: 0.8797\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1704 - acc: 0.9662 - val_loss: 0.4567 - val_acc: 0.8639\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1727 - acc: 0.9664 - val_loss: 0.3906 - val_acc: 0.8829\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1622 - acc: 0.9664 - val_loss: 0.4792 - val_acc: 0.8576\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1618 - acc: 0.9675 - val_loss: 0.4806 - val_acc: 0.8560\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1516 - acc: 0.9712 - val_loss: 0.3684 - val_acc: 0.8987\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1398 - acc: 0.9694 - val_loss: 0.3777 - val_acc: 0.8877\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1315 - acc: 0.9750 - val_loss: 0.4257 - val_acc: 0.8782\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1321 - acc: 0.9736 - val_loss: 0.4066 - val_acc: 0.8877\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1266 - acc: 0.9764 - val_loss: 0.4004 - val_acc: 0.8813\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1320 - acc: 0.9724 - val_loss: 0.3950 - val_acc: 0.8797\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1204 - acc: 0.9749 - val_loss: 0.3972 - val_acc: 0.8797\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1034 - acc: 0.9789 - val_loss: 0.3635 - val_acc: 0.8940\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1086 - acc: 0.9756 - val_loss: 0.4746 - val_acc: 0.8655\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0984 - acc: 0.9800 - val_loss: 0.3818 - val_acc: 0.8892\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0997 - acc: 0.9824 - val_loss: 0.4256 - val_acc: 0.8845\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.1041 - acc: 0.9782 - val_loss: 0.3864 - val_acc: 0.8877\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0916 - acc: 0.9819 - val_loss: 0.3888 - val_acc: 0.8845\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0878 - acc: 0.9826 - val_loss: 0.4145 - val_acc: 0.8782\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0873 - acc: 0.9819 - val_loss: 0.4041 - val_acc: 0.8797\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0845 - acc: 0.9826 - val_loss: 0.3851 - val_acc: 0.8892\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0838 - acc: 0.9815 - val_loss: 0.4424 - val_acc: 0.8671\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0863 - acc: 0.9814 - val_loss: 0.3678 - val_acc: 0.8987\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0799 - acc: 0.9822 - val_loss: 0.4563 - val_acc: 0.8655\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0784 - acc: 0.9822 - val_loss: 0.3606 - val_acc: 0.8908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0789 - acc: 0.9833 - val_loss: 0.3969 - val_acc: 0.8940\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0681 - acc: 0.9856 - val_loss: 0.3698 - val_acc: 0.8908\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 9s 1ms/step - loss: 0.0685 - acc: 0.9854 - val_loss: 0.4011 - val_acc: 0.8908\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0686 - acc: 0.9836 - val_loss: 0.3505 - val_acc: 0.9082\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0648 - acc: 0.9879 - val_loss: 0.3845 - val_acc: 0.8892\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0716 - acc: 0.9838 - val_loss: 0.4015 - val_acc: 0.8940\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0659 - acc: 0.9868 - val_loss: 0.3491 - val_acc: 0.9082\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0686 - acc: 0.9840 - val_loss: 0.3654 - val_acc: 0.8956\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0603 - acc: 0.9865 - val_loss: 0.3154 - val_acc: 0.9161\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0582 - acc: 0.9875 - val_loss: 0.4116 - val_acc: 0.8797\n",
      "Epoch 71/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0669 - acc: 0.9831 - val_loss: 0.3736 - val_acc: 0.8972\n",
      "Epoch 72/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0620 - acc: 0.9845 - val_loss: 0.3552 - val_acc: 0.9019\n",
      "Epoch 73/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0576 - acc: 0.9880 - val_loss: 0.4375 - val_acc: 0.8845\n",
      "Epoch 74/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0556 - acc: 0.9868 - val_loss: 0.3459 - val_acc: 0.9051\n",
      "Epoch 75/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0476 - acc: 0.9912 - val_loss: 0.3713 - val_acc: 0.9098\n",
      "Epoch 76/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0525 - acc: 0.9870 - val_loss: 0.3881 - val_acc: 0.8987\n",
      "Epoch 77/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0574 - acc: 0.9875 - val_loss: 0.4026 - val_acc: 0.8924\n",
      "Epoch 78/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0553 - acc: 0.9889 - val_loss: 0.3402 - val_acc: 0.9082\n",
      "Epoch 79/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0457 - acc: 0.9917 - val_loss: 0.3386 - val_acc: 0.9019\n",
      "Epoch 80/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0455 - acc: 0.9919 - val_loss: 0.3988 - val_acc: 0.8877\n",
      "Epoch 81/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0526 - acc: 0.9863 - val_loss: 0.3756 - val_acc: 0.8877\n",
      "Epoch 82/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0548 - acc: 0.9877 - val_loss: 0.3993 - val_acc: 0.8877\n",
      "Epoch 83/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0493 - acc: 0.9886 - val_loss: 0.3800 - val_acc: 0.8940\n",
      "Epoch 84/500\n",
      "5688/5688 [==============================] - 8s 1ms/step - loss: 0.0466 - acc: 0.9896 - val_loss: 0.3817 - val_acc: 0.9019\n",
      "0.9161392405063291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train,enac_train_lines], Y_train, validation_data=([X_test,enac_test_lines], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test,enac_test_lines])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dme(f, k, xo):\n",
    "    x1o=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (xo)\n",
    "    x1o = BatchNormalization()(x1o)\n",
    "    x1o=Dropout(0.2)(x1o)\n",
    "    x1o=Activation('relu')(x1o)\n",
    "    #x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dmax(f, k, x):\n",
    "    x1=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1=Dropout(0.2)(x1)\n",
    "    x1=Activation('relu')(x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense1(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(746, 4))\n",
    "    \n",
    "    x1 = Conv1Dme(256, 20, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    f1 = 64\n",
    "\n",
    "    x1 = Conv1Dme(f1, 5, x1)\n",
    "    x11 = Conv1Dme(f1, 5, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, 5, x11)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    x1 = Conv1Dme(f1, 5, x1)\n",
    "    x11 = Conv1Dme(f1, 5, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, 5, x11)\n",
    "    x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "        \n",
    "    #x2 = Conv1Dme(f1, 7, x1)\n",
    "    #x21 = Conv1Dme(f1, 7, x2)\n",
    "    #x21 = Concatenate(axis=-1)([x2,x21])\n",
    "    #x21 = Conv1Dme(f1, 7, x21)\n",
    "    \n",
    "    #x3 = Concatenate(axis=-1)([x11,x21])\n",
    "    #x3 = Conv1Dme(64, 5, x3)\n",
    "    #x3=MaxPooling1D(pool_size=2, strides=2)(x3)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    \n",
    "    xx1 = Conv1Dme(256, 20, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st parallel dense block\n",
    "    \n",
    "    f1 = 64\n",
    "\n",
    "    xx1 = Conv1Dme(f1, 5, xx1)\n",
    "    xx11 = Conv1Dme(f1, 5, xx1)\n",
    "    xx11 = Concatenate(axis=-1)([xx1,xx11])\n",
    "    xx11 = Conv1Dme(f1, 5, xx11)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx11)\n",
    "    \n",
    "    xx1 = Conv1Dme(f1, 5, xx1)\n",
    "    xx11 = Conv1Dme(f1, 5, xx1)\n",
    "    xx11 = Concatenate(axis=-1)([xx1,xx11])\n",
    "    xx11 = Conv1Dme(f1, 5, xx11)\n",
    "    xx11=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "        \n",
    "    #xx2 = Conv1Dme(f1, 7, xx1)\n",
    "    #xx21 = Conv1Dme(f1, 7, xx2)\n",
    "    #xx21 = Concatenate(axis=-1)([xx2,xx21])\n",
    "    #xx21 = Conv1Dme(f1, 7, xx21)\n",
    "    \n",
    "    #xx3 = Concatenate(axis=-1)([xx11,xx21])\n",
    "    #xx3 = Conv1Dme(64, 5, xx3)\n",
    "    #xx3=MaxPooling1D(pool_size=2, strides=2)(xx3)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          FC block\n",
    "    \n",
    "    xxx=keras.layers.concatenate([xx11,x11],axis=1)\n",
    "    \n",
    "    xxx = Conv1Dme(32, 3, xxx)\n",
    "    xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "    \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(128,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(32,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 750, 256)     20736       input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 750, 256)     1024        conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)           (None, 750, 256)     0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 750, 256)     0           dropout_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 375, 256)     0           activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, 375, 64)      81984       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 375, 64)      256         conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_154 (Dropout)           (None, 375, 64)      0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 746, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 375, 64)      0           dropout_154[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 746, 256)     20736       input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, 375, 64)      20544       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 746, 256)     1024        conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 375, 64)      256         conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_160 (Dropout)           (None, 746, 256)     0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_155 (Dropout)           (None, 375, 64)      0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 746, 256)     0           dropout_160[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 375, 64)      0           dropout_155[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 373, 256)     0           activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 375, 128)     0           activation_154[0][0]             \n",
      "                                                                 activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 373, 64)      81984       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, 375, 64)      41024       concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 373, 64)      256         conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 375, 64)      256         conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_161 (Dropout)           (None, 373, 64)      0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_156 (Dropout)           (None, 375, 64)      0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 373, 64)      0           dropout_161[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 375, 64)      0           dropout_156[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 373, 64)      20544       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 187, 64)      0           activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 373, 64)      256         conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, 187, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_162 (Dropout)           (None, 373, 64)      0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 187, 64)      256         conv1d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 373, 64)      0           dropout_162[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_157 (Dropout)           (None, 187, 64)      0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 373, 128)     0           activation_161[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 187, 64)      0           dropout_157[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 373, 64)      41024       concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, 187, 64)      20544       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 373, 64)      256         conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 187, 64)      256         conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_163 (Dropout)           (None, 373, 64)      0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_158 (Dropout)           (None, 187, 64)      0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 373, 64)      0           dropout_163[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 187, 64)      0           dropout_158[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 186, 64)      0           activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 187, 128)     0           activation_157[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 186, 64)      20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, 187, 64)      41024       concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 186, 64)      256         conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 187, 64)      256         conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_164 (Dropout)           (None, 186, 64)      0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_159 (Dropout)           (None, 187, 64)      0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 186, 64)      0           dropout_164[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 187, 64)      0           dropout_159[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 93, 64)       0           activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 93, 64)       0           activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 186, 64)      0           max_pooling1d_46[0][0]           \n",
      "                                                                 max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_151 (Conv1D)             (None, 186, 32)      6176        concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 186, 32)      128         conv1d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_167 (Dropout)           (None, 186, 32)      0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 186, 32)      0           dropout_167[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 93, 32)       0           activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2976)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          381056      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 128)          512         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_168 (Dropout)           (None, 128)          0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 128)          0           dropout_168[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 32)           4128        activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 32)           128         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_169 (Dropout)           (None, 32)           0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 32)           0           dropout_169[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 13)           429         activation_169[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 828,397\n",
      "Trainable params: 825,709\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 43s 8ms/step - loss: 2.2085 - acc: 0.2716 - val_loss: 2.2845 - val_acc: 0.2089\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 1.7199 - acc: 0.4564 - val_loss: 2.0706 - val_acc: 0.3766\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 1.4677 - acc: 0.5522 - val_loss: 1.9126 - val_acc: 0.4146\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 1.2957 - acc: 0.5962 - val_loss: 1.8848 - val_acc: 0.3608\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 1.1773 - acc: 0.6399 - val_loss: 1.8671 - val_acc: 0.3481\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 1.0780 - acc: 0.6728 - val_loss: 1.7614 - val_acc: 0.4066\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.9812 - acc: 0.7133 - val_loss: 1.6336 - val_acc: 0.4509\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.9223 - acc: 0.7294 - val_loss: 1.5364 - val_acc: 0.5032\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.8370 - acc: 0.7583 - val_loss: 1.4436 - val_acc: 0.5538\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.7844 - acc: 0.7736 - val_loss: 1.3488 - val_acc: 0.5934\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.7246 - acc: 0.7945 - val_loss: 1.2864 - val_acc: 0.5902\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.6895 - acc: 0.8054 - val_loss: 1.1991 - val_acc: 0.6408\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.6454 - acc: 0.8187 - val_loss: 1.1300 - val_acc: 0.6630\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.5988 - acc: 0.8316 - val_loss: 1.0604 - val_acc: 0.6788\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.5500 - acc: 0.8481 - val_loss: 1.0187 - val_acc: 0.6946\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.5309 - acc: 0.8514 - val_loss: 0.9985 - val_acc: 0.6883\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.4905 - acc: 0.8560 - val_loss: 0.8256 - val_acc: 0.7500\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.4590 - acc: 0.8708 - val_loss: 1.0272 - val_acc: 0.6804\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.4513 - acc: 0.8718 - val_loss: 0.8429 - val_acc: 0.7468\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.4157 - acc: 0.8841 - val_loss: 0.8204 - val_acc: 0.7484\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.3904 - acc: 0.8940 - val_loss: 0.7398 - val_acc: 0.7579\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.3621 - acc: 0.8982 - val_loss: 0.7233 - val_acc: 0.7674\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.3453 - acc: 0.9035 - val_loss: 0.6886 - val_acc: 0.7927\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.3207 - acc: 0.9112 - val_loss: 0.6607 - val_acc: 0.7943\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2973 - acc: 0.9195 - val_loss: 0.6332 - val_acc: 0.8212\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2978 - acc: 0.9195 - val_loss: 0.4944 - val_acc: 0.8481\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2682 - acc: 0.9300 - val_loss: 0.6401 - val_acc: 0.7959\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2506 - acc: 0.9348 - val_loss: 0.5609 - val_acc: 0.8275\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2402 - acc: 0.9367 - val_loss: 0.6435 - val_acc: 0.7975\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2316 - acc: 0.9378 - val_loss: 0.5869 - val_acc: 0.8149\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2193 - acc: 0.9415 - val_loss: 0.5141 - val_acc: 0.8434\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.2023 - acc: 0.9471 - val_loss: 0.4186 - val_acc: 0.8782\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1927 - acc: 0.9462 - val_loss: 0.5149 - val_acc: 0.8402\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1858 - acc: 0.9504 - val_loss: 0.5522 - val_acc: 0.8323\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1764 - acc: 0.9562 - val_loss: 0.5899 - val_acc: 0.8070\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1661 - acc: 0.9560 - val_loss: 0.5318 - val_acc: 0.8275\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1643 - acc: 0.9580 - val_loss: 0.4474 - val_acc: 0.8544\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1526 - acc: 0.9608 - val_loss: 0.4937 - val_acc: 0.8418\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1484 - acc: 0.9627 - val_loss: 0.4212 - val_acc: 0.8513\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1327 - acc: 0.9657 - val_loss: 0.5612 - val_acc: 0.8291\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 16s 3ms/step - loss: 0.1246 - acc: 0.9685 - val_loss: 0.5799 - val_acc: 0.8006\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1260 - acc: 0.9678 - val_loss: 0.4629 - val_acc: 0.8465\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1114 - acc: 0.9745 - val_loss: 0.5374 - val_acc: 0.8354\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1110 - acc: 0.9705 - val_loss: 0.5914 - val_acc: 0.8212\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1078 - acc: 0.9735 - val_loss: 0.4290 - val_acc: 0.8687\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1006 - acc: 0.9731 - val_loss: 0.4561 - val_acc: 0.8592\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 17s 3ms/step - loss: 0.1007 - acc: 0.9743 - val_loss: 0.5623 - val_acc: 0.8339\n",
      "0.8781645569620253\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train,enac_train_lines], Y_train, validation_data=([X_test,enac_test_lines], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test,enac_test_lines])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_1(xin, f, k):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense1(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(746, 4))\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 4)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    xx1 = Conv1Dme(64, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 4)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    xxx = Conv1Dme(64, 3, xxx)\n",
    "    xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_73 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_74 (InputLayer)           (None, 746, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_538 (Conv1D)             (None, 750, 64)      832         input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_551 (Conv1D)             (None, 746, 64)      832         input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 750, 64)      256         conv1d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 746, 64)      256         conv1d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_614 (Dropout)           (None, 750, 64)      0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_627 (Dropout)           (None, 746, 64)      0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 750, 64)      0           dropout_614[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 746, 64)      0           dropout_627[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_202 (MaxPooling1D (None, 375, 64)      0           activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_206 (MaxPooling1D (None, 373, 64)      0           activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_539 (Conv1D)             (None, 375, 128)     32896       max_pooling1d_202[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_552 (Conv1D)             (None, 373, 128)     32896       max_pooling1d_206[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 375, 128)     512         conv1d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 373, 128)     512         conv1d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_615 (Dropout)           (None, 375, 128)     0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_628 (Dropout)           (None, 373, 128)     0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 375, 128)     0           dropout_615[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 373, 128)     0           dropout_628[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_540 (Conv1D)             (None, 375, 128)     65664       activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_553 (Conv1D)             (None, 373, 128)     65664       activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 375, 128)     512         conv1d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 373, 128)     512         conv1d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_616 (Dropout)           (None, 375, 128)     0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_629 (Dropout)           (None, 373, 128)     0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 375, 128)     0           dropout_616[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 373, 128)     0           dropout_629[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_238 (Concatenate)   (None, 375, 256)     0           activation_615[0][0]             \n",
      "                                                                 activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_244 (Concatenate)   (None, 373, 256)     0           activation_628[0][0]             \n",
      "                                                                 activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_541 (Conv1D)             (None, 375, 128)     131200      concatenate_238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_554 (Conv1D)             (None, 373, 128)     131200      concatenate_244[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 375, 128)     512         conv1d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 373, 128)     512         conv1d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_617 (Dropout)           (None, 375, 128)     0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_630 (Dropout)           (None, 373, 128)     0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 375, 128)     0           dropout_617[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 373, 128)     0           dropout_630[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_239 (Concatenate)   (None, 375, 256)     0           activation_615[0][0]             \n",
      "                                                                 activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_245 (Concatenate)   (None, 373, 256)     0           activation_628[0][0]             \n",
      "                                                                 activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_542 (Conv1D)             (None, 375, 128)     131200      concatenate_239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_555 (Conv1D)             (None, 373, 128)     131200      concatenate_245[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 375, 128)     512         conv1d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 373, 128)     512         conv1d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_618 (Dropout)           (None, 375, 128)     0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_631 (Dropout)           (None, 373, 128)     0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 375, 128)     0           dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 373, 128)     0           dropout_631[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_203 (MaxPooling1D (None, 187, 128)     0           activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_207 (MaxPooling1D (None, 186, 128)     0           activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_543 (Conv1D)             (None, 187, 256)     164096      max_pooling1d_203[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_556 (Conv1D)             (None, 186, 256)     164096      max_pooling1d_207[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 187, 256)     1024        conv1d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 186, 256)     1024        conv1d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_619 (Dropout)           (None, 187, 256)     0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_632 (Dropout)           (None, 186, 256)     0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 187, 256)     0           dropout_619[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 186, 256)     0           dropout_632[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_544 (Conv1D)             (None, 187, 256)     327936      activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_557 (Conv1D)             (None, 186, 256)     327936      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 187, 256)     1024        conv1d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 186, 256)     1024        conv1d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_620 (Dropout)           (None, 187, 256)     0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_633 (Dropout)           (None, 186, 256)     0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 187, 256)     0           dropout_620[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 186, 256)     0           dropout_633[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_240 (Concatenate)   (None, 187, 512)     0           activation_619[0][0]             \n",
      "                                                                 activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_246 (Concatenate)   (None, 186, 512)     0           activation_632[0][0]             \n",
      "                                                                 activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_545 (Conv1D)             (None, 187, 256)     655616      concatenate_240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_558 (Conv1D)             (None, 186, 256)     655616      concatenate_246[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 187, 256)     1024        conv1d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 186, 256)     1024        conv1d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_621 (Dropout)           (None, 187, 256)     0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_634 (Dropout)           (None, 186, 256)     0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 187, 256)     0           dropout_621[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 186, 256)     0           dropout_634[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_241 (Concatenate)   (None, 187, 512)     0           activation_619[0][0]             \n",
      "                                                                 activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_247 (Concatenate)   (None, 186, 512)     0           activation_632[0][0]             \n",
      "                                                                 activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_546 (Conv1D)             (None, 187, 256)     655616      concatenate_241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_559 (Conv1D)             (None, 186, 256)     655616      concatenate_247[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 187, 256)     1024        conv1d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 186, 256)     1024        conv1d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_622 (Dropout)           (None, 187, 256)     0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_635 (Dropout)           (None, 186, 256)     0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 187, 256)     0           dropout_622[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 186, 256)     0           dropout_635[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_204 (MaxPooling1D (None, 93, 256)      0           activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_208 (MaxPooling1D (None, 93, 256)      0           activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_547 (Conv1D)             (None, 93, 256)      327936      max_pooling1d_204[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_560 (Conv1D)             (None, 93, 256)      327936      max_pooling1d_208[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 93, 256)      1024        conv1d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 93, 256)      1024        conv1d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_623 (Dropout)           (None, 93, 256)      0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_636 (Dropout)           (None, 93, 256)      0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 93, 256)      0           dropout_623[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 93, 256)      0           dropout_636[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_548 (Conv1D)             (None, 93, 256)      327936      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_561 (Conv1D)             (None, 93, 256)      327936      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 93, 256)      1024        conv1d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 93, 256)      1024        conv1d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_624 (Dropout)           (None, 93, 256)      0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_637 (Dropout)           (None, 93, 256)      0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 93, 256)      0           dropout_624[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 93, 256)      0           dropout_637[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_242 (Concatenate)   (None, 93, 512)      0           activation_623[0][0]             \n",
      "                                                                 activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_248 (Concatenate)   (None, 93, 512)      0           activation_636[0][0]             \n",
      "                                                                 activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_549 (Conv1D)             (None, 93, 256)      655616      concatenate_242[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_562 (Conv1D)             (None, 93, 256)      655616      concatenate_248[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 93, 256)      1024        conv1d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 93, 256)      1024        conv1d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_625 (Dropout)           (None, 93, 256)      0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_638 (Dropout)           (None, 93, 256)      0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 93, 256)      0           dropout_625[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 93, 256)      0           dropout_638[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_243 (Concatenate)   (None, 93, 512)      0           activation_623[0][0]             \n",
      "                                                                 activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_249 (Concatenate)   (None, 93, 512)      0           activation_636[0][0]             \n",
      "                                                                 activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_550 (Conv1D)             (None, 93, 256)      655616      concatenate_243[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_563 (Conv1D)             (None, 93, 256)      655616      concatenate_249[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 93, 256)      1024        conv1d_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 93, 256)      1024        conv1d_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_626 (Dropout)           (None, 93, 256)      0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_639 (Dropout)           (None, 93, 256)      0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 93, 256)      0           dropout_626[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 93, 256)      0           dropout_639[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_205 (MaxPooling1D (None, 46, 256)      0           activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_209 (MaxPooling1D (None, 46, 256)      0           activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_250 (Concatenate)   (None, 46, 512)      0           max_pooling1d_205[0][0]          \n",
      "                                                                 max_pooling1d_209[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_564 (Conv1D)             (None, 46, 64)       98368       concatenate_250[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 46, 64)       256         conv1d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_640 (Dropout)           (None, 46, 64)       0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 46, 64)       0           dropout_640[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_210 (MaxPooling1D (None, 23, 64)       0           activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 1472)         0           max_pooling1d_210[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 256)          377088      flatten_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 256)          1024        dense_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_641 (Dropout)           (None, 256)          0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 256)          0           dropout_641[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 64)           16448       activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 64)           256         dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_642 (Dropout)           (None, 64)           0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 64)           0           dropout_642[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 13)           845         activation_642[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,779,597\n",
      "Trainable params: 8,768,333\n",
      "Non-trainable params: 11,264\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 74s 13ms/step - loss: 2.4207 - acc: 0.1999 - val_loss: 2.8186 - val_acc: 0.0902\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 2.0059 - acc: 0.3477 - val_loss: 2.8739 - val_acc: 0.0791\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.7967 - acc: 0.4255 - val_loss: 2.8615 - val_acc: 0.1266\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.6242 - acc: 0.4759 - val_loss: 3.0503 - val_acc: 0.1218\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.4844 - acc: 0.5390 - val_loss: 2.9217 - val_acc: 0.1392\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.3796 - acc: 0.5703 - val_loss: 2.9495 - val_acc: 0.1377\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.2810 - acc: 0.6002 - val_loss: 2.9831 - val_acc: 0.1282\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.2205 - acc: 0.6120 - val_loss: 2.8469 - val_acc: 0.1155\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.1324 - acc: 0.6370 - val_loss: 2.9304 - val_acc: 0.1203\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.0756 - acc: 0.6595 - val_loss: 2.8704 - val_acc: 0.1472\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 1.0184 - acc: 0.6848 - val_loss: 2.5498 - val_acc: 0.1915\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.9684 - acc: 0.7015 - val_loss: 2.0912 - val_acc: 0.3275\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.9200 - acc: 0.7089 - val_loss: 2.1760 - val_acc: 0.2911\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.8780 - acc: 0.7301 - val_loss: 2.2141 - val_acc: 0.3006\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.8268 - acc: 0.7484 - val_loss: 2.0486 - val_acc: 0.3165\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7790 - acc: 0.7572 - val_loss: 1.7531 - val_acc: 0.4304\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7498 - acc: 0.7686 - val_loss: 1.7436 - val_acc: 0.4272\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.7112 - acc: 0.7820 - val_loss: 1.6726 - val_acc: 0.4699\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.6822 - acc: 0.7911 - val_loss: 1.4850 - val_acc: 0.5253\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.6500 - acc: 0.8015 - val_loss: 1.3898 - val_acc: 0.5443\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5998 - acc: 0.8194 - val_loss: 1.6542 - val_acc: 0.4636\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5781 - acc: 0.8224 - val_loss: 1.2935 - val_acc: 0.5918\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5553 - acc: 0.8296 - val_loss: 1.3766 - val_acc: 0.5506\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5289 - acc: 0.8448 - val_loss: 0.8742 - val_acc: 0.7215\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.5067 - acc: 0.8539 - val_loss: 1.0402 - val_acc: 0.6741\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4716 - acc: 0.8609 - val_loss: 0.9719 - val_acc: 0.6883\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4614 - acc: 0.8632 - val_loss: 0.8593 - val_acc: 0.7247\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4414 - acc: 0.8746 - val_loss: 0.9269 - val_acc: 0.7057\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4178 - acc: 0.8734 - val_loss: 1.0341 - val_acc: 0.6598\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.4014 - acc: 0.8848 - val_loss: 0.8350 - val_acc: 0.7453\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3826 - acc: 0.8906 - val_loss: 0.7451 - val_acc: 0.7722\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3610 - acc: 0.8922 - val_loss: 0.6873 - val_acc: 0.7848\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3504 - acc: 0.9015 - val_loss: 0.8867 - val_acc: 0.7184\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3270 - acc: 0.9040 - val_loss: 0.6999 - val_acc: 0.7627\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.3133 - acc: 0.9052 - val_loss: 0.7378 - val_acc: 0.7737\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2844 - acc: 0.9205 - val_loss: 0.6373 - val_acc: 0.7975\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2762 - acc: 0.9172 - val_loss: 0.5701 - val_acc: 0.8244\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2624 - acc: 0.9205 - val_loss: 0.5083 - val_acc: 0.8307\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2589 - acc: 0.9221 - val_loss: 0.6018 - val_acc: 0.8022\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2574 - acc: 0.9255 - val_loss: 0.5843 - val_acc: 0.8228\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2371 - acc: 0.9306 - val_loss: 0.6025 - val_acc: 0.8054\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2197 - acc: 0.9372 - val_loss: 0.5780 - val_acc: 0.8101\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2128 - acc: 0.9416 - val_loss: 0.4823 - val_acc: 0.8449\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.2122 - acc: 0.9416 - val_loss: 0.4197 - val_acc: 0.8608\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1928 - acc: 0.9448 - val_loss: 0.4082 - val_acc: 0.8623\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1763 - acc: 0.9494 - val_loss: 0.5968 - val_acc: 0.8054\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1667 - acc: 0.9543 - val_loss: 0.4885 - val_acc: 0.8465\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1704 - acc: 0.9518 - val_loss: 0.4375 - val_acc: 0.8465\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1687 - acc: 0.9490 - val_loss: 0.4368 - val_acc: 0.8671\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1559 - acc: 0.9568 - val_loss: 0.4806 - val_acc: 0.8481\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1439 - acc: 0.9580 - val_loss: 0.4030 - val_acc: 0.8718\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1411 - acc: 0.9622 - val_loss: 0.4363 - val_acc: 0.8639\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1416 - acc: 0.9611 - val_loss: 0.4270 - val_acc: 0.8671\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1203 - acc: 0.9662 - val_loss: 0.4743 - val_acc: 0.8481\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1166 - acc: 0.9680 - val_loss: 0.5228 - val_acc: 0.8323\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1118 - acc: 0.9706 - val_loss: 0.3525 - val_acc: 0.8845\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1198 - acc: 0.9645 - val_loss: 0.4781 - val_acc: 0.8402\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1072 - acc: 0.9694 - val_loss: 0.5072 - val_acc: 0.8481\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1155 - acc: 0.9652 - val_loss: 0.4136 - val_acc: 0.8750\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1131 - acc: 0.9677 - val_loss: 0.5266 - val_acc: 0.8497\n",
      "Epoch 61/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.1003 - acc: 0.9726 - val_loss: 0.3605 - val_acc: 0.8908\n",
      "Epoch 62/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0972 - acc: 0.9708 - val_loss: 0.4639 - val_acc: 0.8608\n",
      "Epoch 63/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0788 - acc: 0.9801 - val_loss: 0.4426 - val_acc: 0.8576\n",
      "Epoch 64/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0920 - acc: 0.9724 - val_loss: 0.3585 - val_acc: 0.8940\n",
      "Epoch 65/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0781 - acc: 0.9798 - val_loss: 0.3108 - val_acc: 0.8972\n",
      "Epoch 66/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0881 - acc: 0.9742 - val_loss: 0.3307 - val_acc: 0.8940\n",
      "Epoch 67/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0786 - acc: 0.9787 - val_loss: 0.3372 - val_acc: 0.9003\n",
      "Epoch 68/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0808 - acc: 0.9780 - val_loss: 0.3910 - val_acc: 0.8908\n",
      "Epoch 69/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0772 - acc: 0.9780 - val_loss: 0.4848 - val_acc: 0.8544\n",
      "Epoch 70/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0699 - acc: 0.9803 - val_loss: 0.3700 - val_acc: 0.8845\n",
      "Epoch 71/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0718 - acc: 0.9777 - val_loss: 0.4106 - val_acc: 0.8813\n",
      "Epoch 72/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0689 - acc: 0.9810 - val_loss: 0.3662 - val_acc: 0.8797\n",
      "Epoch 73/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0588 - acc: 0.9851 - val_loss: 0.3522 - val_acc: 0.9019\n",
      "Epoch 74/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.3971 - val_acc: 0.8750\n",
      "Epoch 75/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0591 - acc: 0.9840 - val_loss: 0.4130 - val_acc: 0.8687\n",
      "Epoch 76/500\n",
      "5688/5688 [==============================] - 24s 4ms/step - loss: 0.0669 - acc: 0.9810 - val_loss: 0.3750 - val_acc: 0.8924\n",
      "Epoch 77/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0586 - acc: 0.9824 - val_loss: 0.2969 - val_acc: 0.9098\n",
      "Epoch 78/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0602 - acc: 0.9836 - val_loss: 0.3173 - val_acc: 0.9003\n",
      "Epoch 79/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0601 - acc: 0.9826 - val_loss: 0.3597 - val_acc: 0.8972\n",
      "Epoch 80/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0556 - acc: 0.9836 - val_loss: 0.3477 - val_acc: 0.8877\n",
      "Epoch 81/500\n",
      "5688/5688 [==============================] - 22s 4ms/step - loss: 0.0634 - acc: 0.9803 - val_loss: 0.4140 - val_acc: 0.8734\n",
      "Epoch 82/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0471 - acc: 0.9859 - val_loss: 0.5931 - val_acc: 0.8370\n",
      "Epoch 83/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0520 - acc: 0.9844 - val_loss: 0.3631 - val_acc: 0.9019\n",
      "Epoch 84/500\n",
      "5688/5688 [==============================] - 22s 4ms/step - loss: 0.0569 - acc: 0.9842 - val_loss: 0.4429 - val_acc: 0.8734\n",
      "Epoch 85/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0529 - acc: 0.9870 - val_loss: 0.4291 - val_acc: 0.8797\n",
      "Epoch 86/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0473 - acc: 0.9879 - val_loss: 0.4184 - val_acc: 0.8829\n",
      "Epoch 87/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0565 - acc: 0.9835 - val_loss: 0.4315 - val_acc: 0.8829\n",
      "Epoch 88/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0473 - acc: 0.9880 - val_loss: 0.4001 - val_acc: 0.8750\n",
      "Epoch 89/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0504 - acc: 0.9852 - val_loss: 0.3009 - val_acc: 0.9098\n",
      "Epoch 90/500\n",
      "5688/5688 [==============================] - 22s 4ms/step - loss: 0.0483 - acc: 0.9865 - val_loss: 0.4009 - val_acc: 0.8829\n",
      "Epoch 91/500\n",
      "5688/5688 [==============================] - 22s 4ms/step - loss: 0.0466 - acc: 0.9865 - val_loss: 0.3736 - val_acc: 0.8892\n",
      "Epoch 92/500\n",
      "5688/5688 [==============================] - 23s 4ms/step - loss: 0.0422 - acc: 0.9879 - val_loss: 0.4745 - val_acc: 0.8718\n",
      "0.9098101265822784\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train,enac_train_lines], Y_train, validation_data=([X_test,enac_test_lines], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test,enac_test_lines])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "enac_pad = np.zeros((632,2,4))\n",
    "enac_pad2 = np.zeros((5688,2,4))\n",
    "enac_test_last = np.concatenate((enac_pad,enac_test_lines,enac_pad),axis=1)\n",
    "enac_train_last = np.concatenate((enac_pad2,enac_train_lines,enac_pad2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense2(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    #x11 = Conv1Dme(128, 3, x1)\n",
    "    #x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    #x12 = Conv1Dme(128, 5, x1)\n",
    "    #x12=MaxPooling1D(pool_size=2, strides=2)(x12)\n",
    "    \n",
    "    #x1=keras.layers.concatenate([x11,x12],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, x1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    #xx1 = Conv1Dme(64, 3, inputs2)\n",
    "    #xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #xx1 = dense_block_1(xx1, 128, 4)\n",
    "    #xx1 = dense_block_1(xx1, 256, 5)\n",
    "    #xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    #xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    #xxx = Conv1Dme(64, 3, xxx)\n",
    "    #xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "        \n",
    "    xf=Flatten()(x1)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_85 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_86 (InputLayer)           (None, 750, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_287 (Concatenate)   (None, 750, 8)       0           input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_631 (Conv1D)             (None, 750, 128)     3200        concatenate_287[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_719 (BatchN (None, 750, 128)     512         conv1d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_719 (Dropout)           (None, 750, 128)     0           batch_normalization_719[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 750, 128)     0           dropout_719[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_232 (MaxPooling1D (None, 375, 128)     0           activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_632 (Conv1D)             (None, 375, 128)     49280       max_pooling1d_232[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_720 (BatchN (None, 375, 128)     512         conv1d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_720 (Dropout)           (None, 375, 128)     0           batch_normalization_720[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 375, 128)     0           dropout_720[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_633 (Conv1D)             (None, 375, 128)     49280       activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_721 (BatchN (None, 375, 128)     512         conv1d_633[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_721 (Dropout)           (None, 375, 128)     0           batch_normalization_721[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 375, 128)     0           dropout_721[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_288 (Concatenate)   (None, 375, 256)     0           activation_720[0][0]             \n",
      "                                                                 activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_634 (Conv1D)             (None, 375, 128)     98432       concatenate_288[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_722 (BatchN (None, 375, 128)     512         conv1d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_722 (Dropout)           (None, 375, 128)     0           batch_normalization_722[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 375, 128)     0           dropout_722[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_289 (Concatenate)   (None, 375, 256)     0           activation_720[0][0]             \n",
      "                                                                 activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_635 (Conv1D)             (None, 375, 128)     98432       concatenate_289[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_723 (BatchN (None, 375, 128)     512         conv1d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_723 (Dropout)           (None, 375, 128)     0           batch_normalization_723[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 375, 128)     0           dropout_723[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_233 (MaxPooling1D (None, 187, 128)     0           activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_636 (Conv1D)             (None, 187, 256)     164096      max_pooling1d_233[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_724 (BatchN (None, 187, 256)     1024        conv1d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_724 (Dropout)           (None, 187, 256)     0           batch_normalization_724[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 187, 256)     0           dropout_724[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_637 (Conv1D)             (None, 187, 256)     327936      activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_725 (BatchN (None, 187, 256)     1024        conv1d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_725 (Dropout)           (None, 187, 256)     0           batch_normalization_725[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 187, 256)     0           dropout_725[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_290 (Concatenate)   (None, 187, 512)     0           activation_724[0][0]             \n",
      "                                                                 activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_638 (Conv1D)             (None, 187, 256)     655616      concatenate_290[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_726 (BatchN (None, 187, 256)     1024        conv1d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_726 (Dropout)           (None, 187, 256)     0           batch_normalization_726[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 187, 256)     0           dropout_726[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_291 (Concatenate)   (None, 187, 512)     0           activation_724[0][0]             \n",
      "                                                                 activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_639 (Conv1D)             (None, 187, 256)     655616      concatenate_291[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_727 (BatchN (None, 187, 256)     1024        conv1d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_727 (Dropout)           (None, 187, 256)     0           batch_normalization_727[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 187, 256)     0           dropout_727[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_234 (MaxPooling1D (None, 93, 256)      0           activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_640 (Conv1D)             (None, 93, 256)      327936      max_pooling1d_234[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_728 (BatchN (None, 93, 256)      1024        conv1d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_728 (Dropout)           (None, 93, 256)      0           batch_normalization_728[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 93, 256)      0           dropout_728[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_641 (Conv1D)             (None, 93, 256)      327936      activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 93, 256)      1024        conv1d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_729 (Dropout)           (None, 93, 256)      0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 93, 256)      0           dropout_729[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_292 (Concatenate)   (None, 93, 512)      0           activation_728[0][0]             \n",
      "                                                                 activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_642 (Conv1D)             (None, 93, 256)      655616      concatenate_292[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 93, 256)      1024        conv1d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_730 (Dropout)           (None, 93, 256)      0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 93, 256)      0           dropout_730[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_293 (Concatenate)   (None, 93, 512)      0           activation_728[0][0]             \n",
      "                                                                 activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_643 (Conv1D)             (None, 93, 256)      655616      concatenate_293[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 93, 256)      1024        conv1d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_731 (Dropout)           (None, 93, 256)      0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 93, 256)      0           dropout_731[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_235 (MaxPooling1D (None, 46, 256)      0           activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_46 (Flatten)            (None, 11776)        0           max_pooling1d_235[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, 256)          3014912     flatten_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 256)          1024        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_732 (Dropout)           (None, 256)          0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 256)          0           dropout_732[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 64)           16448       activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 64)           256         dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_733 (Dropout)           (None, 64)           0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 64)           0           dropout_733[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 13)           845         activation_733[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 7,113,229\n",
      "Trainable params: 7,107,213\n",
      "Non-trainable params: 6,016\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_dense2()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5688 samples, validate on 632 samples\n",
      "Epoch 1/500\n",
      "5688/5688 [==============================] - 65s 11ms/step - loss: 2.2802 - acc: 0.2523 - val_loss: 2.6809 - val_acc: 0.0886\n",
      "Epoch 2/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.9324 - acc: 0.3680 - val_loss: 2.8840 - val_acc: 0.0918\n",
      "Epoch 3/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.6966 - acc: 0.4580 - val_loss: 2.7198 - val_acc: 0.1408\n",
      "Epoch 4/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.4845 - acc: 0.5389 - val_loss: 2.3402 - val_acc: 0.2547\n",
      "Epoch 5/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.3157 - acc: 0.5942 - val_loss: 2.1528 - val_acc: 0.3196\n",
      "Epoch 6/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.2091 - acc: 0.6338 - val_loss: 1.9563 - val_acc: 0.3576\n",
      "Epoch 7/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.1002 - acc: 0.6695 - val_loss: 1.9191 - val_acc: 0.3750\n",
      "Epoch 8/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 1.0119 - acc: 0.6950 - val_loss: 1.5084 - val_acc: 0.5285\n",
      "Epoch 9/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.9486 - acc: 0.7141 - val_loss: 1.4292 - val_acc: 0.5316\n",
      "Epoch 10/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.8683 - acc: 0.7395 - val_loss: 1.4620 - val_acc: 0.5475\n",
      "Epoch 11/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.8179 - acc: 0.7641 - val_loss: 1.3033 - val_acc: 0.5918\n",
      "Epoch 12/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.7475 - acc: 0.7806 - val_loss: 1.2866 - val_acc: 0.5791\n",
      "Epoch 13/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.7254 - acc: 0.7887 - val_loss: 1.1636 - val_acc: 0.6361\n",
      "Epoch 14/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.6608 - acc: 0.8068 - val_loss: 1.1872 - val_acc: 0.6266\n",
      "Epoch 15/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.6116 - acc: 0.8268 - val_loss: 0.9952 - val_acc: 0.6835\n",
      "Epoch 16/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.5743 - acc: 0.8337 - val_loss: 0.9970 - val_acc: 0.6804\n",
      "Epoch 17/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.5411 - acc: 0.8467 - val_loss: 0.8859 - val_acc: 0.7500\n",
      "Epoch 18/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.5065 - acc: 0.8583 - val_loss: 0.8495 - val_acc: 0.7326\n",
      "Epoch 19/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.4743 - acc: 0.8676 - val_loss: 0.7818 - val_acc: 0.7500\n",
      "Epoch 20/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.4350 - acc: 0.8803 - val_loss: 0.6801 - val_acc: 0.7848\n",
      "Epoch 21/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.4028 - acc: 0.8864 - val_loss: 0.6791 - val_acc: 0.7816\n",
      "Epoch 22/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.3900 - acc: 0.8928 - val_loss: 0.6201 - val_acc: 0.8038\n",
      "Epoch 23/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.3452 - acc: 0.9061 - val_loss: 0.5816 - val_acc: 0.8259\n",
      "Epoch 24/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.3238 - acc: 0.9160 - val_loss: 0.7123 - val_acc: 0.7737\n",
      "Epoch 25/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2973 - acc: 0.9237 - val_loss: 0.5768 - val_acc: 0.8101\n",
      "Epoch 26/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.2954 - acc: 0.9184 - val_loss: 0.5078 - val_acc: 0.8307\n",
      "Epoch 27/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2673 - acc: 0.9290 - val_loss: 0.5027 - val_acc: 0.8449\n",
      "Epoch 28/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2492 - acc: 0.9360 - val_loss: 0.4772 - val_acc: 0.8703\n",
      "Epoch 29/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2335 - acc: 0.9385 - val_loss: 0.5028 - val_acc: 0.8449\n",
      "Epoch 30/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2280 - acc: 0.9390 - val_loss: 0.4811 - val_acc: 0.8592\n",
      "Epoch 31/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.2181 - acc: 0.9388 - val_loss: 0.4531 - val_acc: 0.8560\n",
      "Epoch 32/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1931 - acc: 0.9494 - val_loss: 0.5092 - val_acc: 0.8402\n",
      "Epoch 33/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1693 - acc: 0.9571 - val_loss: 0.4204 - val_acc: 0.8718\n",
      "Epoch 34/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1687 - acc: 0.9559 - val_loss: 0.4112 - val_acc: 0.8703\n",
      "Epoch 35/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1614 - acc: 0.9578 - val_loss: 0.4076 - val_acc: 0.8845\n",
      "Epoch 36/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1449 - acc: 0.9652 - val_loss: 0.4513 - val_acc: 0.8608\n",
      "Epoch 37/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1348 - acc: 0.9659 - val_loss: 0.3891 - val_acc: 0.8766\n",
      "Epoch 38/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1203 - acc: 0.9720 - val_loss: 0.4496 - val_acc: 0.8655\n",
      "Epoch 39/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1296 - acc: 0.9678 - val_loss: 0.4369 - val_acc: 0.8623\n",
      "Epoch 40/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1199 - acc: 0.9698 - val_loss: 0.4040 - val_acc: 0.8734\n",
      "Epoch 41/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1165 - acc: 0.9710 - val_loss: 0.4329 - val_acc: 0.8734\n",
      "Epoch 42/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.1057 - acc: 0.9731 - val_loss: 0.4310 - val_acc: 0.8687\n",
      "Epoch 43/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0929 - acc: 0.9800 - val_loss: 0.4247 - val_acc: 0.8734\n",
      "Epoch 44/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0870 - acc: 0.9794 - val_loss: 0.4150 - val_acc: 0.8797\n",
      "Epoch 45/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0875 - acc: 0.9801 - val_loss: 0.3390 - val_acc: 0.9019\n",
      "Epoch 46/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0783 - acc: 0.9831 - val_loss: 0.3999 - val_acc: 0.8861\n",
      "Epoch 47/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0730 - acc: 0.9847 - val_loss: 0.4316 - val_acc: 0.8782\n",
      "Epoch 48/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0790 - acc: 0.9808 - val_loss: 0.4173 - val_acc: 0.8861\n",
      "Epoch 49/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0795 - acc: 0.9812 - val_loss: 0.3736 - val_acc: 0.8924\n",
      "Epoch 50/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0760 - acc: 0.9798 - val_loss: 0.4264 - val_acc: 0.8813\n",
      "Epoch 51/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0738 - acc: 0.9810 - val_loss: 0.3866 - val_acc: 0.8892\n",
      "Epoch 52/500\n",
      "5688/5688 [==============================] - 14s 3ms/step - loss: 0.0678 - acc: 0.9828 - val_loss: 0.3817 - val_acc: 0.8972\n",
      "Epoch 53/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0648 - acc: 0.9838 - val_loss: 0.4992 - val_acc: 0.8576\n",
      "Epoch 54/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0654 - acc: 0.9840 - val_loss: 0.3480 - val_acc: 0.9082\n",
      "Epoch 55/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0581 - acc: 0.9852 - val_loss: 0.3919 - val_acc: 0.8908\n",
      "Epoch 56/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0529 - acc: 0.9880 - val_loss: 0.3767 - val_acc: 0.8972\n",
      "Epoch 57/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0496 - acc: 0.9886 - val_loss: 0.3846 - val_acc: 0.8940\n",
      "Epoch 58/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0474 - acc: 0.9884 - val_loss: 0.3903 - val_acc: 0.9019\n",
      "Epoch 59/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0515 - acc: 0.9882 - val_loss: 0.3749 - val_acc: 0.8908\n",
      "Epoch 60/500\n",
      "5688/5688 [==============================] - 15s 3ms/step - loss: 0.0497 - acc: 0.9882 - val_loss: 0.3719 - val_acc: 0.8924\n",
      "0.9018987341772152\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit([X_train,enac_train_last], Y_train, validation_data=([X_test,enac_test_last], Y_test), epochs=500, verbose=1, callbacks=[es])\n",
    "y = model.predict([X_test,enac_test_last])\n",
    "y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8639240506329114\n",
      "0.9066455696202531\n",
      "0.9113924050632911\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense2()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train,enac_train_last], Y_train, validation_data=([X_test,enac_test_last], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test,enac_test_last])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense3(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    #x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    #x11 = Conv1Dme(128, 3, x1)\n",
    "    #x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    #x12 = Conv1Dme(128, 5, x1)\n",
    "    #x12=MaxPooling1D(pool_size=2, strides=2)(x12)\n",
    "    \n",
    "    #x1=keras.layers.concatenate([x11,x12],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    xx1 = Conv1Dme(128, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 3)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    #xxx = Conv1Dme(64, 3, xxx)\n",
    "    #xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9098101265822784\n",
      "0.8987341772151899\n",
      "0.8892405063291139\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense3()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train,enac_train_last], Y_train, validation_data=([X_test,enac_test_last], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test,enac_test_last])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense3(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    #x1=keras.layers.concatenate([inputs1,inputs2],axis=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    #x11 = Conv1Dme(128, 3, x1)\n",
    "    #x11=MaxPooling1D(pool_size=2, strides=2)(x11)\n",
    "    \n",
    "    #x12 = Conv1Dme(128, 5, x1)\n",
    "    #x12=MaxPooling1D(pool_size=2, strides=2)(x12)\n",
    "    \n",
    "    #x1=keras.layers.concatenate([x11,x12],axis=-1)\n",
    "    \n",
    "    x1 = Conv1Dme(128, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 3)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    xx1 = Conv1Dme(128, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 3)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    xxx = Conv1Dme(64, 3, xxx)\n",
    "    xxx=MaxPooling1D(pool_size=2, strides=2)(xxx)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9018987341772152\n",
      "0.9066455696202531\n",
      "0.9003164556962026\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = model_dense3()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train,enac_train_last], Y_train, validation_data=([X_test,enac_test_last], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    y = model.predict([X_test,enac_test_last])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_test ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    print(auc)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9003164556962026, 0.8952884615384616, 0.902749341607792, 0.896544642700091, 0.8923078819058271)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "print(auc,recall,precision,fscore,mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "============================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAJGCAYAAADPieS3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi41LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvSM8oowAAIABJREFUeJzs3Xd8FVX+//HXByJFKQlNSAApUZAo0rEiTdElYEUURBFXdlfsdRV/ruXL2te+uroWFlEQWUVQKcLigihgpasgUUhAigmgQiDh8/vjTrJJbgghpNwb3s/HYx65c+acmffMvSSHM+WauyMiIiIih54qFR1ARERERCqGOoIiIiIihyh1BEVEREQOUeoIioiIiByi1BEUEREROUSpIygiIiJyiFJHUETKnJndY2avBa+bm9kvZla1lLeRYmZ9S3Odxdjmn8zsp2B/6h/Een4xs1alma2imNlyM+tZ0TlEpHhiKjqAiISYmQNHu/vqPGX3AInufmmFBStl7v4jUKuicxwsMzsM+Btwort/fTDrcveIPx5m9iqw3t3vKqqeuyeVTyIRKQ0aERSRfCxEvxv270igBrC8ooNEAjPTwIJIFNIve5EoYWY9zWy9md1sZpvMbIOZXZFn+atm9qyZvWdmO8xsoZm1zrP8ZDNbbGbbgp8n51k218zGmNnHwG9Aq6Ds/8xsQXDqcqqZ1Tez8Wa2PVhHizzreNLM1gXLPjez0/axHy3MzM0sxsxOCtadM+0ys5SgXhUz+7OZrTGzrWb2ppnVy7OeYWb2Q7Bs9H6OXU0zeyyov83M5ptZzWDZwOB0Zkawz8fmaZdiZreY2ZKg3UQzq2FmxwDfBNUyzGxO3v0qcFx/H7xONLOPgvVsMbOJeeq5mSUGr+ua2b/MbHOQ966cjrmZDQ+yP2pm6Wa21szOLmK/U8zs1iD/r2b2kpkdaWYfBJ+RD80sLk/9SWa2Mcj4XzNLCspHAkOB23I+C3nWf7uZLQF+Dd7T3FP0Zva+mT2WZ/0TzOzlot4rESlf6giKRJfGQF0gAbgSeDbvH3LgYuBeIA5YDYwBCDpQ7wFPAfUJndJ8z/Jf1zYMGAnUBn7Is75hwfZaA58ArwD1gJXAX/K0Xwx0CJa9DkwysxpF7Yy7f+LutYJTo3HAQuCNYPG1wLnA6UA8kA48G+xPO+C5IFt8sE9Ni9jUo0Bn4OQg323A3qBD9wZwA9AQeB+YambV8rS9CDgLaAm0B4a7+7dAzinQWHfvXdR+Bu4HZgb72RR4eh/1nib0HrcK9v0y4Io8y7sT6oQ2AB4GXjIzK2K7FwBnAMcAA4APgDuD/a0CXJen7gfA0UAj4AtgPIC7vxC8fjh4vwbkaXMJ0J/QccgqsO0RwDAz621mQ4FuwPVFZBWRcqaOoEh02QPc5+573P194BegTZ7lb7v7ouAP8nhCHTMI/aH+zt3HuXuWu78BrCLUMcjxqrsvD5bvCcpecfc17r6NUCdhjbt/GKx/EtAxp7G7v+buW4P2jwHVC2Tbn6eAHUDO6N4fgdHuvt7dM4F7gAuDEbcLgWnu/t9g2f8D9ha20mA0bQRwvbununu2uy8I2g0G3nP3WcE+PwrUJNRhzM3l7mnu/jMwNc8xPVB7gKOAeHff5e7zC8lalVDn+w533+HuKcBjhDq8OX5w9xfdPRsYCzQhdJp6X55295/cPRWYByx09y/dfRfwNvnfw5eD7eYc7xPMrO5+9uspd1/n7jsLLnD3jcCfgpxPApe5+479rE9EypE6giKRIxs4rEDZYYQ6EDm2Fhh1+Y38N15s3MeyeP43ypfjB0IjfTnWFZLppzyvdxYyn7vt4BTqyuC0YgahUa0GhawzjJn9AegJDHH3nA7dUcDbwSnbDEIjkNmEOj3xefO6+6/A1n2svgGha/nWFLIs33EJtr2O/MdlX8f0QN0GGLAoOBU9Yh9ZDyP/e1XwfcrN4+6/BS+LylSs99DMqprZg8Gp+O1ASp5MRSnsc5PXVKAq8E1hnV8RqVjqCIpEjh+BFgXKWhLegSuJNEIdq7yaA6l55r2kKw+uB7yN0GnUOHePBbYR6vgUp+39wDnuvj3PonXA2e4em2eqEYxsbQCa5VnH4YRODxdmC7CL0KntgvIdl+AUazPyH5fi+jX4eXiessY5L9x9o7tf5e7xwB+Av+dcF1gga87IYY6C71NZGQKcA/Ql1IlvEZTnvIf7+nzs73MzhlAnvomZXXKQGUWklKkjKBI5JgJ3mVnT4EaJvoRO3b5VCut+HzjGzIYEF/QPBtoB00ph3RC6rjAL2AzEmNndQJ39NTKzZsCbhE4Zfltg8fPAGDM7Kqjb0MzOCZa9BSSb2anB9Xz3sY/fZ8Eo38vA38wsPhj5OsnMqgfb7m9mfSz0OJibgUxgwQHtfWg7mwl12C4NtjGCPJ1PMxtkZjnXMaYT6kDtLbCO7CDTGDOrHez7TcBrB5qnBGoT2vethDqzfy2w/CdC1y0Wm5n1IHR942XA5cDTZpZQdCsRKU/qCIpEjvsIdUDmE+ooPAwMdfdlB7tid98KJBPq6GwlNHqX7O5bDnbdgRnAdOBbQiOYu9j/KUOAPoRO9b5l/7tzOOdxLE8C7wIzzWwH8CmhGyVw9+XAKEI3pWwgdLzWF7GdW4ClhG5o+Rl4CKji7t8AlxK6QWMLoY73AHffXcz9Lugq4FZCxziJ/B3KrsBCM/sl2K/r3f37QtZxLaHRxe8JfRZeJ9SRLWv/IvTepQIrCB3vvF4C2gWn6t/Z38rMrE6wzmuCazPnBet4ZT83t4hIOTL3Ep8NEhEREZEophFBERERkUOUngQvIiIiEoUs9AD+HYSeqJDl7l2C58ZOJHTDVwpwkbun72sdGhEUERERiV693L2Du3cJ5v8MzHb3o4HZwfw+qSMoIiIiUnmcQ+gh7gQ/zy2qsm4WKQUWU9OtWu2KjpFPx2ObV3QEERGRMvPFF59vcfeGFbX9qnWOcs8K+0KdUuM7Ny8n9ASGHC8EX/eYy8zW8r/HUf3D3V8ws4zgWa45z0ZNz5kvjK4RLAVWrTbV21xU0THy+XjhMxUdQUREpMzUPMxK42H7JeZZO8v0b/+ur57dled0776c6u6pZtYImGVmq/JldHczK3LET6eGRURERKJQ8E1LuPsmQt8d3g34ycyaAAQ/NxW1DnUERURERA6YgVUpu2l/Wzc7wsxq57wGzgSWEXpg/eVBtcuBKUWtR6eGRURERKLPkcDbwRf1xACvu/t0M1sMvGlmVxL6tqAiz1+rIygiIiJyoAyowG9LDL6i8oRCyrcS+vrOYtGpYREREZFDlEYERUREREqiGNfyRTp1BEVERERKogJPDZeW6O/KioiIiEiJaERQRERE5IBZpTg1HP17ICIiIiIloo5gGVr13r0sfvNOPp3wZ+aPvw2AuDqHM+25a1g65W6mPXcNsbVrFtp26IDuLJ1yN0un3M3QAd1zyzse24zFb97Jsil/4bHbLjyofDNnTKd9UhuS2ibyyMMPhi3PzMzk0iGDSWqbyGknd+eHlJTcZY889ABJbRNpn9SGWTNnHFQOZYruPMqkTPp8K1OkZipzZmU3lRd313SQk9Vs6DU6jAqbUlK3eELP2/KVPfbKTL/ryXe8RodRfteT7/ijL88Ma9ekx63+/brN3qTHrd74tFv8+3WbvfFpt3iNDqN88dK13mPYI16jwyifPn+ZDxz1bKHb3rnHi5x+2ZXlLVu18hXfrPFtv2b68ce39y++Xp6vzhNPPeu/v+oPvnOP+9jX3vALBl3kO/e4f/H1cj/++Pae8csuX/nt996yVSv/ZVfWfrepTAefKdLyKJMy6fOtTBWVCfisQv/2H36k1+h2S5lN5bV/GhEsZ8k92/Pa1IUAvDZ1IQN6tQ+rc8bJxzL701Wkb/+NjB07mf3pKs48pR2NG9Sh9hE1WLQ0BYDXpy1iQM/w9sWxeNEiWrdOpGWrVlSrVo1Bgy9m2tT830IzbeoUhg4LfUvN+RdcyNw5s3F3pk2dwqDBF1O9enVatGxJ69aJLF60qEQ5lCm68yiTMunzrUyRmqnMGRX6FXOlRR3BMuTuTP37NXw8/jZGnH8KAI3q12bjlu0AbNyynUb1a4e1i28Yy/qf0nPnUzdlEN8wlvhGsaRuyvhf+U8ZxDeKLVG2tLRUmjZtljufkNCU1NTU8DrNQnViYmKoU7cuW7duJTU1vG1aWv62ylQ2mSItjzIpU2lmirQ8yhTdmaR4dNdwGepzxeOkbd5Gw7haTHv+Gr5J2RhWx70CgomIiMhBKudr+cpIRIwImlmKmS01s6/M7LOg7EQzWxiUrTSzew5gfT3NbFvQdpWZPZpn2XAz22tm7fOULTOzFnnmO5iZm9lZB7NfaZu3AbA5/RfenbOErkkt2LR1B40b1AGgcYM6bP55RyHtMmh6ZFzufEKjWNI2Z5C2KYOEPCOACUfGkpZnhPBAxMcnsH79utz51NT1JCQkhNdZF6qTlZXF9m3bqF+/PgkJ4W3j4/O3VaayyRRpeZRJmUozU6TlUaboziTFExEdwUAvd+/g7l2C+bHASHfvABwHvFmclZhZzijnvKBtRyDZzE7JU209MLqI1VwCzA9+lsjhNapR6/Dqua/7ntSW5WvSeO+jpVwa3AV86YDuTJu7JKztrAUr6XtSW2Jr1yS2dk36ntSWWQtWsnHLdnb8uotux7cAYEhyN6Z9FN6+OLp07crq1d+RsnYtu3fvZtLECfRPHpivTv/kgYwfNxaAf09+i9N79cbM6J88kEkTJ5CZmUnK2rWsXv0dXbt1K1EOZYruPMqkTPp8K1OkZioXleAawUg+NdwI2ADg7tnAin1VDEYLWwOtgB+Bf+Qsc/edZvYVkPe/F9OAHmbWxt2/KbAuAwYBZwDzzKyGu+8qZJsjgZEAHFYrPHz92kz821UAxFStysQPPmPWgpV8vvxHXntoBJefexI/bviZS297GYBO7Zrz+wtP5er7Xid9+2888OJ05r8WeuTMX1+YTvr23wC4/oE3eeHeS6lZ/TBmfryCGfP3eViKFBMTw+NPPsOA/v3Izs7m8uEjaJeUxH333E2nzl1IHjCQ4SOuZMTwYSS1TSQurh7jxk8AoF1SEhcMuoiO7dsRExPDE089S9WqVUuUQ5miO48yKZM+38oUqZmkeMwj4CI1M1sLpAMO/MPdXzCzu4EbgbnAdGBsYR2yoP09wADg1KDj1xO4xd2TzSwO+BDo7+4bzWw40AVYBPRx98vNbBmQ7O4pwcjhfe7ex8xeBya7++Si8lc5vJFXb3PRwR6GUpW++JmKjiAiIlJmah5mn+c5i1juqtRq4tVPGFFm69+14K/lsn+Rcmr4VHfvBJwNjDKzHu5+H6EO20xgCKHOYFHedfedeeZPM7OvgVRghrsXvFPjdeBEM2tZoPwSYELwegIHcXpYREREKiurFKeGI6Ij6O6pwc9NwNtAt2B+jbs/B/QBTjCz+kWs5tcC8/Pc/QQgCbjSzDoU2GYW8Bhwe06ZmVUFLgDuNrMU4GngLDMLf8aLiIiISJSr8I6gmR2R09EysyOAM4FlZtY/uF4P4GggGzjgW2TdfS3wIHk6fHm8CvQFGgbzfYAl7t7M3Vu4+1HAZOC8A92uiIiIVGJGpfiKuQrvCAJHAvOD07iLgPfcfTowDPgmuNFjHDA0uGmkJJ4ndHNIi7yF7r4beIrQjSkQOg38doG2k9HpYREREamEIuJmkWinm0VERETKV4XfLFI73qt3HFlm6981795D6mYRERERESlnkfwcwTBmdgVwfYHij919VEXkERERkUOVlevdvWUlqjqC7v4K8EpF5xARERGpDKKqIygiIiISMaqU3929ZSX6xzRFREREpEQ0IigiIiJyoIxKcY1g9O+BiIiIiJSIRgRFRERESqIcvwGkrGhEUEREROQQpRFBERERkQOm5wiKiIiIHLoqwalhdQRLQcdjm/Pxwsj6bt+4k2+u6Ahh0hc8VtERIl5W9t6KjhAmpmr0/49XREQKp46giIiISElUglPD0b8HIiIiIlIiGhEUEREROVBmleIaQY0IioiIiByiNCIoIiIiUhK6RlBEREREopVGBEVERERKQtcIioiIiEi00oigiIiIyAGrHF8xF/17ICIiIiIloo5gOZk5Yzrtk9qQ1DaRRx5+MGx5ZmYmlw4ZTFLbRE47uTs/pKTkLnvkoQdIaptI+6Q2zJo546CzVKlifDLuJib/7UoATu+SyIJ/3chnb9zCi3+5mKr7+Eqxof27sPStP7P0rT8ztH+X3PKObZuy+PVbWDb5Dh67+dyDyhZJxylSM/1p5JW0bNaYbp3aF7rc3bn1pus5od0xnNilA199+UXusvHjxtIhqQ0dktowftzYUskDkXeMlCl6M0VaHmWK7kxlLudZgmUxlRd313SQU6dOnX3nHt/n9MuuLG/ZqpWv+GaNb/s1048/vr1/8fXyfHWeeOpZ//1Vf/Cde9zHvvaGXzDoIt+5x/2Lr5f78ce394xfdvnKb7/3lq1a+S+7sorc3s497jW63rTP6bbH3/EJ0z/39+Yt95rdbvZ1G9P9uAv+6jW63uRjXpzhf7h/QlibJn1G+/frt3iTPqO9ce/Q68a9R3uNrjf54mU/eI8rnvAaXW/y6R+v8IHXvVDodveXuSKOU6Rl2rEre7/TB7P+4/M+WezHtksqdPlb70z1M87s59t3Zvnsjz72Ll27+Y5d2f5D2mZv0aKl/5C22X/csMVbtGjpP27Yst/tRdoxisT3TZlKJ1Ok5VGmyM4EfFaRf/utbjOv0f/pMpvKa/80IlgOFi9aROvWibRs1Ypq1aoxaPDFTJs6JV+daVOnMHTY5QCcf8GFzJ0zG3dn2tQpDBp8MdWrV6dFy5a0bp3I4kWLSpwloVFdzjqlHa9MWQhA/bqHs3tPFqt/3ALAnEXfcm6v8JGmM05sy+yF35K+fScZO3Yye+G3nHlSWxrXr03tI2qwaNmPALz+/ucMOP24EmWLpOMUyZlOPa0HcXH19rn8vanvcsnQYZgZ3bqfSEZGBhs3bGD2rBn06tOXevXqERcXR68+fflw5vSDzhOJx0iZojNTpOVRpujOJMWjjmA5SEtLpWnTZrnzCQlNSU1NDa/TLFQnJiaGOnXrsnXrVlJTw9umpeVveyAeufEcRj89jb17HYAtGb8SU7UKnY5tCsB5vdvT9MjYsHbxDeuyflNG7nzqpgziG9YlvlFdUguWN6pbomyRdJwiOVNxMicUst20tLRC8qSVyvYi7RgpU3RmirQ8yhTdmcpecLNIWU3lRB3BQ8jZpx7LpvRf+HLV+nzll931Gg/feA7zXrmeHb9lkr13bwUlFBERkfJU5h1BM0sxs6Vm9pWZfVbI8rpmNtXMvjaz5WZ2RVBexcyeMrNlQfvFZtbyALY718y+Cda72Mw6FMg0Oc/8hWb2aoH275jZpyXa6QLi4xNYv35d7nxq6noSEhLC66wL1cnKymL7tm3Ur1+fhITwtvHx+dsW10ntW5J8WhKr3hnNv8ZcSs8uibx87xAWLv2BviOf5bQrnmT+l9+z+sfNYW3TNm+jaaP/jRQmNIolbfM20jZtI6Fg+aZtJcoXKccp0jMVJ3NqIduNj48vJE98qWwv0o6RMkVnpkjLo0zRnalcVIKbRcprRLCXu3dw9y6FLBsFrHD3E4CewGNmVg0YDMQD7d39eOA8IKOQ9mHMrGrwcmiw3r8DjxSo1tnM2u2jfSzQGahrZq2Ks82idOnaldWrvyNl7Vp2797NpIkT6J88MF+d/skDc+/i/Pfktzi9V2/MjP7JA5k0cQKZmZmkrF3L6tXf0bVbtxLluPvv75M44H7anjuGy0a/xtzPVjPiL6/TMK4WANUOq8rNl/XmxX9/EtZ21qer6HviMcTWrkls7Zr0PfEYZn26io1bd7Dj1110O645AEN+15lp/11WonyRcpwiPdP+/C55AG+MH4e7s2jhp9StW5fGTZrQ54x+zPlwFunp6aSnpzPnw1n0OaPfQW8vEo+RMkVnpkjLo0zRnUmKJxIeKO1AbTMzoBbwM5AFNAE2uPteAHdfv+9VgJn9AvwD6Euoc5nXJ8CtBcoeA0YDQwtZ3fnAVOAn4GLgrwewP2FiYmJ4/MlnGNC/H9nZ2Vw+fATtkpK475676dS5C8kDBjJ8xJWMGD6MpLaJxMXVY9z4CQC0S0rigkEX0bF9O2JiYnjiqWepWrXqfrZ4YG68tCdnn9qOKlWMFycv4KPPVgPQ6dim/P78k7l6zJukb9/JAy99yPxXbwDgr/+cRfr2nQBc//BkXrj7YmpWP4yZC1YxY8GqEuWIxOMUiZmuGDaEefM+YuuWLbRp3Zw77/oLWVl7ALjyqj/S76zfMXP6B5zQ7hhqHn44z73wEgD16tXjtjtG0/OU7gDcfudd1Ku375tOiisSj5EyRWemSMujTNGdqVxUggdKm7uX7QbM1gLphDp8/3D3Fwosrw28C7QFagOD3f09M2sKzCc0CjgbeM3dvyxiOx60fTOYnwvc4u6fmdkNQCN3vzNYlgJ0B+YCA4AOQLK7Dw+WzwLuI9QRnByMSBbc3khgJECz5s07f7vmhwM+NmUp7uSbKzpCmPQFj1V0hIiXlR1512fG7OO5kiIiFanmYfb5Ps40losqsUd59dPvLLP173r3j+Wyf+UxIniqu6eaWSNglpmtcvf/5lneD/gK6A20DurMc/f1ZtYmKO8NzDazQe4+ex/byQYmFygbH5xmrkWos1ew/iPAHcAHOYVmdiRwNDDf3d3M9pjZce6e73xn0KF9AaBz5y5l25sWERGRyFOeD34uI2X+X313Tw1+bgLeBgqe+L8C+LeHrAbWEhodxN0z3f0Dd7+V0OnZor62Ype7ZxcoGwq0AsYCTxfSZhzQA2iWp+wiIA5YG4wctgAu2c9uioiIiESdMu0ImtkRwalfzOwI4Eyg4J0EPwJ9gjpHAm2A782sk5nFB+VVgPbAAZ9/9dC57/8HnGhmbQss2wM8DtyYp/gS4Cx3b+HuLQjdNHLxgW5XREREKjHTcwSL40hgvpl9DSwC3nP36Wb2RzP7Y1DnfuBkM1tK6FrA2919C9AImGpmy4AlhG4geaYkIdx9J6GbQwreMALwEsEpcjNrARwFfJqn7Vpgm5l1L8m2RURERCJVmV4j6O7fAycUUv58ntdphEYKC9aZDhT7+6/cvVaB+Z4F5h/L87pFnteZhB5TkyPs4UXu3qm4OUREROQQoWsERURERCRaRcJzBA+ImS0EqhcoHubuSysij4iIiByarBKMCEZdR9Ddda2eiIiIVCijcnQEdWpYRERE5BAVdSOCIiIiIhXOginKaURQRERE5BClEUERERGRA2a6RlBEREREopdGBEVERERKQCOCIiIiIhK1NCIoIiIiUgIaERQRERGRqKURwUoqfcFjFR0hTFzXayo6Qj7pi5+p6AhhYqrq/2YiItFCI4IiIiIiErU0IigiIiJyoPTNIiIiIiISzTQiKCIiInKArJJ8s4g6giIiIiIlUBk6gjo1LCIiInKI0oigiIiISAloRFBEREREopZGBEVERERKQCOCIiIiIhK11BEsJzNnTKd9UhuS2ibyyMMPhi3PzMzk0iGDSWqbyGknd+eHlJTcZY889ABJbRNpn9SGWTNnVLpMq967l8Vv3smnE/7M/PG3ARBX53CmPXcNS6fczbTnriG2ds1C2w4d0J2lU+5m6ZS7GTqge255x2ObsfjNO1k25S88dtuFB5UvUo5TpOZRJmXS51uZIjVTmbIynsqLu2s6yKlTp86+c4/vc/plV5a3bNXKV3yzxrf9munHH9/ev/h6eb46Tzz1rP/+qj/4zj3uY197wy8YdJHv3OP+xdfL/fjj23vGL7t85bffe8tWrfyXXVlFbq84U0VkqtFhVKFTSuoWT+h5W76yx16Z6Xc9+Y7X6DDK73ryHX/05Zlh7Zr0uNW/X7fZm/S41Rufdot/v26zNz7tFq/RYZQvXrrWewx7xGt0GOXT5y/zgaOeDWsfqccpmvIokzLp861MFZUJ+Kwi//ZXrd/S6132eplN5bV/GhEsB4sXLaJ160RatmpFtWrVGDT4YqZNnZKvzrSpUxg67HIAzr/gQubOmY27M23qFAYNvpjq1avTomVLWrdOZPGiRZUyU17JPdvz2tSFALw2dSEDerUPq3PGyccy+9NVpG//jYwdO5n96SrOPKUdjRvUofYRNVi0NAWA16ctYkDP8PbFEWnHKdLyKJMy6fOtTJGaqTyYWZlN5UUdwXKQlpZK06bNcucTEpqSmpoaXqdZqE5MTAx16tZl69atpKaGt01Ly9822jO5O1P/fg0fj7+NEeefAkCj+rXZuGU7ABu3bKdR/dph7eIbxrL+p/Tc+dRNGcQ3jCW+USypmzL+V/5TBvGNYkuULZKOUyTmUSZlKs1MkZZHmaI7kxRPpbhr2MxigSHu/nczawGsBL4BqgGfAVe6+x4z6wn8Bxjo7lODttOAR919bjDfANgAXOvuz5fzrhyS+lzxOGmbt9EwrhbTnr+Gb1I2htVxr4BgIiIi+1BZvmKusowIxgJX55lf4+4dgOOBpsBFeZatB0YXsa5BwKfAJaUVLj4+gfXr1+XOp6auJyEhIbzOulCdrKwstm/bRv369UlICG8bH5+/bbRnStu8DYDN6b/w7pwldE1qwaatO2jcoA4AjRvUYfPPOwppl0HTI+Ny5xMaxZK2OYO0TRkk5BkBTDgylrQ8I4QHIpKOUyTmUSZlKs1MkZZHmaI7kxRPZekIPgi0NrOvgEdyCt09G1gE5P1EfQ1sM7Mz9rGuS4CbgQQza1oa4bp07crq1d+RsnYtu3fvZtLECfRPHpivTv/kgYwfNxaAf09+i9N79cbM6J88kEkTJ5CZmUnK2rWsXv0dXbt1qzSZDq9RjVqHV8993fektixfk8Z7Hy3l0uAu4EsHdGfa3CVhbWctWEnfk9oSW7smsbVr0vektsxasJKNW7az49dddDu+BQBDkrsx7aPw9sURKccpUvMokzLp861MkZqpPFSGawQrxalh4M/Ace7eITg1PA3AzGoA3YHrC9QD5L/bAAAgAElEQVQfA9wPzMpbaGbNgCbuvsjM3gQGA48VtkEzGwmMBGjWvHmR4WJiYnj8yWcY0L8f2dnZXD58BO2Skrjvnrvp1LkLyQMGMnzElYwYPoyktonExdVj3PgJALRLSuKCQRfRsX07YmJieOKpZ6lateoBHJrIztSofm0m/u2qUKaqVZn4wWfMWrCSz5f/yGsPjeDyc0/ixw0/c+ltLwPQqV1zfn/hqVx93+ukb/+NB16czvzXQo+c+esL00nf/hsA1z/wJi/ceyk1qx/GzI9XMGP+iqg+TpGaR5mUSZ9vZYrUTOUi+s8MY14JLr7K6fy5+3EFrhFsCbzn7kOCej2BW9w92cw+InSK+M8E1wia2S1AnLuPNrP2wMvu3mV/2+/cuYt/vPCz0t+xSiau6zUVHSGf9MXPVHQEEREpoZqH2efF+RtdVg5r0NrjznmgzNa/+eXB+90/M6tK6F6I1KBv0xKYANQHPgeGufvuotZRWU4NF5RzjWBroLOZDSykzhjgrgJllwDDzSwFeBdob2ZHl2lSERERiT4WEaeGryc0+JXjIeBxd08E0oEr97eCytIR3AGEPV/E3bcQGvG7o5BlM4E4oD2AmR0D1HL3BHdv4e4tgAcoxZtGREREREpDcB9Df+CfwbwBvYG3gipjgXP3t55K0RF0963Ax2a2jDw3iwTeAQ43s9MKaToGyHl40SXA2wWWT0YdQRERESlEGY8INjCzz/JMIwts/gngNmBvMF8fyHD3rGB+Pflvli1UZblZhJzrAAspd+CEPEVz8yx7l/9d6jmXAtx9CXBsqYUUERERKZ4t+7pG0MySgU3u/nlw/0OJVZqOoIiIiEh5qsAHSp8CDDSz3wE1gDrAk0CsmcUEo4JNgf1+RUulODUsIiIicqhw9zvcvWlwP8PFwBx3H0ro29MuDKpdDkzZxypyqSMoIiIicoByvmIuwh4ofTtwk5mtJnTN4Ev7a6BTwyIiIiJRyt3nEtzn4O7fAwf0tSzqCIqIiIiURCX4ZhGdGhYRERE5RGlEUERERORAWYXeNVxqNCIoIiIicojSiKCIiIhICVSGEUF1BEVERERKoDJ0BHVqWEREROQQpRFBERERkZKI/gFBdQSl/KQvfqaiI+QTd/roio4QZvOc+ys6QpiYqjpxIKUnK3tvRUfIR59vOdSpIygiIiJSArpGUERERESilkYERURERA6QmWlEUERERESil0YERUREREpAI4IiIiIiErU0IigiIiJSAhoRFBEREZGopRFBERERkZKI/gFBdQRFRERESkKnhkVEREQkaqkjWE5mzphO+6Q2JLVN5JGHHwxbnpmZyaVDBpPUNpHTTu7ODykpucseeegBktom0j6pDbNmzlCmcshUpYrxySujmPzwMAB6dm7FgpdH8emr1zD771fRKqFeoe1uGdaDZRNv4us3bqBvt8Tc8jO6H83Xb9zAsok3cculPQ4q259GXknLZo3p1ql9ocvdnVtvup4T2h3DiV068NWXX+QuGz9uLB2S2tAhqQ3jx409qBx5Rcr7pkzRn0mfb2UqzUxlyv73UOmymMqLOoLlIDs7mxuuG8WUqR/w5ZIVTJrwBitXrMhX59WXXyIuNo7lq1Zz7fU3MvrO2wFYuWIFkyZO4Iuvl/PutOlcf+3VZGdnK1MZZ7pm0Ml8k7I5d/6pW87hinvf5MThzzBx1hL+PLxXWJu2LRoyqE97Ol36JANvGsuTtwykShWjShXjiZsHcM7NY+k49EkG9W1P2xYNS5xt6LDLefvd9/e5fOaMD1iz+ju+Wv4NTz37PDdeNwqAn3/+mQfH3M+ceZ/wn/mf8uCY+0lPTy9xjhyR9L4pU/Rn0udbmUorkxSPOoLlYPGiRbRunUjLVq2oVq0agwZfzLSpU/LVmTZ1CkOHXQ7A+RdcyNw5s3F3pk2dwqDBF1O9enVatGxJ69aJLF60SJnKMFNCwzqcdXIbXpn6WW6Z49Q5ojoAdWpVZ8OW7WHtkk87lkmzl7B7TzY/bEhnzfqf6XpsU7oe25Q1638mJS2dPVnZTJq9hOTTji1RNoBTT+tBXFzhI5IA7019l0uGDsPM6Nb9RDIyMti4YQOzZ82gV5++1KtXj7i4OHr16cuHM6eXOEeOSHnflKlyZNLnW5lKK1NZM8Cs7Kbyoo5gOUhLS6Vp02a58wkJTUlNTQ2v0yxUJyYmhjp167J161ZSU8PbpqXlb6tMpZvpkev7M/rv09nrnlt29YNv8/ajl7P67dsY0q8jj477b1i7hIZ1Wf/Tttz51E3biG9Yh/iGdVi/KW/5dhIa1i1RtuJIS0sloZBjkZaWVsgxSiuV7UXC+6ZMlSNTcTLr861MUnrKvSNoZu+bWayZtTCzZeW9fZGinH1yGzal/8qX3+T/A3Lt4FM475axJJ73MOPe/5yHrvtdBSUUEZHIUHbXB1baawQttGfJ7p5RntutaPHxCaxfvy53PjV1PQkJCeF11oXqZGVlsX3bNurXr09CQnjb+Pj8bZWp9DKd1P4okk9ty6q3buFf9w6mZ+dW/PuRyzg+sTGLV6wH4K3ZSznxuOZhbVM3b6Ppkf8b6UtoVJe0zdtJ27ydpo3yltchdfO2sPalJT4+gdRCjkV8fHwhxyi+VLZX0e+bMlWeTMXJrM+3MknpKfOOYDDy942Z/QtYBmSbWYNgcYyZjTezlWb2lpkdHrTpY2ZfmtlSM3vZzKqbWRcz+yqYlpqZm1lrM/siz7aOzpk3sxQzeyCo/5mZdTKzGWa2xsz+mKfNrWa22MyWmNm9QdkRZvaemX1tZsvMbPDBHIMuXbuyevV3pKxdy+7du5k0cQL9kwfmq9M/eWDuXW7/nvwWp/fqjZnRP3kgkyZOIDMzk5S1a1m9+ju6dut2MHGUqQh3Pz+TxPMepu2Fj3LZXyYy9/PvGfTn16hzRA0Sm9UHoHfXRL75YVNY2/fmr2JQn/ZUO6wqRzWJI7FpfRavXM9nq1JJbFqfo5rEcVhMVQb1ac9781eV4AgVz++SB/DG+HG4O4sWfkrdunVp3KQJfc7ox5wPZ5Genk56ejpzPpxFnzP6HfT2IuF9U6bKk2l/9PlWpkhSGa4RLK8HSh8NXO7un5pZSp7yNsCV7v6xmb0MXG1mzwCvAn3c/dugA/knd38C6ABgZo8A0919jZltM7MO7v4VcAXwSp71/+juHczs8WCdpwA1CHVInzezM4Ns3Qhd9/mumfUAGgJp7t4/2F7YBV1mNhIYCdCsefjoUF4xMTE8/uQzDOjfj+zsbC4fPoJ2SUncd8/ddOrcheQBAxk+4kpGDB9GUttE4uLqMW78BADaJSVxwaCL6Ni+HTExMTzx1LNUrVq1WAddmQ4+E0B29l5GPfQOb4wZwt69TsaOnfzhgX8D0P/UtnRqm8D9/5zNyrWbmDxnGV+Ov56s7L3c8Lep7N3rgHPj41OZ+rfhVK1qjJ32BSvXhncki+uKYUOYN+8jtm7ZQpvWzbnzrr+QlbUHgCuv+iP9zvodM6d/wAntjqHm4Yfz3AsvAVCvXj1uu2M0PU/pDsDtd95FvXr7vii/uCLxfVOm6M2kz7cylebvb9k/8zwXxJfJBsxaAP9x95bBfArQBagF/NfdmwflvYHrgL8AT7t7j6C8DzDK3c8P5gcT6oCd6e7ZZjaUUEfuJuBboJu7bw22c4q7p5rZCOAkd78qWMePQHvgLuBCIOdUdS3gAWAeMBOYCExz93lF7WPnzl3844WfFVVFIlDc6aMrOkKYzXPur+gIYWKq6p4yKT1Z2XsrOkI++nxHr5qH2efu3qWitl+j8TF+1OVPl9n6v334rHLZv/IaEfx1H+UFe6FF9krN7DjgHqCHu+c8ZGgyoc7jHOBzd9+ap0lm8HNvntc58zGERgEfcPd/FLKtTsDvgP8zs9nufl9R2URERESiTUX/V6i5mZ0UvB4CzAe+AVqYWc7XMgwDPjKzWOAN4DJ3z33Sr7vvAmYAz5H/tHBxzABGmFktADNLMLNGZhYP/OburwGPAJ1KtnsiIiJSKZXh9YGV8RrBffkGGBVcH7gCeM7dd5nZFcAkM4sBFgPPAxcDRwEv5txW7e4dgvWMB84jdDq32Nx9ppkdC3wSrPMX4FIgEXjEzPYCe4A/HdReioiIiESgMu8IunsKcFye+RbByy1A2320mQ10LFA8NpgKcyrwSp7TxXm3g7u/SuhmkcKWPQk8WWB9awiNFoqIiIiEMULfSx/tKnpE8KCZ2dtAa6B3RWcRERGRQ0d5nsItK1HfEXT38yo6g4iIiEg0ivqOoIiIiEhFKM+vgisrFX3XsIiIiIhUEI0IioiIiByocn7MS1nRiKCIiIjIIUojgiIiIiIHyNA1giIiIiISxTQiKCIiInLATCOCIiIiIhK9NCIoIiIiUgKVYEBQI4IiIiIihyqNCMohK/2jMRUdIUzc2Q9XdIQwa9+6oaIjhIk9olpFR5ASiqmq8QepPHSNoIiIiIhELY0IioiIiByoSvLNIuoIioiIiBwgPVBaRERERKKaRgRFRERESqASDAhqRFBERETkUKURQREREZES0DWCIiIiIhK1NCIoIiIiUgKVYEBQI4IiIiIihyqNCIqIiIgcKNM1gnIAZs6YTvukNiS1TeSRhx8MW56ZmcmlQwaT1DaR007uzg8pKbnLHnnoAZLaJtI+qQ2zZs5QpkM0U5UqxifPXc7k+y/ILbvnitNY8srv+fKlK7n63E6Ftht6RhJLX72Kpa9exdAzknLLOx59JItfuIJlr17FY1f3OahsANnZ2ZxxWjeGDT43bFlmZiZ/uGIoJ3U8lt/1OZV1P6TkLnvqbw9zUsdjObXLcfxn9syDzpEjUt43ZYruPMoU3Zlk/9QRLAfZ2dnccN0opkz9gC+XrGDShDdYuWJFvjqvvvwScbFxLF+1mmuvv5HRd94OwMoVK5g0cQJffL2cd6dN5/prryY7O1uZDsFM15zXmW9+3Jo7P6zfcTRtWJsTRvyTjle+xKS5q8LaxNWuwehhp9Dj2nGcds2/GD3sFGJrVQfgqevOZNTj0zlu+Iu0TojjzK4tS5wN4MXnnuboNm0LXfbGuFeoGxvLJ1+uZOTV1/F/94wG4JtVK5ky+U3mfvoVr781lTtuvq7SvW/KFL15lCm6M5W10DeLlN1UXtQRLAeLFy2idetEWrZqRbVq1Rg0+GKmTZ2Sr860qVMYOuxyAM6/4ELmzpmNuzNt6hQGDb6Y6tWr06JlS1q3TmTxokXKdIhlSmhQi7O6t+aVD5bklo1M7shfX1uAe2h+c8ZvYe3O6NKS2Z+nkL5jFxm/ZDL78xTO7NqKxvWOoPbh1Vi0cgMAr3+4nAEnH12ibABpqeuZPfMDhgy7otDl09+fykWXDAMg+ZzzmffRf3B3Zrw/lXMuuIjq1avTvEVLWrRqzZefLy5xjhyR8r4pU3TnUaboziTFo45gOUhLS6Vp02a58wkJTUlNTQ2v0yxUJyYmhjp167J161ZSU8PbpqXlb6tMlT/TI3/qw+gX57J3r+eWtYyP5cKebZn/7GW8M+ZCWifEhbWLr1+L9Zt35M6nbtlBfP1axDeoTeqWPOWbdxDfoHaJsgHcfcct3HXfA1SpUvivlI0b0ohPaAoEx6hOHX7+eSsbN6TmlgPExzdl44a0EufIESnvmzJFdx5liu5MZc8wK7upvER0R9DMGpvZBDNbY2afm9n7ZnaMme00sy/NbKWZLTKz4UWsI8XMGpRjbJFSdXb31mzK+I0vv/spX3n1w6qSuTubU0f9i1c++Jp/3HxWheSbNf09GjRsyAkdCr9GUUSkstKp4TJkoe7w28Bcd2/t7p2BO4AjgTXu3tHdjwUuBm4ws8LPSUWA+PgE1q9flzufmrqehISE8DrrQnWysrLYvm0b9evXJyEhvG18fP62ylS5M52UlEDySYmsGvcH/jV6AD07NOfl2/uTunkH78z/FoAp87/juFaNwtqmbf2Fpg3/N9KX0KA2aVt/IW3LDhLyjAAmNKxNWp4RwgOxaOEnzPzgPboefwx/vHIY8/87l1Ejh+er07hJPGmp64HgGG3fTr169WncJCG3HCAtbT2Nm8SXKEdekfC+KVP051Gm6M4kxROxHUGgF7DH3Z/PKXD3r4F1eSu5+/fATcB1RazrNjNbGoweJgKYWQszm2NmS8xstpk1D8qnmNllwes/mNn4g92RLl27snr1d6SsXcvu3buZNHEC/ZMH5qvTP3kg48eNBeDfk9/i9F69MTP6Jw9k0sQJZGZmkrJ2LatXf0fXbt0ONpIyRVGmu1/+L4lDnqPtsH9w2ZipzP3qR0Y89B5TF3zH6Sc0B+C09s1Yvf7nsLazPltL384tiK1Vndha1enbuQWzPlvLxp9/Zcdvu+l2bBMAhvRNYtonqw84G8Dov/wfX6z4nsVLv+X5l8Zxao+ePPvCq/nq9Ds7mTffGAfAtCn/5tQePTEz+p2dzJTJb5KZmcmPKWtZu2Y1HTt3LVGOvCLhfVOm6M+jTNGdqTxUhlPDkfwcweOAz4tZ9wug8NsVQ7a5+/FBB+8JIBl4Ghjr7mPNbATwFHAuMBL42MzWAjcDJxa2QjMbGdSlWfPmRYaLiYnh8SefYUD/fmRnZ3P58BG0S0rivnvuplPnLiQPGMjwEVcyYvgwktomEhdXj3HjJwDQLimJCwZdRMf27YiJieGJp56latWqxTwsylQZM+V4dMJCXrkjmWsv6MKvO3fzp79NB6DTMY35fXIHrv7bdNJ37OKB8Z8w/5nLAPjr+AWk79gFwPVPz+KFW86mZvUYZi5ey4xF35daNoCHx9zLCR070e93A7hk2BVc+4crOKnjscTG1eP5l0OdwjbHtmPAeRdyevcTiImJ4a+PPllp3zdlir48yhTdmaR4zN33X6sCmNl1QEt3v7FAeQtgmrsfl6csDkhz95qFrCcF6O3u35vZYcBGd69vZluAJu6+Jyjf4O4NgjZDgH8B57n71P1l7dy5i3+88LOS7qpIrrizH67oCGHWvnVDRUcIE3tEtYqOICIVrOZh9rm7d6mo7ddq1tY7XP9ima3/41t7lMv+RfKp4eVA52LW7QisBDCzGWb2lZn9M89y38frfTke2Aoc/MVKIiIiIhEqkjuCc4DqwSlYAMysPdAsb6VghPBRQqd6cfd+7t7B3X+fp9rgPD8/CV4vIHSjCcBQYF6wvm7A2YQ6l7eY2cE9ZVdEREQqndADpXWNYJlxdzez84AnzOx2YBeQAtwAtDazL4EawA7gKXd/tYjVxZnZEiATuCQouxZ4xcxuBTYDV5hZdeBF4Ap3TzOzm4GXzay3R+o5dBEREZESitiOIIC7pwEXFbIo7FrAItbRInh5e4HyH4DehTQ5IU+dd4F3i7stEREROXSU58hdWYnkU8MiIiIiUoYiekRQREREJFJVggFBjQiKiIiIHKo0IigiIiJSArpGUERERESilkYERURERA6UVY5rBNURFBERETlARvk++Lms6NSwiIiIyCFKI4IiIiIiJVAJBgQ1IigiIiJyqNKIoIiIiEgJVKkEQ4IaERQRERE5RGlEsBQ4kJW9t6JjyAGKqRp5/w9a+cZ1FR0hTIdb3q3oCGFSnruwoiNEhUj8vRSJ/+5ESqoSDAhqRFBERETkUKURQREREZEDZKavmBMRERGRKKYRQREREZESqBL9A4IaERQRERGJNmZWw8wWmdnXZrbczO4Nylua2UIzW21mE82sWlHrUUdQREREpATMrMymYsgEerv7CUAH4CwzOxF4CHjc3ROBdODKolaijqCIiIhIlPGQX4LZw4LJgd7AW0H5WODcotajjqCIiIhICYTuHC6bCWhgZp/lmUaGb9+qmtlXwCZgFrAGyHD3rKDKeiChqH3QzSIiIiIiB8gAo0zvFtni7l2KquDu2UAHM4sF3gbaHuhGNCIoIiIiEsXcPQP4D3ASEGtmOQN9TYHUotqqIygiIiJSAlWs7Kb9MbOGwUggZlYTOANYSahDmPM9nJcDU4rch4M5AFJ8fxp5JS2bNaZbp/aFLnd3br3pek5odwwndunAV19+kbts/LixdEhqQ4ekNowfN7ZS5onUTAAzZ0ynfVIbktom8sjDD4Ytz8zM5NIhg0lqm8hpJ3fnh5SU3GWPPPQASW0TaZ/UhlkzZ5RKnu3bMvjTFZfQ56QT6HtyB75Y/Gm+5e7OPXfcRM+uSZx1eleWff1l7rLJE16jV7fj6NXtOCZPeO2gs1QxmPX/+jDu2lMAaN7gcN6/ozefjDmLf4zszmFVC/9tdu3ZbfhkzFnMv78fPZOOzC3vlXQk8+/vxydjzuKas9ocVLZIe98iMVMk/puLtGOkTNGdqZJrAvzHzJYAi4FZ7j4NuB24ycxWA/WBl4paiTqC5WTosMt5+93397l85owPWLP6O75a/g1PPfs8N143CoCff/6ZB8fcz5x5n/Cf+Z/y4Jj7SU9Pr3R5IjVTdnY2N1w3iilTP+DLJSuYNOENVq5Yka/Oqy+/RFxsHMtXreba629k9J23A7ByxQomTZzAF18v591p07n+2qvJzs4+6Ez33nkLp/c+k9mffM37cxeReEz+S0LmfjiDlO/X8J9Fy3jgsWe467brAMhI/5knHx3D2zP+yzsz5/Hko2PYlnFwx+mqvkfz3YYdufN3XXA8//jwW04aPZ2M33Yz5NSWYW2OaVKbc7s24/S/zGTIk/N4cEjH3P8BPzCkI0OenE+Pu2dwXrdmHNOkdolyReL7FomZIu3fXCQeI2WK3kxlrgwfHVOcx8e4+xJ37+ju7d39OHe/Lyj/3t27uXuiuw9y98yi1qOOYDk59bQexMXV2+fy96a+yyVDh2FmdOt+IhkZGWzcsIHZs2bQq09f6tWrR1xcHL369OXDmdMrXZ5IzbR40SJat06kZatWVKtWjUGDL2ba1Pyj7NOmTmHosMsBOP+CC5k7ZzbuzrSpUxg0+GKqV69Oi5Ytad06kcWLFh1Unu3bt7Ho0/kMvnQ4ANWqVaNO3dh8dWZNn8b5g4dgZnTs0p3t27axaeMG/vufWZx6eh9i4+pRNzaOU0/vw0dzZpY4S5O4mvQ9vgnj56/NLTulTSOmfR66HOXNBT9wVsf4sHb9OsTzzuJ17M7ay49bfmPt5l/o2LIeHVvWY+3mX/hxy6/syXbeWbyOfh3C2xdHpL1vkZop0v7NReIxUqbozSTFo45ghEhLSyWhabPc+YSEpqSlpZKWlkbTsPK0Qy5PRWVKS0sNW3dqamp4nWahOjExMdSpW5etW7eSmhreNi2tyGt292v9DynUq9+AW68dSf9eJ3L7DX/it19/zVfnpw1pNIlvmjvfJD6BjRvT2FigvHF8Ahs3lPw43T/4BO5/awm+NzRfr1Y1tu/cQ/ZeB2BD+k6axNYMa9cktiZpP+/Mnc+pt6/ykoi09y1SMxUnc3n+m4vEY6RM0ZupPJTx42PKRaXpCJrZQDP7c/D6HjNLNbOvzGyFmV2Sp96rwbLqwXwDM0spsK4bzGyXmdUt150Q2Y+s7CyWL/mKoVdcxXv/+ZTDDz+c5556tNxznNG+CVu2Z7Lkx4xy37aIiJSeStMRdPd33T3v1amPu3sH4BzgH2Z2WJ5l2cCIIlZ3CaELL88v/aSFi49PIHX9utz51NT1xMcnEB8fz/qw8pKdLovmPBWVKT4+IWzdCQkJ4XXWhepkZWWxfds26tevT0JCeNv4+CKf67lfTZok0Dg+gY6duwFw9oDzWL7kq3x1jmwSz4a09bnzG9JSadw4nsYFyjempdK4ScmOU9fW9TmzQxMWP3A2z4/sziltGnL/xR2oU/Mwqga3uzWJq8mGjJ1hbTdk7CS+3v9G+nLq7au8JCLtfYvUTMXJXJ7/5iLxGClT9GYqawZUMSuzqbxERUfQzFqY2apgNO9bMxtvZn3N7GMz+87MupnZcDN7pmBbd/8O+A2Iy1P8BHBjnufs5N1Wa6AWcBehDmG5+F3yAN4YPw53Z9HCT6lbty6NmzShzxn9mPPhLNLT00lPT2fOh7Poc0a/Qy5PRWXq0rUrq1d/R8ratezevZtJEyfQP3lgvjr9kwfm3jX578lvcXqv3pgZ/ZMHMmniBDIzM0lZu5bVq7+ja7duB5Wn4ZGNaRLflDWrvwVgwby5JLbJf7NI3379+ffE13F3vvxsIbXr1KFR4yb06HUG8+Z+yLaMdLZlpDNv7of06HVGiXL89e1ldLrtfbre8QF/fGEhH3+zmVH/XMSCbzaT3Dn0C/yik49ixlfhpwtnfr2Bc7s2o1pMFZo3OJxWjWrx5dqf+SolnVaNavH/2bvv+Cjq/I/jrw+JNCGQIEoSwACRBCKhhSZFEBCVYgUEjbQTC4q9gWf3Zz0L6unhWRARsB4EBAIICBwYqgUQQQiSglJCUwgkfH5/7JJL2DQSdneSfJ485pGdme/MvPe7y+Sb77SG51TnrADhqnYNSPw+vUT5nPa5OTVTUXz9f86JdWSZym4mUzxl6ckikcBAXD15q4ChQBdgADAO+E9+C4lIG2CLqv6Ra/JvwDIgHkg4ZZHrgWnAUiBKRM5T1d/zWe9oYDRAgwYNiww/In4oS5cuYe+ePUQ1aci4Rx8nK+s4AKNuvpU+l11B4tw5tGzelGrVq/P2RNfV3iEhITz4yHi6d+4AwEPjHiUkpOCTu4vLaXmcmikwMJBXX3+T/n37kJ2dzbDhI2keE8NTTzxGm7Zx9Os/gOEjRzFyeDwx0ZEEB4cweco0AJrHxHDtwEG0jm1OYGAgr014i4CAgFJnevK5V7jn1hEcO36MhudH8NKEiUz58F0Abhh+Mz16X8aiBfPo3j6GatWq8+KEfwFQOziEO+99hCt7dwFg7H3jqF3IhQIl8fQXP/Kv0R14+KoL+em3/XyyLBmAS1uG0ur8YF6cuZHNaQeZuTqFb5+8lKwTyiOfrJwNAekAACAASURBVOeEAqqM+2Q9U+/uSoAIU5cnszntYIlyOPFzc2Imp/2fc2IdWaaym8kXfHkun7eIqvo7Q5FEJALX/XEucI9/BMxT1Ski0hj4ElcvX5yq3iEiTwA3A/uBpkB/VZ3rXvZDYBbwPa6bLHYHklQ1wj3/J+BqVd0iIq8A21TVo6cxtzZt4/Tb/9oVTmVNYIDzOsR37T/q7wgeOj4yy98RPCS/fV3RhQxZ2Sf8HcGDE//fmbKp2lmypqhHsHlTcERzveSxyV5b/5ej4nzy/spSj2Du++CcyDV+gvzfx6uq+rKIDADeE5EmqprzW9bd0FsPDDo5TURaABcA89338KkMbAcKbQgaY4wxpuIpzv3+nK7c/2mmqjOB1bges3KqZ4H7c40PAZ5Q1Qj3EAaEicj5PohqjDHGmDLCm7eOsdvHnHlP4XrcSp73q6obgLW5Jl0PfHXKsl+5pxtjjDHGlCtl4tCwqiYDF+YaH17AvA/d0544Zfk1wMkHlw4/Zd41uV43zmfb95Y4uDHGGGPKLV/e5sVbKkqPoDHGGGOMOUWZ6BE0xhhjjHGast8faD2CxhhjjDEVlvUIGmOMMcaUgN0+xhhjjDHGlFnWI2iMMcYYc5oEqFT2OwStR9AYY4wxpqIqsEdQRIIKW1BVS/Y0eGOMMcaYsk6kXJwjWNih4Q2Akvfq6JPjCjT0Yi5jjDHGGONlBTYEVbWBL4MYY4wxxpQl5aBDsHjnCIrI9SIyzv26voi09W4sY4wxxhjjbUU2BEXkTaAHEO+e9BfwjjdDGWOMMcY4nbjPE/TG4CvFuX3MRaraRkTWAajqPhGp7OVcZYoAgQF2AXZZk5V9wt8RPNSrXdXfETwkv32dvyN4CL54vL8jeMhY8qy/I3iw/ZI5U5y4v/S3inT7mOMiUgnXBSKISB3AvhHGGGOMMWVccXoE3wK+AOqKyJPAIOBJr6YyxhhjjHG48n77GABU9SMRWQP0ck8aqKo/eTeWMcYYY4zxtuI+Yi4AOI7r8LCddGKMMcaYCq/s9wcW76rh8cBUIAyoD3wiIo94O5gxxhhjjPGu4vQI3gS0VtW/AETkWWAd8Jw3gxljjDHGOJUIVCoH5wgW5zBvOnkbjIHuacYYY4wxpgwrsEdQRF7FdU7gPmCDiMxzj18KrPJNPGOMMcYYZyoHHYKFHho+eWXwBmB2rukrvRfHGGOMMcb4SoENQVV9z5dBjDHGGGPKkvJwH8HiXDXcRESmicgPIvLLycEX4cqTxHlziY2JIiY6kpdefN5jfmZmJjcOHUxMdCRdL+rAjuTknHkvvfAcMdGRxMZEMT9xnmXyYabbRo+iUYN6tG8Tm+98VeWBe++iZfOmdIxrxfp1a3PmTZk8iVYxUbSKiWLK5ElnJA84r46clqlSJWHFB2P44kXX49G7t23Mf98fw8oP72DhP2+mcXhIvsvdH9+Nn6bfy/dT76ZX+8ic6b07XMD3U+/mp+n3cv+N3UqVzUn15NRMTstjmYrHiftKUzzFuVjkQ+ADXLfLuRz4FJjuxUzlTnZ2NnePHcOMhDms+2Ejn02byqaNG/OU+fD99wiuHcyGn7dy5133MH7cQwBs2riRz6ZPY+33G5g5ay533Xk72dnZlslHmW6IH8ZXM78ucH7ivDn8unUL6zdsZsJb73DP2DEA7Nu3j+effZpvlq5g0bKVPP/s02RkZJQ6jxPryGmZ7hh4EZuTd+eMT7j/SkY8+Skdh7/J9Pk/8PDwHh7LREfUZWDPWNrc+DoD7p3E6/cPoFIloVIl4bX7+nPlfZNofcPrDOwVS3RE3RLlclo9OTGT0/JYpuJz2r7SV0S8N/hKcRqC1VV1HoCq/qqqj+JqEJpiWpWURJMmkTRq3JjKlSszcPD1zEqYkafMrIQZ3BA/DIBrrr2Oxd8sRFWZlTCDgYOvp0qVKkQ0akSTJpGsSkqyTD7K1KVrN4KD8+9BApidMJMhN8QjIrTv0JH9+/ezKz2dhfPn0aNnL0JCQggODqZHz14sSJxb6jxOrCMnZQqvG8RlF0XxQcLqnGmKEnR2FQCCalQhfc9Bj+X6dW3GZwt/4NjxbHakZ/Bryj7aNatPu2b1+TVlH8lpGRzPyuazhT/Qr2uzEmVzUj05NZPT8lim4nPavtIXBKGSeG/wleI0BDNFpBLwq4jcKiL9gZpezlWupKWlUr9+g5zx8PD6pKamepZp4CoTGBhIUK1a7N27l9RUz2XT0vIua5m8l6k4mcPz2W5aWlo+edLOyPacVkdOyvTSXX0Z/8+5nFDNmXb781/x1cvD2PrVgwzt05qXJ3/rsVx43Vqk/H4gZzz1jwOE1Q0irG4QKX/knn6Q8Lq1SpTNSfXk1ExOy2OZzhxf7ytN8RWnIXgPcDYwFugM3AyM9GYoY4w5XZdfFMUfGX+ybnPeXyJ3Du7M1fdPIvLqF5n89RpeGHuFnxIaY8oVLx4WdtShYVX9TlUPqepvqhqvqgNUdfnpbEREDhejzN0iUv101usLIrJYROJKs46wsHBSUnbmjKemphAeHu5ZZqerTFZWFgcPHKBOnTqEh3suGxaWd1nL5L1Mxcmcms92w8LC8skTdka257Q6ckqmTrHn069LND9/fj8fPTmY7m0b8+VLN9Eish6rNqYA8PnCH+l4YUOPZVN3H6D+ef/r6Qs/txZpuw+Stvsg9c/NPT2I1N0HPJYvDqfUk5MzOS2PZTpzfL2vNMVXYENQRL4SkS8LGryQ5W4g34agiAR4YXs+E9euHVu3biF5+3aOHTvGZ9On0bffgDxl+vYbkHO11JdffM7FPS5BROjbbwCfTZ9GZmYmydu3s3XrFtq1b2+ZfJSpKFf068/UKZNRVZK+W0mtWrWoFxpKz959+GbBfDIyMsjIyOCbBfPp2btPqbfnxDpySqbH3kkk8uoXib7uZW56fDqL12xj4MMfE3R2VSIb1AHgknaRbN7xh8eys5f9zMCesVQ+K4DzQ4OJrF+HVZtSWP1zKpH163B+aDBnBQYwsGcss5f9XKJ8TqknJ2dyWh7LdOb4el/pKyLitcFXCruh9JtnemMi0h14AtgDXAisAW4E7gTCgEUiskdVe7h7Ef8F9ALGiEgV4GV35lXAbaqaKSLJwCSgP3AWMFBV891Ti0hd4BP3tlYAvYG2QA1grjtPG1w30b7p5POVSyswMJBXX3+T/n37kJ2dzbDhI2keE8NTTzxGm7Zx9Os/gOEjRzFyeDwx0ZEEB4cweco0AJrHxHDtwEG0jm1OYGAgr014i4CA0reLLVPxjIgfytKlS9i7Zw9RTRoy7tHHyco6DsCom2+lz2VXkDh3Di2bN6Va9eq8PdF1+82QkBAefGQ83Tt3AOChcY8SElLwidTF5cQ6cmKmk7KzTzDmhf8w9dmhnDih7D90hFuec/0d27dLNG2iw3n63wvZtP0PvvjmJ9ZNuYus7BPc/UoCJ04ooNzzagIJrwwnIECYNGstm7Z7NiSLw4n15LRMTstjmYrPaftKU3yiuU6q9tpGRA6rag13Q3AGEAOkAcuBB1R1mbtBF6eqe9zLKDBYVT8VkarAFqCnqv4iIh8Ba1X1Nfdy/1DVN0TkdqCNqv6tgBxvAqmq+pyIXAbMAeriaghuB7qo6nIReR/YqKovi8hi4H5VXX3KukYDowEaNGzY9pdfd5yx+jK+kZV9wt8RPAQGFOe0XRN88Xh/R/CQseRZf0cwxmucuL+sWTVgjaqW6tSt0jg38kId/NJnXlv/m9c098n788dvnSRVTVHVE8B6IKKActnAF+7XUcB2VT15I+tJQO47u548VL2mkPUBdAGmAajqXCD3zYp25jr38WN32QKp6kRVjVPVuLrnlOy+YsYYY4wx/lTYoWFvycz1OruQDEdVtbh3uTy5zsLWV5RTu0a931VqjDHGmDJJqCCPmDvJfY6eNx2i4PsTbgYiROTkc5/igSUl2MZyYBCAiFwKBOea11BEOrlfDwWWlWD9xhhjjDFlRnGeNdxeRH7EdY4eItJSRN7wQpaJwFwRWXTqDFU9CowAPnNnOQG8U4JtPAlcKiI/AQOBXbgaoOBqbI4RkU24Gohvl2D9xhhjjKkgKon3Bl8pzmHUCUA/4D8Aqvq9iHg+rLMQqlrD/XMxsDjX9DtyvX4DeOPUZXKNLwRa57PuiFyvVwPdC4lyAOijqlnu3r927iuPAbJU9cZ81l/Y+owxxhhjyqziNAQrqeqOU46Dl/4J1f7REPjU/ci8Y7iekmKMMcYYc9p82XPnLcVpCO4UkfaAum/sfCfwSxHL+JWIjADuOmXyclUdQ/69ism47mtojDHGGFNhFKcheBuuw8MNgd+BBe5pjqWqHwAf+DuHMcYYY8on1zOBy36XYJENQVX9A7jeB1mMMcYYY8qMCnFoWETeJZ976qnqaK8kMsYYY4wxPlGcQ8MLcr2uClwN7PROHGOMMcaYsqEcHBku1qHh6bnHRWQydrNlY4wxxpgyrySPY2sEnHemgxhjjDHGlBUCVCoHXYLFOUcwg/+dI1gJ2Ac87M1QxhhjjDHG+wptCIrruuiWQKp70glV9bhwxBhjjDGmoinyOb1lQKHvwd3o+1pVs92DNQKNMcYYY8qJ4jRm14uIx9M4jDHGGGMqMtdNpb0z+EqBh4ZFJFBVs3A9km2ViPwK/Inr/EhV1TY+ymiMVwQGlIdO/YopY8mz/o7gIbjdHf6O4CFj1Zv+jmDKCdtfll+FnSOYBLQBBvgoizHGGGNMmSAi5f6qYQFQ1V99lMUYY4wxxvhQYQ3BuiJyb0EzVfUVL+QxxhhjjCkTykGHYKENwQCgBu6eQWOMMcYYU74U1hBMV9WnfJbEGGOMMaYMqVQOusqKPEfQGGOMMcbkVV4eMVfY9eA9fZbCGGOMMcb4XIE9gqq6z5dBjDHGGGPKknLQIVguHpNnjDHGGGNKoLBzBI0xxhhjTH6kfFwsYj2CPpI4by6xMVHEREfy0ovPe8zPzMzkxqGDiYmOpOtFHdiRnJwz76UXniMmOpLYmCjmJ86zTBU8k9PyWKbC/Tz7SVZ9Oo6V0x5m2ZQHAQgOqs6st+/gxxmPMevtO6hds1q+y97QvwM/zniMH2c8xg39O+RMb92sAas+HcdPMx7nHw9eV6p8Tqknp+axTGU7kykGVbWhlEObNm31yHEtcDh8NEsbNW6sGzf/qgf+zNQWLWJ17fcb8pR5bcJb+rebb9Ejx1UnfTxVrx04SI8cV137/QZt0SJW9x8+qpt+2aaNGjfWw0ezCt1ecQbLVDYzOS2PZfrfULXVmHyH5NQ9Gt79wTzT/vFBoj76+n+0aqsx+ujr/9GX30/0WC602wO6beduDe32gNbrer9u27lb63W9X6u2GqOrftyu3eJf0qqtxujcZT/pgDFv5bttJ9ZTWcpjmZydCVjtz9/9YU0v1GcXbPXa4Kv3Zz2CPrAqKYkmTSJp1LgxlStXZuDg65mVMCNPmVkJM7ghfhgA11x7HYu/WYiqMithBgMHX0+VKlWIaNSIJk0iWZWUZJkqaCan5bFMJdOveywfJ3wHwMcJ39G/R6xHmd4XNWPhyp/JOPgX+w8dYeHKn7m0c3PqnRNEzbOrkvRjMgCfzEqif3fP5YvDafXktDyWqWxnMsVjDUEfSEtLpX79Bjnj4eH1SU1N9SzTwFUmMDCQoFq12Lt3L6mpnsumpeVd1jJVnExOy2OZiqaqJPzzDpZPeZCR13QG4Nw6Ndm15yAAu/Yc5Nw6NT2WC6tbm5TfM3LGU//YT1jd2oSdW5vUP/b/b/rv+wk7t3aJsjmpnpyYxzKV7Uze5rqPoPcGX7GLRYwxxot6jniVtN0HqBtcg1nv3MHm5F0eZVT9EMwYY/Bij6CIHC5GmbtFpLq3MpSUiCwWkTj362QROac06wsLCyclZWfOeGpqCuHh4Z5ldrrKZGVlcfDAAerUqUN4uOeyYWF5l7VMFSeT0/JYpqKl7T4AwO6Mw8z85gfaxUTwx95D1DsnCIB65wSxe9+hfJbbT/3zgnPGw8+tTdru/aT9sZ/wXD2A4efVJi1XD+HpcFI9OTGPZSrbmXyhPPQI+vvQ8N1Avg1BEQnwcRaviWvXjq1bt5C8fTvHjh3js+nT6NtvQJ4yffsNYMrkSQB8+cXnXNzjEkSEvv0G8Nn0aWRmZpK8fTtbt26hXfv2lqmCZnJaHstUuOpVK1OjepWc1706RbPh1zRmL/mRG91XAd/YvwOzFv/gsez8/26iV6doatesRu2a1ejVKZr5/93Erj0HOfTnUdq3iABgaL/2zFriuXxxOKWenJrHMpXtTKZ4vH5oWES6A08Ae4ALgTXAjcCdQBiwSET2qGoPdy/iv4BewBgRqQK87M65CrhNVTNFJBmYBPQHzgIGqurPBWy/LvCJe1srgN5AW6AGMNedpw2wAbhJVf8q5vsaDYwGaNCwYaFlAwMDefX1N+nftw/Z2dkMGz6S5jExPPXEY7RpG0e//gMYPnIUI4fHExMdSXBwCJOnTAOgeUwM1w4cROvY5gQGBvLahLcICCh9G9kylc1MTstjmQp3bp2aTH/lZlemgACmz1nN/P9uYs2G3/j4hZEMu6oTv6Xv48YH3wegTfOG/O26Ltz+1CdkHPyL596dy7KPXbec+b+Jc8k46No93fXcp0x88kaqVTmLxOUbmbdsY5muJ6fmsUxlO5MvSDl4tIiol05OEZHDqlrD3RCcAcQAacBy4AFVXeZu0MWp6h73MgoMVtVPRaQqsAXoqaq/iMhHwFpVfc293D9U9Q0RuR1oo6p/KyDHm0Cqqj4nIpcBc4C6uBqC24EuqrpcRN4HNqrqyyKyGLhfVVefmjE/bdvG6fLvVpeuwowxZVpwuzv8HcFDxqo3/R3BGK+pdpasUdU4f22/QVQLvXvijKILltD93Zv45P356tBwkqqmqOoJYD0QUUC5bOAL9+soYLuq/uIenwR0y1X2S/fPNYWsD6ALMA1AVecCGbnm7VTV5e7XH7vLGmOMMcZUCL66ajgz1+vsQrZ7VFWzT3Odha2vKKd2h9q1e8YYY4wpmkA5ODLs94tFDgGeN9By2QxEiEikezweWFKCbSwHBgGIyKVAcK55DUWkk/v1UGBZCdZvjDHGGFMm+bshOBGYKyKLTp2hqkeBEcBnIvIjcAJ4pwTbeBK4VER+AgYCu3A1QMHV2BwjIptwNRDfLsH6jTHGGFMBVRLx2uArXjs0rKo13D8XA4tzTb8j1+s3gDdOXSbX+EKgdT7rjsj1ejXQvZAoB4A+qprl7v1r577yGCBLVW/MZ/3dc72OOHW+McYYY0x5UBGeLNIQ+FREKgHHgJv9nMcYY4wxZdzJR8yVdeWmISgiI4C7Tpm8XFXHkH+vYjKu+xoaY4wxxpQpItIA+Ag4D9fFrhNV9XURCQGm47qjSjIwSFUzClpPuWkIquoHwAf+zmGMMcaYisHPVw1nAfep6loRqQmsEZH5wHBgoao+LyIPAw8DDxW0En9fLGKMMcYYY06Tqqar6lr360PAJiAcuBLXvZdx/7yqsPWUmx5BY4wxxhjfESrh1S7Bc0Qk92PLJqrqxHyTiETgOg3uO+A8VU13z9qF69BxgawhaIwxxhjjPHuK84g5EamB66lsd6vqwdzPP1ZVdT++t0DWEDTGGGOMOU2C388RRETOwtUInKKqJx+9+7uIhKpquoiEAn8Utg47R9AYY4wxpowRV9ffe8AmVX0l16yZwDD362HAjMLWYz2CxhhjjDGnS/x+H8HOuB6/+6OIrHdPGwc8j+v+yaOAHbgfs1sQawgaY4wxxpSALx8FdypVXQYFXq3Ss7jrsUPDxhhjjDEVlPUIGmOMMcacJidcLHImWEPQGAfJyj7h7wgeDh/N8ncED7XPruzvCB4yVr3p7wge6gxx3sOWfv94WNGFfCgwwHkHxpy4H3BiPZkzwxqCxhhjjDEl4M9zBM8Ua+IbY4wxxlRQ1iNojDHGGFMC5aBD0HoEjTHGGGMqKusRNMYYY4w5TUL56E0rD+/BGGOMMcaUgPUIGmOMMcacLgEpBycJWo+gMcYYY0wFZT2CxhhjjDElUPb7A61H0BhjjDGmwrKGoI8kzptLbEwUMdGRvPTi8x7zMzMzuXHoYGKiI+l6UQd2JCfnzHvpheeIiY4kNiaK+YnzLFMFz3Tb6FE0alCP9m1i852vqjxw7120bN6UjnGtWL9ubc68KZMn0SomilYxUUyZPOmM5DkpOzub3l3bEz/4Ko95mZmZ3DLiBjq1bsYVPbuwc0dyzrwJr7xIp9bN6BJ3IYsWJp6xPE773JyUqVIl4b8vDuDzh3sB8M/bOrPypSv57uUr+fi+HpxdNf+DRfdf1YIf3riWda9fQ6+WYTnTe7cKZ93r1/DDG9dy31UtSpXNid9vp3xuJzmxjsB59eRtguvJIt4afMUagj6QnZ3N3WPHMCNhDut+2Mhn06ayaePGPGU+fP89gmsHs+Hnrdx51z2MH/cQAJs2buSz6dNY+/0GZs6ay1133k52drZlqsCZbogfxlczvy5wfuK8Ofy6dQvrN2xmwlvvcM/YMQDs27eP5599mm+WrmDRspU8/+zTZGRklDrPSe++/QYXREXnO2/q5A+oVbs2K9ZtYvTtY3nmifEAbP55EzO++JTFK9fzyecJPHLf2HL7uTkp05grmrM5dX/O+EMfJtHxgRl0uH8GKXsOc+tlzTyWia5fi+s6Nybunq+46tlEXv1bJypVEipVEl4Z1ZGrn02k7T1fMbBzY6Lr1ypxNqd9v530uZ3ktDoCZ9aTL4gXB1+xhqAPrEpKokmTSBo1bkzlypUZOPh6ZiXMyFNmVsIMboh3PYz9mmuvY/E3C1FVZiXMYODg66lSpQoRjRrRpEkkq5KSLFMFztSlazeCg0MKnD87YSZDbohHRGjfoSP79+9nV3o6C+fPo0fPXoSEhBAcHEyPnr1YkDi31HkA0lJTWJg4h6HxI/KdP/frBAYNiQeg35XXsHTJIlSVeV8ncOW1g6hSpQoNIxoR0bgJ69asKnUeJ35uTskUFlKdy9rU58OFW3KmHTpyPOd11cqBqHou1y+uIZ8v38axrBPs+OMw23YdIi7yHOIiz2HbrkMk/3GY41kn+Hz5NvrFNSxRNnDe99spn1tuTqsjcGY9meKxhqAPpKWlUr9+g5zx8PD6pKamepZp4CoTGBhIUK1a7N27l9RUz2XT0vIua5kqVqbiZA7PZ7tpaWn55Ek7I9t87JH7efSp56hUKf9dyq70NMLC6wPuOgoKYt++vexKT82ZDhAWVp9d6aXP5MTPzSmZXhzRgfEfr+bEibytvXdu78L2d6+naVgt3p6z0WO50Dpnk7L3z5zx1H1/EhZSnbCQ6qdM/4vQOmeXKFtx+Pr77ZTP7XQz+3ofUBbr6UwQ8d7gKz5pCIrI4VPGh4vIm6dMWy8i0/JZ9n4R+dk9f5WI3JRr3jkiclxEbi1k2x7bMsacOfPnzuacunVp2aqNv6OYIlzWpj67Dxxh/ba9HvNu/ecymtwync2p+7nuokZ+SGeM8QdH9AiKSDMgAOgqImfnmn4r0Btor6qtgJ7kPXQ+EFgJDPFh3NMWFhZOSsrOnPHU1BTCw8M9y+x0lcnKyuLggQPUqVOH8HDPZcPC8i5rmSpWpuJkTs1nu2FhYfnkCctvFacl6bsVJM6ZTbsWTbl1VDzLvl3MmNHD85SpFxpGWmoK4K6jgwcJCalDvdDwnOkAaWkp1AstfSYnfm5OyNQp+jz6xjVk41vXMemei7n4wlDeu7NbzvwTJ5TPl2/nyo4RHsum7/2T+rl6+sJDziZt31+k7fvrlOnVSc/VQ3im+fr77YTPrSSZfVlHJ7dZ1uqp9AQR7w2+4oiGIK6G3GQgEbgy1/RxwG2qehBAVQ+q6qRTlrsPCBeR+hSsgYgsFpEtIvL4yYkicq+I/OQe7nZPayciP4hIVRE5W0Q2iMiFpXlzce3asXXrFpK3b+fYsWN8Nn0affsNyFOmb78BOVdwffnF51zc4xJEhL79BvDZ9GlkZmaSvH07W7duoV379qWJY5nKeKaiXNGvP1OnTEZVSfpuJbVq1aJeaCg9e/fhmwXzycjIICMjg28WzKdn7z6l3t74x59h7cZtrPrxF955bzJdunXnrYkf5inT5/J+fDp1MgCzZnxJl27dERH6XN6PGV98SmZmJr8lb2f7r1tp3bZdqTM58XNzQqbHP1lD01s/pfmYzxn26hKW/JTOqDe+pXG9mv/LENeQX1IPeCw7e/VOruvcmMqBlTj/3Bo0CQ1i9dY9rNm6hyahQZx/bg3OCqzEdZ0bM3v1To/lzxRff7+d8LmdLl/XEZTNejIuvrqhdDURWZ9rPASYmWt8MK6ev2jgTuATEQkCaqrqtvxWKCINgFBVTRKRT93r+EcB228PXAj8BawSkdmAAiOADrh6Gb8TkSWqukpEZgLPANWAj1X1p3y2PxoYDdCgYeEnRgcGBvLq62/Sv28fsrOzGTZ8JM1jYnjqicdo0zaOfv0HMHzkKEYOjycmOpLg4BAmT3EdJW8eE8O1AwfROrY5gYGBvDbhLQICAgrdXnFYprKbaUT8UJYuXcLePXuIatKQcY8+TlaW62T/UTffSp/LriBx7hxaNm9KterVeXviewCEhITw4CPj6d65AwAPjXuUkJCCTzgvrReffZKWrdvQ54r+DIkfwZ23jKBT62bUDg7hnfddjcKoZs3pf/V1XNyhJYGBgfzfy6+X28/NiZnAdS7SxDFdCapeGQF+3LGPu95dAcAVcQ1o0+Qcnpm+jk0p+/lixXbWvHo1WSeUe/+9Iuc8w/veW8mM8ZcSUEn4aNEWNqXsL2SLhXPa99uJn5vT6gicWU/eJjinN600RPO7POxMb0TksKrWyDU+HIhT1TtEJA54XVU7i0gAsAOIBbKAHaoaXMA67weCVXW8N4n0BQAAIABJREFUiMQC76tqXD7lhgOXqOpN7vGngH24GoJ1VPUx9/Sngd2qOkFEKgOrgKPARapa6HXsbdvG6fLvVp9OlRiTr6zsE/6O4OHw0Sx/R/BQ++zK/o5QJtQZ8oG/I3j4/eNh/o6QR2CA836VO3E/4MR6qnaWrMnv976vNGneUv9vSsG38Smt69vU98n7c8Ij5oYA0SKS7B4PAq5V1XdF5LCINC6gV3AIUE9EbnCPh4nIBbh6/k4e/v2b++eprd2iWr91gBrAWUBVwHsnvBhjjDGmTPLluXze4tcmvohUAgYBLVQ1QlUjcJ0jePLij+eAt9yHiRGRGiJyk4g0BWqoaniu5Z4DhqjqV6rayj2c7KbrLSIhIlINuApYDiwFrhKR6u4LVK52TwP4F/B3YArwgndrwRhjjDHGP/zdI9gVSFXV3Dcy+hZoLiKhwNu4euZWichx4Diu8wCHAF+dsq4vgOnAU/lsJ8k9vz6uc/5WA4jIh+55AP9W1XXu29McV9VP3Ieq/ysil6jqN6V/u8YYY4wpL8p+f6CPGoK5zw90j38IfOge7XjKvGygXq5JL7qHorbxA+DxXKRTtnXqvFeAV06Z9hHwUa4sHYratjHGGGNMWeTvHkFjjDHGmLJHysc5gtYQNMYYY4w5TeXl9jHl4T0YY4wxxpgSsB5BY4wxxpgSKA+Hhq1H0BhjjDGmgrIeQWOMMcaYEij7/YHWI2iMMcYYU2FZj6AxxhhjTAmUg1MErUfQGGOMMaaish5BY4wxxpjT5LqPYNnvErQeQWOMMcaYCsp6BI0xxhhjSsDOETTGGGOMMWWW9QiWU0ePZfs7goeqlQP8HSGPrOwT/o5gjFelTrrJ3xE8tBo/z98R8lj91KX+juAhMKAcdDNVCILYOYLGGGOMMaassh5BY4wxxpgSKA/nCFpD0BhjjDHmNNntY4wxxhhjTJlmPYLGGGOMMadLysehYesRNMYYY4ypoKxH0BhjjDGmBKxH0BhjjDHGlFnWI2iMMcYYUwJ2Q2ljjDHGGFNmWUPQRxLnzSU2JoqY6EheevF5j/mZmZncOHQwMdGRdL2oAzuSk3PmvfTCc8RERxIbE8X8xDPzeKaUlJ30v7wnHdu2oFNcLO+8NcGjjKry0P1306ZFFJ3bt+b7dWtz5k39+CPaxkbTNjaaqR9/dEYygfPq6bbRo2jUoB7t28TmO19VeeDeu2jZvCkd41qxPlcdTZk8iVYxUbSKiWLK5ElnJI9TMwFkZ2fTu2t74gdf5TEvMzOTW0bcQKfWzbiiZxd27kjOmTfhlRfp1LoZXeIuZNHCxDOWx2nfJSdmcsJ+oHJgJb4Y24mEezsz5/4u3HVppGvdt3dg5j2dmXlPZ5b/vQdvD2+T7/JXx4Wz4KFuLHioG1fHhedMjwkPYvZ9XVj4cDf+fmWzEmUDZ9TRqZy6D3Da99vbBKgk3ht8xRqCPpCdnc3dY8cwI2EO637YyGfTprJp48Y8ZT58/z2Cawez4eet3HnXPYwf9xAAmzZu5LPp01j7/QZmzprLXXfeTnZ26Z8jHBgQyDP/9xIr1/xI4qLl/Hvi2/y8KW+m+fPm8OvWLaz54Wdee/Nt7rt7DAAZ+/bxwnNPs2Dxf1m4ZAUvPPc0+zMySp3JifV0Q/wwvpr5dYHzE911tH7DZia89Q73jHXV0b59+3j+2af5ZukKFi1byfPPPk3GGagjp2YCePftN7ggKjrfeVMnf0Ct2rVZsW4To28fyzNPjAdg88+bmPHFpyxeuZ5PPk/gkfvGnpHPzYnfJSdmcsJ+4FjWCeLfSaL/K8vp/8pyukbXpVXD2gz553cMeHU5A15dzrod+5n34y6PZWtVO4s7e0dy7YQVXDPhv9zZO5Kgaq4znp66Nobxn/1Ez+e/JaLu2XSLPqcENeSMOjqVE/cBTvx+m+KxhqAPrEpKokmTSBo1bkzlypUZOPh6ZiXMyFNmVsIMbogfBsA1117H4m8WoqrMSpjBwMHXU6VKFSIaNaJJk0hWJSWVOlO90FBatnb9hV2zZk2aRkWTnpaap8zXsxO4fmg8IkK79h05cOAAu9LTWbggke6X9CI4JITawcF0v6QXC+aX/i84J9ZTl67dCA4OKXD+7ISZDLnBVUftO3Rk//79rjqaP48ePXsREhJCcHAwPXr2YkHi3FLncWqmtNQUFibOYWj8iHznz/06gUFD4gHod+U1LF2yCFVl3tcJXHntIKpUqULDiEZENG7CujWrSp3Hid8lJ2Zyyn7gr2OuX/qBAcJZlQRFc+bVqBJIp8g6LPjpD4/lukadw/Jf9nDgyHEOHsli+S976BZVl7o1q1CjaiDrf9sPwFerU+kdc16JsjmljnJz4j7Aid9vXxAv/vMVawj6QFpaKvXrN8gZDw+vT2pqqmeZBq4ygYGBBNWqxd69e0lN9Vw27ZSdUGn9tiOZH75fT9t2HfJMT09LJbx+/ZzxsLBw0tNTSU9LpX6u6eHh4R47xpJwej0VlDk8n+2mpaXlkyfN63n8lemxR+7n0aeeo1Kl/Hcpu9LTCAt3fWcCAwMJCgpi37697EpPzZkOEBZWn13ppc/kxO+SEzPl5s/9QCWBmfd05rsnerJsy16+/+1AzrxeF57Liq17OZyZ5bHcebWqkr7/aM74rgNHOa9WVc6rVYVd+UwvLafsK4vij32A07/fpmA+bwiKyNciUltEIkTkJx9sb4CIPOx+fZWINC+ifHcRmeXtXE5x+PBhbho6iOdefIWgoCB/xzFl0Py5szmnbl1atsr/HC7jfP7eD5xQGPDqcro8vYiWDWpxQb0aOfP6tw4jYZ1v/ogqjL/ryDiTiPcGX/FpQ1BEBOinqvt9tU1VnamqJ89avQootCHoDWFh4aSk7MwZT01NITw83LPMTleZrKwsDh44QJ06dQgP91w2LCzvsiV1/Phxhg0dyMDBQ+h/5dUe80PDwklNSckZT0tLJTQ0nNCwcFJyTU9NTSX0DGRyaj0VlTk1n+2GhYXlkyfM63n8kSnpuxUkzplNuxZNuXVUPMu+XcyY0cPzlKkXGkZaqus7k5WVxcGDBwkJqUO90PCc6QBpaSnUCy19Jid+l5yYCZy1Hzh0NIuVv+6jW1RdAIKrn0Vsg1os2rQ73/K/HzhKaO3/9fTVq1WV3w8c5fcDmdTLZ3pJOamOisMf+yWnfr+9zQ4NF4O752+ziHwE/ARki8jJs3YDRWSKiGwSkc9FpLp7mZ4isk5EfhSR90Wkinv68yKyUUR+EJGXRSRARLaLS20RyRaRbu6y34rIBSIyXETeFJGLgAHASyKyXkSaiEikiCwQke9FZK2INHHnquHO87M7X6k+kbh27di6dQvJ27dz7NgxPps+jb79BuQp07ffgJwruL784nMu7nEJIkLffgP4bPo0MjMzSd6+na1bt9CuffvSxAFcV5XdedvNNI1qxpix9+Rb5vK+/Zj2yWRUlVVJKwkKCqJeaCg9e13KooXz2Z+Rwf6MDBYtnE/PXpeWOpMT66koV/Trz9QprjpK+m4ltWrVctVR7z58s2A+GRkZZGRk8M2C+fTs3cfrefyRafzjz7B24zZW/fgL77w3mS7duvPWxA/zlOlzeT8+nToZgFkzvqRLt+6ICH0u78eMLz4lMzOT35K3s/3XrbRu267UmZz4XXJiJifsB0LOrkzNqq4LPKoEVqLzBXXY9sdhAC5rWY9Fm/7gWNaJfJddunkPXaLOIahaIEHVAukSdQ5LN+9h96FMDh/NolXD2oD7yuINnucYFocT6uh0+WO/5MTvtykeX91Q+gJgmKquFJHkXNOjgFGqulxE3gduF5E3gQ+Bnqr6i7sBeZuITAauBqJVVUWktqpmi8hmXL18jYC1QFcR+Q5ooKpbRKQzgKr+V0RmArNU9XMAd7nnVfUrEamKq2HcAGgNxABpwHKgM7As9xsSkdHAaIAGDRsW+uYDAwN59fU36d+3D9nZ2QwbPpLmMTE89cRjtGkbR7/+Axg+chQjh8cTEx1JcHAIk6dMA6B5TAzXDhxE69jmBAYG8tqEtwgICDjtD+BUK1csZ/rUj2ke04KuHdsC8Pcnns75q2zk327h0j5XMH/eXNq0iKJateq89a9/AxAcEsIDD43nkm4dAXjw4UcJDin4xOXicmI9jYgfytKlS9i7Zw9RTRoy7tHHyco6DsCom2+lz2VXkDh3Di2bN6Va9eq8PfE9AEJCQnjwkfF07+w6l+ihcY8ScgbqyKmZ8vPis0/SsnUb+lzRnyHxI7jzlhF0at2M2sEhvPO+q1EY1aw5/a++jos7tCQwMJD/e/n1M/K5OfG75MRMTtgP1A2qwkvXx7pumVFJ+Pr7XTk9gP1ahfKvb7blKX9h/SCGdmrIuM9+4sCR47w1/1e+uusiAN6cv5UDR1z/Fx7/cgMvXh9L1cAAlmzezZKf8+9VLIoT6uhUTtwHOPH77W0nbx9T1omqFl2qNBsQiQAWqWoj93gyEAfUAL5V1Ybu6ZcAY4HHgTdU9WTPXk9gDDAIWOMeZuFq0B0TkfHAPlwNwZXAzcCzwFhVHSQiw4E4Vb1DRD50L/e5iNQENqnq/87kdW2vOzBeVXu7x98GlqvqxwW9x7Zt43T5d6tLU01n3NFjzrv0vmplZ/3HzsrOv5fB5HX4qOdJ+v5W++zK/o5QJjhxPxD32Jm7V+SZsPop7/fQna7AAOe1LgIDnHdtabWzZI2qxvlr+9EXttKJX37jtfVfHFXHJ+/PV5/snwVMP7UVWmCrVFWzgPbA50A/4OQ1798CXd3zvgZqA92BpSWPS2au19nYo/iMMcYYk4c3zxAsR+cIFqGhiHRyvx6K6/DrZiBCRCLd0+OBJSJSA6ilql8D9wAt3fOTgIuAE6p6FFgP3IKrgXiqQ0BNAFU9BKSIyFUAIlLl5DmKxhhjjDEVgb8bgpuBMSKyCQgG3nY35kYAn4nIj8AJ4B1cDbhZIvIDrgbjvQCqmgnsxHVYGFw9gTWBH/PZ3jTgAfeFKE1wNTLHutf5X6Ced96mMcYYY8oVL946xpe3j/H6IU9VTQYuzDUe4X65B8j3eVSquhDXBRu5peM6/Jtf+a65Xn8CfJJr/ENcF5+gqsvxvH3MJaeMbwMW51r+jvy2aYwxxhhT1tm5b8YYY4wxJeC8y3pOn78PDRtjjDHGGD+xHkFjjDHGmNPkuo9g2e8TtB5BY4wxxpgKynoEjTHGGGNKoOz3B1qPoDHGGGNMhWU9gsYYY4wxJVEOugStIWiMMcYYUwK+fBSct9ihYWOMMcaYCsp6BI0xxhhjSqAc3D3GegSNMcYYYyoq6xE0xhhjjCmBctAhaA3B8iowoDx8Pb0rK1v9HcFD1coB/o7goepZzstkyq71z/bxd4Q8nl+01d8RPDzaq6m/I3jIyj7h7wjGS6whaIwxxhhTEuWgz8XOETTGGGOMqaCsR9AYY4wx5jQJdh9BY4wxxhhThlmPoDHGGGPM6RK7j6AxxhhjjCnDrEfQGGOMMaYEykGHoPUIGmOMMcZUVNYjaIwxxhhTEuWgS9AagsYYY4wxp03s9jHGGGOMMabssoagjyTOm0tsTBQx0ZG89OLzHvMzMzO5cehgYqIj6XpRB3YkJ+fMe+mF54iJjiQ2Jor5ifPOSJ7bRo+iUYN6tG8Tm+98VeWBe++iZfOmdIxrxfp1a3PmTZk8iVYxUbSKiWLK5ElnJM9JTqunlJSd9L+8Jx3btqBTXCzvvDXBo4yq8tD9d9OmRRSd27fm+1x1NfXjj2gbG03b2GimfvzRGclkdVQ8TqsnJ2Zy4mfnhH3Twd3pfPxQPP8afQX/uqUvSf9xrWvxR6/x7m39eXfMlXwybiSH9v6e7/I/zP+Kf466lH+OupQf5n+VMz19y09MvK0//xzZm3lvP4NqyZ937rTvkhM+N38Q8d5Q9LblfRH5Q0R+yjUtRETmi8gW98/gIlekqjaUcmjTpq0eOa4FDoePZmmjxo114+Zf9cCfmdqiRayu/X5DnjKvTXhL/3bzLXrkuOqkj6fqtQMH6ZHjqmu/36AtWsTq/sNHddMv27RR48Z6+GhWods7clz10NHsQoc58xfp0hWrtFnzmHznf/6fBO19aR89eCRLFy5ZrnHt2uuho9m6I223RkQ00h1pu/W39D0aEdFIf0vfU+T2Dh3NLjKzr+sp48+sIodNW3fq4mVJmvFnlv62K0ObRF6gK1b/kKfM9C9mas/efXTf4eOauGiZto1rpxl/Zum2nX/o+RGNdNvOP3R7ym49P6KRbk/ZXej2nFZHxaknX9eRU+vJiZmc+Nk5bd80fs5mj2HslKU68o0vdfyczXr/F2s0JDxCR78zW+//fE1OmUtvHa+trxjssey9n36ntevV13s//U7v/TTJ/TpJx8/ZrKFNW+jwV6bruK9/1sZxXXXwUxPz3b4Tv0tO+9wOHc1WYLU/f/c3a9Fa1+046LWhqPcHdAPaAD/lmvYi8LD79cPAC0W9D+sR9IFVSUk0aRJJo8aNqVy5MgMHX8+shBl5ysxKmMEN8cMAuOba61j8zUJUlVkJMxg4+HqqVKlCRKNGNGkSyaqkpFJn6tK1G8HBIQXOn50wkyE3xCMitO/Qkf3797MrPZ2F8+fRo2cvQkJCCA4OpkfPXixInFvqPODMeqoXGkrL1m0AqFmzJk2joklPS81T5uvZCVw/1FVX7dp35MCBA666WpBI90t6ERwSQu3gYLpf0osF80v317fVUfE4sZ6cmMmJn50T9k01Q84lNDIGgCrVa1CnQWMO7f2dKmfXyClz7OiRfM8P27ZmGY1ad6ZazdpUq1mLRq07s23NUg7t+4Njfx0mvFkrRITYnlfxy4qFJcrnxO+SEz43XxMvD0VR1W+BfadMvhI42a06CbiqqPVYQ9AH0tJSqV+/Qc54eHh9UlNTPcs0cJUJDAwkqFYt9u7dS2qq57Jpp+yovZU5PJ/tpqWl5ZMn7Yxt08n19NuOZH74fj1t23XIMz09LZXw+vVzxsPCwklPTyU9LZX6uaaHh4d7/JI9XVZHxePEenJiptyc8tkVxdf7pv2/p/D7r5sIj2oJwKIPX2VC/MVsWJRAt/i7PMof2vM7QXXr5YzXPOc8Du35nUN7fqfmObmn1yvw0HJRnP5dKiizr3+nlAPniMjqXMPoYixznqqmu1/vAs4raoFy2RAUkcPunxEickRE1ovIRhH5SETOcs/rLiIH3PNODr3c88aLyAYR+cE9vUNh2zPl3+HDh7lp6CCee/EVgoKC/B3HkayOyi777PJ37MiffPHMWHrfMi6nN7DH8HsYO3kJMT36szrhYz8nNH7n3S7BPaoal2uYeDrR1HV8uMgTUctlQ/AUv6pqK6AFUB8YlGveUlVtlWtYICKdgH5AG1WNBXoBO0sTICwsnJSU/60iNTWF8PBwzzI7XWWysrI4eOAAderUITzcc9mwsLzLekNYWDip+Ww3LCwsnzxhZ2ybTqyn48ePM2zoQAYOHkL/K6/2mB8aFk5qSkrOeFpaKqGh4YSGhZOSa3pqaiqhpcxkdVQ8TqwnJ2YC5312RfHVvik76zhfPDOWC3v0J7rzpR7zL+zRn83LEz2m1zznPA7u3pUz7uoJPM/dM5h7+i5q1imysyZfTv0uFZXZ179TKqjfRSQUwP3zj6IWqAgNQQBUNRtIAor6xofiaoVnupfbo6ql6qeOa9eOrVu3kLx9O8eOHeOz6dPo229AnjJ9+w3IuVrqyy8+5+IelyAi9O03gM+mTyMzM5Pk7dvZunUL7dq3L02cYrmiX3+mTpmMqpL03Upq1apFvdBQevbuwzcL5pORkUFGRgbfLJhPz959zsg2nVhPqsqdt91M06hmjBl7T75lLu/bj2mfuOpqVdJKgoKCXHXV61IWLZzP/owM9mdksGjhfHr28vyFcjqsjorHifXkxExO/OyK4ot9k6oy+7Xx1GnQmA7XjMiZvi81Oef1LysWUqd+Y49lG7ftwra1yzhy6ABHDh1g29plNG7bhZoh51K5eg1SN61HVflh4X9o2rFnifI58btUFH/8TvEF8eK/EpoJDHO/HgbMKKQsUIFuKC0iVYEOQO6TOrqKyPpc49cCicBjIvILsACYrqpL8lnfaGA0QIOGDQvddmBgIK++/ib9+/YhOzubYcNH0jwmhqeeeIw2bePo138Aw0eOYuTweGKiIwkODmHylGkANI+J4dqBg2gd25zAwEBem/AWAQEBpagJlxHxQ1m6dAl79+whqklDxj36OFlZxwEYdfOt9LnsChLnzqFl86ZUq16dtye+B0BISAgPPjKe7p1dR8sfGvcoISEFnyB8OpxYTytXLGf61I9pHtOCrh3bAvD3J57O+Qt25N9u4dI+VzB/3lzatIiiWrXqvPWvfwMQHBLCAw+N55JuHQF48OFHCS5lXVkdFY8T68mJmZz42Tlh35SyYQ0/LpzBuRFNeXfMlQD0GHYv6xM/Z1/KdkSEoHPDufzOJwFI++VH1n49jX53P0u1mrXpMuR2PrjrOgC6Dh1DtZq1AbhszOPMeuURjmcepUm7bjRp161E+Zz4XXLC51bRiMhUoDuucwlTgMeB54FPRWQUsIO8R0HzX4/7EuNyRUQOq2oNEYkANgGbgUbAbFUd6i7THbhfVfvls3wA0BXoAdyC61LsDwvaXtu2cbr8u9Vn+F2UTlb2CX9H8BAY4KwO6KPHsv0dwUPVyqXfIZ9pVk9llxM/u8AAZz2J4flFW/0dwcOjvZr6O4IHJ/5OqVk1YI2qxvlr+zGxbXTa1996bf2xDWr65P056zezd5w8R7AJ0FZEBhS1gKpmq+piVX0cuANXT6ExxhhjTLlSERqCgOtcP1w3V3yksHIiEiUiF+Sa1ApX96oxxhhjTA5/3kfwTKkw5wi6/Qd4QkS6usdPPUfwGWA78IaI1AaygK24zwU0xhhjjAF832LzknLZEFTVGu6fycCFuaYr0DJX0VoFrOIir4UzxhhjjHGIctkQNMYYY4zxtlLc5sUxKsw5gsYYY4wxJi/rETTGGGOMOU0CSNnvELQeQWOMMcaYisp6BI0xxhhjSqAcdAhaj6AxxhhjTEVlPYLGGGOMMSVRDroErUfQGGOMMaaCsh5BY4wxxpgSsPsIGmOMMcaYMst6BI0xxhhjSsDuI2iMMcYYY8os6xEspwIDrI1flMCAcvCnnA8cPZ7t7wgeqlYO8HeEMsHqqWiP9mrq7wgeggdM8HcEDxkzx/o7giOVh98i1hA0xhhjjCmJctAStG4jY4wxxpgKynoEjTHGGGNOk2C3jzHGGGOMMWWY9QgaY4wxxpwusdvHGGOMMcaYMsx6BI0xxhhjSqAcdAhaj6AxxhhjTEVlPYLGGGOMMSVRDroErUfQGGOMMaaCsoagjyTOm0tsTBQx0ZG89OLzHvMzMzO5cehgYqIj6XpRB3YkJ+fMe+mF54iJjiQ2Jor5ifMskw8z3TZ6FI0a1KN9m9h856sqD9x7Fy2bN6VjXCvWr1ubM2/K5Em0iomiVUwUUyZPOiN5wHl1dFJ2dja9u7YnfvBV+Wa6ZcQNdGrdjCt6dmHnjv9lmvDKi3Rq3YwucReyaGHiGcvjxHqyTGUvj9MyVaokrHhjCF880R+Aiff0YtP7w1j5xhBWvjGE2Mbn5LvcDT2j+fHdm/jx3Zu4oWd0zvTWkXVZ9c+h/PTvm/jHLd1Klc1J9eQb4tV/PqOqNpRyaNOmrR45rgUOh49maaPGjXXj5l/1wJ+Z2qJFrK79fkOeMq9NeEv/dvMteuS46qSPp+q1AwfpkeOqa7/foC1axOr+w0d10y/btFHjxnr4aFah2yvOYJlUDx3NLnKYM3+RLl2xSps1j8l3/uf/SdDel/bRg0eydOGS5RrXrr0eOpqtO9J2a0REI92Rtlt/S9+jERGN9Lf0PUVuz2l1dOS4avr+zGINjz/zgl593WDt1edyj3nPvfy6xo/4m6bvz9S335usA66+TtP3Z+rileu1eUwLTf79oH63/mc9P6KRpuz9q8htObGeLFPpMzktj78yVb389QKHByd+q9MW/ayzv9umVS9/XT9K3KBDnpld6DKhA9/RbWn7NXTgO1rP/brewHe06uWv66qf07Xb3dO16uWv69xV23XA3/+T7zqcWE/Aan/+7r+wZRv99Y8jXht89f6sR9AHViUl0aRJJI0aN6Zy5coMHHw9sxJm5CkzK2EGN8QPA+Caa69j8TcLUVVmJcxg4ODrqVKlChGNGtGkSSSrkpIsk48ydenajeDgkALnz06YyZAb4hER2nfoyP79+9mVns7C+fPo0bMXISEhBAcH06NnLxYkzi11HifWEUBaagoLE+cwNH5EvvPnfp3AoCHxAPS78hqWLlmEqjLv6wSuvHYQVapUoWFEIyIaN2HdmlWlzuPEerJMZS+P0zKF16nBZe0i+GDehtNarnfb81m47jcyDmey/3AmC9f9xqVtz6decHVqVq9M0uZdAHyy8Gf6d2xcomxOqidfEvHe4CvWEPSBtLRU6tdvkDMeHl6f1NRUzzINXGUCAwMJqlWLvXv3kprquWxaWt5lLZP3MhUnc3g+201LS8snz/+3d+fxWs75H8dfb2UppUWoU9JCpWg/soRIC2UZpskWWQozP8zMjxnbGMYYM5bfjG0GM8YSQrKEQZElRJvKNohCnaJSVFpPn98f3+uu+2ydU53u73Xq8/Q4D+e6rvu+r3fXuZfv/V0LKuV8abxGV19+CVf94Qa22670t5R5cwvIa9xkfaZdduG77xYyb+6cdfsB8vKaMG/u1nmdPFPVy5O2TDeddxhX/vtN1q61IvuvOfMgJtx5KjcOOZQdqlcrcb+8XXdm9oKl67bnLFxK3q47k9egFnOy9y9YSl6DWpuULU3XKVe0hX9ypUoVBCUtLf9WFXqcwZLuqIzHcm5bN+bF52mw22506Ng5dhS57RXpAAAgAElEQVTntlpHH9CMbxf/yHsz5hfZf/X9b9Nh6DC6X/wY9WrvxP8O6BIpoauqqlRBMBZJmzXNTl5eY2bP/nrd9pw5s2ncuHHJ23wdbrNmzRp++P57dt11Vxo3LnnfvLyi9/VMWy5TRTLPKeW8eXl5peTJq5Tzpe0aTXh3PKNfeJ78/Vtx/jmDePON1/jF0MFFbtOwUR4Fc2avz/TDD9SvvysNGzVetx+goGA2DRttndfJM1W9PGnKdFDbPPof2IL/3jeYB3/blx7tm/DvS3ozb9GPAKxaU8iDYz6ia+s9Sty3YOEymmTV9DXetRYFC5dRsGApjbP3N6hFwYJNq29Jy3XKua2gSrDKFgQlXSppoqTpkq7N2v+0pMmSPpQ0NGv/WZI+lTQBOCRr/26SRiaPNVHSIcn+ayQNk/QWMGxzsnbNz2fGjM+YNXMmq1atYsRjj9Kv/3FFbtOv/3HrRpY+OfIJDj/iSCTRr/9xjHjsUVauXMmsmTOZMeMz8g84YHPieKZKdEz/Yxn+8DDMjAnvvkOdOnVo2KgRPXv1YezLY1i0aBGLFi1i7Mtj6Nmrz2afL43X6Mrf/5EpH33BxPc/5a57h9H9sB7cec/9RW7T5+j+PD48vIyee+ZJuh/WA0n0Obo/z4x8nJUrV/LVrJnM/HwGnbrkb3amNF4nz1T18qQp09X3v83eZ/ybNmfdzxl/eZHXps/m7JtH07BezXW3Oe6gFnw0a2GJ+46Z/CVHdW5K3Vo7UrfWjhzVuSljJn/JvEU/suTHVRzQuiEAp/Zsw3PvfLFJ+dJyndzGq5ITSkvqDewDHEAoN4+SdJiZvQGcbWbfSaoBTJQ0EtgBuBboAnwPvAq8lzzcrcBfzexNSU2Bl4B9k2Ntge5mtnxz8lavXp2/3noHx/brQ2FhIWcOPpu27drxh2uupnOXrvQ/9jgGn30OZw8eRLs2e1OvXn2GPfxoCNCuHScN+Bmd2relevXq/O22O6lWrWQfEM+0ZTKdNehUxo17nYULFtC6ZVOuuOr3rFmzGoBzhpxPn77HMPrFF+jQthU1atbkH/fcC0D9+vX5zeVX0uOQbgD89oqrqF+/7EEnFZXGa1SWG6+/lg6dOtPnmGM5ZdBZXHjeWRzUaV/q1qvPXf8OhcLW+7bl2J/8lMO7daB69er86eZbt9rnkmeqennSminbfb/pQ4M6NRBi+hfzufCOVwHovM/unHvM/vz81ldYtHQlNwyfyJt/GwjAn4ZPYNHSlQBc/PfXuOdXvaixY3VGT5rFS5O+3KQcab9OW0pOp3nZQmRm5d8qJSQtNbNakm4GfgosTg7VAm4ws3slXQP8JNnfDOgDNARONLMzkse5CGhlZv8j6Vsgu3f6bkBr4BLAzOxaSpHUNg4F2LNp0y6ffr5pLx4Xz5rCtbEjlFC9Wvoq6RcvWxU7Qgl1d94hdgTntph6x90WO0IJi0ZdFDtCCTW212Qz6xrr/O07drFnX3l7iz1+swY75eTfVyVrBAm1gDeY2d1Fdko9gKOAg8zsR0mvATuV81jbAQea2YpijwWwrKw7mdk9wD0AXbp0rTqlaeecc85VilxO87KlpK/6oWJeAs6WVAtAUmNJuwN1gEVJIbANcGBy+3eBwyXtKml7YEDWY40GLsxsSOqYk3+Bc84551xkVbJG0MxGS9oXGJ/U3C0FTgdeBM6X9DHwCfBOcvu5SZPxeEJz8tSsh7sIuFPSdML1eAM4P0f/FOecc85VUVtBhWDVKgiaWa2s328lDPQo7ugy7nsfcF8p+xcAA0vZf80mB3XOOeecqwKqVEHQOeeccy4VcrwU3JZSVfsIOuecc865zeQ1gs4555xzm6TqVwl6jaBzzjnn3DbKawSdc8455zaS2Dr6CHpB0DnnnHNuE2wF5UBvGnbOOeec21Z5jaBzzjnn3CbYGpqGvUbQOeecc24b5TWCzjnnnHObQFtBL0GvEXTOOeec20Z5jaBzzjnn3Kao+hWCXiPonHPOObet8hrBrdSawrWxI5RQvZp/76iK6u68Q+wIbhMtWLIydoQSGtTeMXaEIpauWBM7QgmLRl0UO0IJzS54InaEVNoKKgS9RtA555xzblvlNYLOOeeccxtJ8nkEnXPOOedcFeY1gs4555xzm8DnEXTOOeecc1WW1wg655xzzm2Kql8h6AVB55xzzrlNsRWUA71p2DnnnHNuW+U1gs4555xzm8Cnj3HOOeecc1WWFwRzZPRLL9K+XWvatdmbm278c4njK1eu5PRTB9Kuzd4cenA3vpw1a92xm/5yA+3a7E37dq0ZM/qlSslzwdBzaL5nQw7o3L7U42bGpb++mA5tW3Fg145MfW/KumMPD3uAju1a07Fdax4e9kCl5Mnw61S+tF0jz1S1Mx3UoRVHHdKFPocdwDFHHlziuJlx9WW/pnuXtvTq3pX3p7237tiI4cM4tGs7Du3ajhHDh1VKnrRdoxUrVtC7x0H0OKgz3fM78Jfrry0107lnnkp+hzb0OeJgvvpyfaa/3fwX8ju04cBO7Rj78uhKyQTpuk7bCcb8rifDLjwEgKYNavKfy49k/PV9uXtoN7avVnq12YVHt2b89X1587o+9Gi3x7r9R7Tbgzev68P46/vyP31bb3a+LUdb9L+cMTP/2cyfzp272PLVVubP0hVrrHmLFvbRJ5/b98tW2v77t7cp0z4scpu/3XannTvkPFu+2uyBh4bbSQN+ZstXm02Z9qHtv397W7x0hX386RfWvEULW7pizQbPt3y12ZIVhRv8eWHMqzZu/ETbt227Uo8/8fSz1qt3H/th+Rp75fW3rGv+AbZkRaF9WTDfmjVrbl8WzLev5i6wZs2a21dzF5R7viUrCsvNnOvrVJHMub5OabtGFfnxTOnN9PV3K8r9abJnU5v22ewyjz/w2NPWo2dv+2rhcnvmpdetY+d8+/q7FTb98wJrulczm/55gb3/xVxrulcze/+LueWeL23XaP6S1Rv8+faHVTZz7iKbv2S1FXz3o3Xumm8vvDKuyG3+8n+32ZlnD7H5S1bbPfc9ZMefOMDmL1ltb06cZu32299mL1hqk97/1Jo1b2HzFq8o95xpfC7tce6IMn+ufmyqjXznSxs9rcD2OHeEPTPxKxt693jb49wRdv9rM+w3wyaXuM+hv3vRPvhqke15/kjLv+x5m/nNEms0ZIQ1GjLCZn6zxPIv+481Oe8J++CrRXbo714s9bzApJif/R06dbGFS9dssZ9c/fu8RjAHJk6YQMuWe9O8RQt22GEHBgw8meeefabIbZ579hlOG3QmACee9FNeG/sKZsZzzz7DgIEns+OOO9KseXNattybiRMmbHam7oceRr169cs8/vyzozjltEFI4oBuB7J48WLmzZ3LK2Ne4oieR1G/fn3q1avHET2P4uXRL252HvDrVBFpvEaeqepmqojR/3mWk04+DUl0zu/GDz8s5pt5c3l97BgO7dGTevXqU7duPQ7t0ZPXXtm8Gq80XiNJ1KpVC4DVq1ezevVqVKxj2AvPP8vAUwcBcOwJJzHutbGYGS889ywnnDSQHXfckb2aNadZi5ZMmbR1PZca1avBUfs34uE3Z67bd0jr3Xlu8hwAHn/7S/p2yitxvz4d83h64tesWrOWrxb8yMz5S+nUvD6dmtdn5vylfLVgGasLjacnfk2fjiXvnwZi/TJzW+InV7wgmAMFBXNo0mTPdduNGzdhzpw5JW+zZ7hN9erV2aVOHRYuXMicOSXvW1BQ9L5bKnPjUs5bUFBQSp6CSjunX6fyz5e2a+SZqm4mCAWd007qzzFHHMTD9/+rxPF5cwvIa9xk3XajvMbMm1vAvIICGmXtb5jXmHmb+RxP6zUqLCykx8Fd2LdFHj2OOIou+d2KHJ9XULDufSCT6buFC5k7dw6Nm6y/Rnl5jZk7d+t6H7huYAeue2I6tjZs16+1Az8sX03hWgNg7qLlNKpbo8T9GtWtQcF3y9dtZ25X1n635WyVBUFJzSQtlzRV0keSHpS0fXKshySTdGzW7Z+T1CNru4Gk1ZLOjxDfOedyZuR/xvLCa+/w4OPP8MC9d/PO2+NiR0qdatWq8drbk5n+31lMmTyRjz/6IHakVOjVvhELfljJ9K8Wx47iNsNWWRBMfG5mHYH9gSbAz7KOzQau3MB9BwDvAKdURpC8vMbMnv31uu05c2bTuHHjkrf5OtxmzZo1/PD99+y66640blzyvnl5Re+7JeTlNWZOKefNy8srJU/lVNv7darY+dJ2jTxT1c0EoYYPoMFuu9O333FMnTypyPGGjfIomDN73fbcgjk0bJRHw7w85mbtn1cwh4ab+RxP6zXKqFO3Lt0P68HYMUWbwBvm5a17H8hkqr/rrjRq1Jg5s9dfo4KCOTRqtPW8D+S33JXeHRsx8YajuWtoNw5pvRvXndyRXWpsT7XtQttmo3o1mLt4eYn7zl28nLz662v6Mrcra7/bclJZEJS0s6TnJU2T9IGkgZLyJb2d7JsgqXZS8zdO0pTkp8SQNzMrBCYA2c/0acD3knqVEeEU4H+BxpKalHGbCuuan8+MGZ8xa+ZMVq1axYjHHqVf/+OK3KZf/+PWjSx9cuQTHH7EkUiiX//jGPHYo6xcuZJZM2cyY8Zn5B9wwOZGKtcx/Y9l+MPDMDMmvPsOderUoWGjRvTs1YexL49h0aJFLFq0iLEvj6Fnrz6Vck6/TuVL4zXyTFU304/LlrF0yZJ1v7/x6iu03rddkdv0Oro/Ix99GDNjysR3qb1LHfZo2IjDj+zFG6++zOLFi1i8eBFvvPoyhx9Z1ltqxaTxGi2YP5/vF4car+XLl/Pa2JfZp1XRkax9j+nPY4+EUdPPPj2S7ocfgST69uvP0yMfY+XKlXw5ayYzP59B565bz3PpT099QOff/If8y1/g/Hve5a1P5vOLf03g7U/m079L+Mj92cF78dLUks3ho6fN5YT8Pdmh+nY0bVCTFrvX4r2Z3zF11iJa7F6Lpg1qsn01cUL+noyeNneT8uXC1tBHMK0TSvcFCsysH4CkOsB7wEAzmyhpF2A58C3Qy8xWSNoHGA50zX4gSTsB3YCLi53jeuA6YEyx2+8JNDKzCZIeBwYCtxQPKGkoMBRgz6ZNN/iPqV69On+99Q6O7deHwsJCzhx8Nm3bteMP11xN5y5d6X/scQw++xzOHjyIdm32pl69+gx7+FEA2rZrx0kDfkan9m2pXr06f7vtTqpVq1buBSzPWYNOZdy411m4YAGtWzbliqt+z5o1qwE4Z8j59Ol7DKNffIEObVtRo2ZN/nHPvQDUr1+f31x+JT0OCX1kfnvFVdSvX/Zgio3h16l8abxGnqnqZpo//xuGDBoIQOGaNRz/04EccVRvht33TwAGnTWEI3v1ZeyYF+nepS01atTkljvuAaBevfpcdMnl9O8Zpgy5+NIrNjiwqiLSeI2++WYu/3Pe2awtLGTtWuP4E39K76P78ec/XkPHTl3o2+9YTjvjbH4+ZDD5HdpQr1497rnvYQDa7NuO404cQPf89lSrVp0/33LbVvtcynbdyPe5e2g3LjthPz74ajGPvDkLgN4dGtFxr3rcOOojPin4gVGTZvPGtb1Zs9a4/JGprDXAjCsemcrwXx5KNYnhb83ik4IfKjWfK0pmFjtDCZJaAaOBx4DngMXAXWZ2SLHb1QHuADoChUArM6spqRnwMfAJ0Bx43sxOTe7TA7jEzPpLep3QRHwZcLOZvSbpEqCemV0pqT3wbzMrUrgsrkuXrvbWu5M2dJOcW1O4NnaEEqpXS1cFtF8jt7VbsGRl7AglNKi9Y+wIRSxdsSZ2hBJq7ZS+OppmFzwRO0IJ3/xrwOTyPp+3pE6du9prb2250ft1a1bLyb8vfc82wMw+ldQZOAb4IzC2jJv+CvgG6EBo5l6RdexzM+soqQHwlqTjzGxUsftfD1wFZL8TnAI0lHRasp0naR8z+2zz/lXOOeec22rkuAl3S0ll9YOkPOBHM3sIuInQtNtIUn5yvLak6kAdYK6ZrQUGASXqt81sAaHG7/JSjo0G6gHtk8dtBdQys8Zm1szMmgE3UEmDRpxzzjnn0iSVBUHCSN8JkqYCvweuJvTVu13SNEK/vp2AvwNnJvvaAMvKeLyngZqSDi3l2PVAZlKlU4Cnih0fiRcEnXPOOZdFW/gnV9LaNPwSUNoCiAcW2/6MpDYv8dvk/rOA/bIezwjNxxmvZR0bxfpr/hrFmNl0YN+KZnfOOeecqypSWRB0zjnnnEs97yPonHPOOeeqKq8RdM4555zbBNoKqgS9RtA555xzbhvlNYLOOeecc5vA5xF0zjnnnHNVltcIOuecc85tgq2gQtBrBJ1zzjnntlVeI+icc845tym2gipBrxF0zjnnnNtGeUHQOeecc24TaAv+V6HzS30lfSJphqTLNuXf4E3DzjnnnHMbScSdPkZSNeBOoBcwG5goaZSZfbQxj+M1gs4555xzVc8BwAwz+8LMVgGPAsdv7IN4jWAlmDJl8oIa2+vLSnq4BsCCSnqsypC2POCZKsozlS9tecAzVZRnKl/a8kDlZtqrkh5nk0yZMvmlGturwRY8xU6SJmVt32Nm92RtNwa+ztqeDXTb2JN4QbASmNlulfVYkiaZWdfKerzNlbY84JkqyjOVL215wDNVlGcqX9ryQDozbSoz6xs7Q2XwpmHnnHPOuapnDrBn1naTZN9G8YKgc84551zVMxHYR1JzSTsAJwOjNvZBvGk4fe4p/yY5lbY84JkqyjOVL215wDNVlGcqX9ryQDozVUlmtkbS/wAvAdWAf5vZhxv7ODKzSg/nnHPOOefSz5uGnXPOOee2UV4QdM4555zbRnlB0DnnnHNuG+UFQVdhkvJjZ3BVm6Q9JV0aO4crn6S2pezrESGKqyBJ1SQ9HDuHq1q8IJgyknaWNEjS87GzQPgwkHSdpBnAP1KQp6Wk30na6JFRWyDLAEm1k9+vkvSkpM4R8wyTVCdrey9Jr8TKk5VjN0k/lzQOeA3YI2KWzhv6iZSp/oZ+YmRKPC7ptwpqSLoduCFiHgAkHSzpVElnZH4i5ciX1DBr+wxJz0i6LdbfzcwKgb2SqURSKU3v4S7w6WNSIHnR9gNOBfoAI4G7IuZpBpyS/KwmLOPT1cxmRcqTBwwkXJ/9CR9GJ8fIUszvzGyEpO7AUcBNhMLyRi/xU0neBN6V9GvC0kOXAv8bI0hSQD6R8DdrBTwJNDezJjHyZLllA8cMODJXQbJMTs5d2vL1BrTIbZx1ugF/Ad4GagMPA4dEygKELztAS2AqUJjsNuDBCHHuJrzukXQY8GfgQqAjYYqUn0bIBPAF8JakUcCyzE4z+79IedL8Hu7wgmBUknoTClu9gVcJb2b5ZnZWxEzjgV0Ii1efZGafSZoZoxAoaSjh+jQGHgfOAZ4xs2tznaUMmQ+ifoQ1IJ+X9MdYYczs7uRb9quEtTw7mdm8SHG+BSYAVwFvmplJ+kmkLOuY2RGxMxRnZs1jZyjDamA5UAPYCZhpZmvjRqIr0NbSMe9ZNTP7Lvl9IOE9YCQwUtLUiLk+T362IxTgo6kC7+EOLwjG9iIwDuhuZjMBJN0aNxLfEF60ewC7AZ8RvnHHcAcwHjjVzCYBSErDB0DGHEl3A72Av0jakYjdLSQNAn4HnAG0B/4j6SwzmxYhzuWEb/x/B4ZLeixChjJJ2gn4OdCd8PweB9xlZisiZNlgk7SZTclVlmImAs8A+UAD4C5JJ5nZgEh5AD4AGgJzI2bIqCapupmtAXoCQ7OORftszRSyJNU0sx9j5Uik/T3c4RNKRyWpI+HDcgChOv9R4Goz2ytyrjqEZr1TgH2AukAfM5uQ4xy7Eq7NKYQ3/8eBwWa25wbvmCOSagJ9gfeTmtNGwP5mNjpSnqeBoWb2bbJ9AHC3mXWKkSfJ0ILwHM88l34PPGVmn8bKlOR6HFgCPJTsOhWoG6OQI+nVDRw2M4vRXI2krpkP76x9g8xsWIQszxIK7LUJTa8TgJWZ42Z2XIRMVwLHEGrfmwKdk5rvvYEHzCxKM7qkg4B7gVpm1lRSB+A8M/t5hCypfg93gRcEU0LSwYQXy0nANMKHZfSleCTtTmj2OBloGusFLKlJkuMUYGfC9bkiRpZsSf/AfczsPkm7Ed58Z8bOlSFpBzNbFTsHgKT9CAWun5nZ3pGzfGRmbcvbt61LChGHJptvmNn0SDkO39BxM3s9V1mySToQaASMNrNlyb5WhPeBKDW5kt4l9E8clfkSKOkDM9svRp6sXKl8D3deEEwdSdsROiCfbGZnx86TTdJeZvZlCnK0IlyfP0TO8XtCn6XWZtYq6RA9ImJNQBPgdoo2d15sZrNj5EkzSQ8Bd5jZO8l2N+AXZhZlBGqSITXN1Umei4EhhIE+AD8h9IO7PUaeJFNzYG7mmkiqAewRqQ/zkWY2NpMr+wugpBPN7Mmy771Fc71rZt0kvZdVEJxmZh1i5ClNWt7DXeAFwYgkNd3QcTP7KldZMiTdR9l9As3MzslhlsM2dNzM3shVltIkHcI7AVOy3nCnm1n7SHnGAI8Amaa704HTzKxXhCwzKfo8Uta2mVnLXGfKJuljoDWQeY01BT4B1hDy5fxvmKbm6iTPdOCgrJqunYHxsZ7fSYZJwMGZWu5kxoW3zCznc5xKmmJmnYv/Xtp2jnM9AfwfoX9eN+BiwqwPOR+lm/b3cBf4YJG4nqfktBFGGKSxO1AtQqbnStm3J/Arcp+ntImHjTAQYk/iXJ9sq5I+QQbrPihj2s3M7svavl/SLyNl6VpsezvgZ8AlwHu5j1NC39gBSrFfsabpVyV9FC1NeF8qzNoupPQpbnKpenZXBzNbFXHOPJXxe2nbuXQ+cCth0N8cYDTwi0hZ0v4e7vCCYFRmtn/2djJ/328JTcN/ihCJZPqDTJ4WwBVAZo6se3Oc5djsbUmHEKYjmUeYryu2x5NRw3UlDQHOBv4ZMc9CSacDw5PtU4CFMYKY2UJY19VhEOEDYSrQz8xiFm4ylpS2z8xW5zzJelMkHVisuXpSOffZku4jzEv5FKFgczw5fg8oxXxJx5nZKABJxxMGa8RgZfxe2nbOmNkC4LRY589WBd7DHd40nAqS9gGuJFTj30IYcRbtA0lSG8KLtRNhkuSHkikSYuXpSZgWxYA/mdmYWFmKk9SLMA+kgJdiZpO0F6GP4EHJrreAiyJ1MdieUDD+FWGi6z+b2Yxc5yiLpFmEGolFhL9dXcKH0zfAEDObHCFTGpurO7O+z+KbZha1NldSS8LE1nnJrtnAIDP7PEKWxcAbhOfPocnvJNvdzaxejvPczgYKoGZ2UQ7jFJHm93DnBcGoklGUVwLtgBuB4RaWCIqZaQTQhVAgfZyiTUNkTaCaiyz9CNfne+B6M3szV+feGJJ2Iat2PZfXKK0kzSYUYP7G+oLNOrE60mdI+ifwhJm9lGz3JozYvw+41cxyvjpMUpAvU4yBWklB8FBgLaEvXqw5DTN5mpvZTEm1AMxsafGBGjnMkqqRzJLOTH49BGgLZObuHAB8ZGbn5zJPkqlKvIdv67wgGJGkQuBrQl/BEgXAGN/gkpqSdZ36M7vXR7KcLXclaS3hG/80SvmmG2PusGySzgOuBVYQPihFjq9RsTwtCH2DDiRcr/HAr8zsiwhZ7mfDg46ijoiX9H4pXTOmm1l7SVPNrGOETKWtTxutuVrS1YRCxEjCc/sEwqj4aKvnlDYIQ9JkM+sSK1NpJB1iZm9FOvc7hBrJNcn29sA4MzswQpZUv4e7wPsIxpWq6WEAzKxZ7AxZUrccWDGXEDr4x+qjVNwjwJ2EaT4gzP04nAhrH5vZ4FyfcyPNlfRbwiTuEOY3+0ZSNUKhPoYplNJcLSlWc/VpQIesqVr+TOjnmfOCYNJdpR1QR9KJWYd2ISx/l3PJc+VnhEEZL5rZB5L6E/pV1yB0rYmhHuG6ZFomaiX7Ykj7e7jDC4JRmdkDZR0rb2qZXEvmfbrUzIbk6pwbalpJOh3H9jkQewmnbDWt6KoPD0kqbdReTiQflPUyBeVkdOdgQi3lvrFyJU4lrHLyNKGm4q1kX+bDPYYxlN1c/XdyX6AvIBSyMvMY7kgYhRpDa6A/oXCcPQBhCWGuwxjuJRTcJwC3SSogjJa/zMyejpQJwsC+9xRWrBFhsN81MYJUgfdwhzcNR6ewHFBjwqz930pqD1wGHGoRVvFIzn8zoTP204Qapsx8VLeY2V9zmGWD37gt4tJpSb5OJCMrKbrcVU6b9LOaFH9LqE16lFC4GUgoiF2eyzxJppOBu4FlhPWqrwf+TVi/9rrYfc3KI+l2M8vpqMa0NFdnDTpoSlhneEyy3QuYYGYnbuDuWzrbQWY2Ptb5s0n6AGhvZmuTycDnAS0zI+ZjktSQ9V8c3jWzeZFypPo93AVeEIxI0k2Eb7lTgb2Bl4BzgRsIa8TmfEUBheWJ/kHoX9aX8IJ9gLAGck7zJP3MMt+4uxFqKNLwjRsASRMII2LfJ6s5cUM1vVsoR2by5tLmLovSZzH5kDzBzGYkAw7GAz81s2dznWVTxJgQWNJo4BWKNlf3IrwOJ+YqT9agg1Ll+vkNIOk3ZnZjWSNjI/WnTs0k0sVJqkdY23tds3mMyZvT/h7uAi8IRpRMFtvZzFYkL9yvCX3OZkXMVKTmQdIXEQc/pPYbN4CylnByRZXyIRl9rdONEakg2IDQXJ2ZruUt4A+EEZdNY06/I6lzzFpcScea2bNlFVIjFU5/BDJ/EwEtk+3MoLFYKwydS1hNpAmhkuFAwoowR0bIkur3cBd4H8G4VmRq2cxskaTPYhYCEzslTZ6Z2qWV2ds5/jBYZWZrk/OuSAqlaXoDeUHSUOBZijYNR58+RtI9ZjY0YoTdJf06a7tu9raZ/V+ETKmW9KUsqzl6Rozm6iz/AqLVdmXVJL8ZY87AMpYg4cwAAA/kSURBVMTu51qWiwlN+u+Y2RHJQJsoCxSQ/vdwh9cIRpU1IWnGYdnbMYbWJx2My2K5/FaZ1m/cGUmTbHHRpo/JFruZStLvN3TczK7NVZZNkcba3ph/07RcD0mvE2q6JgLjCH2r34+bKl0kTTSzfIW10LuZ2UpJH5pZuwhZUv0e7gKvEYzr+GLbt0RJkSX5BrkdYbH5KPNgZdmX8IZxI0XXrMzsi8rMmsfOsAHfxjx52gt6FXBr7AApk4q/p5kdnow+zwd6AM9LqmVmpc3BuEVJWkLpc2VmCjm75DhSxmxJdQmD/cZIWgTkfDLyRFprTV0WrxGMLBlV9aCZpWJtyIy01ABAmZPITo/9bVLSZMIUEo+Y2eKYWdImjZ37s2WmQwL2ouiqMDnvR1VRkWsEWxKm1zk5Rs1SVo7uhJVODiVMJTOVMFny8A3ecRulsPpJHcKI3VWx87h08hrByMysUNJeknZI2Qv1FUknAU9apG8Lki4Afg60kDQ961BtQkf62AYCZwGTJE0iTCUzOsb1SjrRX0yYbw3gY+A2M3sw11myzg8wiQ2sfxrRCOAu4J+UsqpPSpU2KnzLnUzKIzzHTwX2J8xmcHIuM5TiNWAyIct/UvaemRpJgXkfM7tP0m6E6VtyvgxfVp4DCeug7wvsQJivc1nEWlOXxWsEU0DSg4QXyCjCvGtA3A71SbPHzoT1YlcQoblDUh3CjPg3EOZWzFiShgEZGUlTen/CtDuFrF+vNicZk0LgL4FfE1anEKFj/03A36zoJNM5JSmfMAVRM9Z/8YzeN0gpXJasPJIGm9n9OTjPUOAUQuHh8eTnmTR0hUiaPA8h9KfOJ0zbNN7Mfhc1WIok/XO7Aq3NrFVSoB9hZtEmcE6+KJ9M+ALWFTgDaGUR5jh1JXlBMAXK6lif5n5WktqZ2Yexc8SWTMB9FnAMYR7IhwnTfwyy3E0A/A6hyW5Wsf3NgEctwhqjWRk+ITTBFp9rMVafJQAkXUPoR/kUKRnxnZbmakmrCPM+/q+ZTUr2RZtGqjhJ+wKHE5qHDwa+MrPD46ZKj2SQSCdgSqZ7T+yuNJImmVnX7Bxp6n60rfOm4RQor8AXedqIsgwj4nQSaZD0EVxM6Cd4mZllChTvKrfLJ+1S2rRDZjZLUuyml/lmNipyhtJk5qPLHoRkQMzCTlqaqxsBA4BbkhUqHge2j5hnHUlfAP8ljBj+B3CWNw+XsMrMTJIBSNo5diDgx2SQz1RJNwJzge0iZ3IJrxGsAmJPBVIa/zYHklqY2RcpyFFmM2fsJlBJPQnNjK9QtObtyViZ0ir236o0kpoQ+gmeQugq8pSZXRExz3aZeelc6SRdQlhVpBehW83ZhAFtt0fMtBfwDaF/4K8IA1j+bhEnSXfreUGwCkhpQTB1mXJNUiGhH97lmQEikVakyJ6rq8ghoIWZRasRkPQQ0Ab4kPVNw2ZmZ8fKBCBpe+ACQl8zCIMQ7jaz1REzXUPKmquzSdoHOMXM/hAxw43AH4HlwItAe+BXZvZQrExpJKkX0JvwHvCSmY2JnGdnYHmmEJ/MlrGjmf0YM5cLvGnYuU33IaF5Y7SkgckHdk5HdiY6AHsQlijMtidhSaeY8s2sdfk3y7l/EJo7/55sD0r2nRstUUqaqyWdTqgkKD7I6EBK/8KRS73N7DeSfgLMAk4kTMLvBcEsScEvauGvmFeAo4ClyXYNYDShj6eLzAuCVUOMwkV5vF8OrEk+lAYC4ySdQZypUv5KqJUsMgAj6R/4V+DYCJky3pbU1sw+ipihNPlm1iFre6ykadHSkKoJyi8Eepay/0lCoeuR3MYpItNXsR9hJOz3UhrfHnMvxRNcA+xkZplCIGa2VFLNiHlcFi8IpoykesDiYnPR5WyVg6Qvx2Iz+z7ZPgI4gTAz/R2ZjtkxR6KmSGb95cckfUj4gGwaIccepS2zZWbvJyOHYzqQ0EF8JqG5My1LSxVKamnJurWSWhB5PsEUNVdvn/2hnWFmy5KMMT0r6b+EpuELkjnyVkTOlApmVjt2hg1YJqmzJWvVS+pK+Bu6FPA+ghFJuhp43Mz+K2lHQp+XDoS5+041s5cjZHoX+ImZFUjqCLxM6HDcHlhtZjGbzlJFUhczm5y1XQc4PteTOEv6zMz2KePYDDPbO5d5ip1/r9L2p2D6mJ6E+R6/IBRO9yKMQN3QWttbOtO/CDVeDyS7BgGFuX7NSfoY6Gpmy4rtrw1MNLM2ucxTnKT6wPfJZPw1CaPmY3eBcBuQFPweAwqSXY2Agdnvny4erxGMayBwXfJ7pn/QbkArwodBzguCQA0zy7xYTwf+bWa3JJMmT42QJ82mS7qI9TU4rxOm/8i1SZKGmNk/s3dKOpewCkM0sQt8pUmey8sJIysz/Rc/yZr+J5a0NFffCzwh6fzM3y+pWb4zORZNUiN5OnBY0iQc6zXnNk5zwtyGTQn9OruRzhWHtkleEIxrVVYTcB/C5L+FwMeSYv1tsjvcHAlcDmBma70vTglpGXDwS+ApSaexvuDXlTBVw09ynCX1kufyncn0R9PLvUPupKK52sxulrQUeENSrWT3UuDPZvaPXOcpJi2vObdxfmdmI5KVYY4Abib83brFjeXAC4KxrZS0H2F+pSOAS7KOxepIO1bS44QJP+sBYwEkNcIHiBSXihocM/sGODjpz7lfsvt5Mxub6yxVSPS1tEtxKfBqMmnyuubqGEHM7C7grqQ5GDNbAiBpj+T5FksqXnNuo2W+0PQD/mlmz0v6Y8xAbj0vCMb1S+AJQnPwX81sJoCkY4D3ImYaSOjD0T2ro3pD4MpImdIqFTU4GUn/tmh93KqY8whrM6+RFGUt7Wxpba42syWS6ko6BziVsCZ6XsRIqXrNuQqbI+luwiTXf0n6xPvKIinhg0VSRFJ34ADgAzMbHTFHdTNbk/xeizAh8Bdpmdg2LdI44MBVXWlarUdSDeB4QuGvE1CbMHvAGzFX9vDXXNWUDOrpC7xvZp8lLUz7x/ycc+t5QTAiSRPM7IDk9yHALwirCvQGnjWzP0fINBi4BVgIXEzoID6TMIDlN2Y2PNeZ0iz5ZpuaGhxXMZJeMbOe5e3LcaabgfFEbq6W9AhwKGHC30cJ3UNmpGWeQ3/NOVe5vCAYUXYNgKSJwDFmNj9ZjucdM9s/Qqb3Cf0VawPTgE5m9rmkPYAxKZj/LTUk7QT8HOhOGAE3DrjLzHxes5RK/mY1CU3oPVg/OGoX4MWYU6MkEwLvTJg+KlpztaSphGa7BwkD2GZL+sLMcrrCSWn8Nedc5fM+gnFtl0wgvR2hUD4f1k3cuiZSpkIzWwAskLQ00xfHzL7xUcMlPAgsATKLuZ8KDAMGREvkynMeoR9sHmGEdeZJ/QNwR6xQkJ4Jgc2so6Q2wCnAy5IWALVTMFAE/DXnXKXzGsGIJM0C1pJ88wcOMbO5Sb+8N82sY4RMowhr6NYG2hIGrTxJWCfyYDPrk+tMaSXpIzNrW94+lz6SLjSz28u/Ze6ksbk6ydCFUOAaAMw2s2jrw/przrnK5zWCEZlZszIOrSXe/G+nE/oqfg9cRpjf8HLCEnODI2VKqymSDjSzdwAkdQMmRc7kKsDMbpd0MNCMrPfBXK8KA0WaqxskLQTZzdWNc52nuGT1h8mSLiH0HYzJX3POVTKvEXRuIyX9KI0wsW1r4Ktkey/gv147kX6ShgEtCavlZKYfMTO7KEKWi1nfXD2Hos3V/zSznDZZS7qdDaz6EOka+WvOuS3EC4KuwiQNNbN7YueIraz1czPSuKyaKypZT7dtiiaTTk1ztaQzszavBX6ffdzMHiDH/DXn3JbjTcNuY/hokWCRmf0gqX7sIG6TfUCYJH1u7CAZaWmuzi7oSfpljIJfKfw159wW4gVBt0HFJrm+O3aelHhE0rHAAmAWRQvIBkSfZsOVqwHwkaQJwLp56MzsuFiBymquJoyUjSUtNab+mnNuC/GmYVdEGie5TitJH5jZfuXf0qWNpMNL229mr+c6S0ZKm6unmFnn2Dky/DXnXOXzGkFX3PZZvw8FeiWTXN8MvAN4QXC9yZLyzWxi7CBu48Qs8G1AKpqrk4mtM4XRmpJ+yBwi4nrMCX/NOVfJvCDoikvjJNdp1Q04TdKXwDLWf1D66ispVayQU+QQ8Qs5qWiuTsvE1mXw15xzlcwLgq64OqxfccEkNcqa5NoHixTlk2tXMSkv5FwTO0AV4K855yqZ9xF0FSKpJrCHmc2MncU555xzlcMLgs45F1HKm6udc1s5Lwg655xzzm2jtosdwDnnnHPOxeEFQeecc865bZQXBJ1zOSOpUNJUSR9IGpEMQtrUx+oh6bnk9+MkXbaB29aV9PNNOMc1ki6p6P5it7lf0k834lzNJH2wsRmdc25zeEHQOZdLy82sY7I6xCrg/OyDCjb6fcnMRpWz6k1dYKMLgs45t7XzgqBzLpZxwN5JTdgnkh4krK6xp6TeksZLmpLUHNYCkNRX0n8lTQFOzDyQpMGS7kh+30PSU5KmJT8HE1bEaZnURt6U3O5SSRMlTZd0bdZjXSnpU0lvAq3L+0dIGpI8zjRJI4vVch4laVLyeP2T21eTdFPWuc/b3AvpnHObyguCzrmck1QdOBp4P9m1D/B3M2tHWDHiKuCoZJ3bScCvJe0E/BM4FuhCWI6tNLcBr5tZB6Az8CFwGfB5Uht5qaTeyTkPADoCXSQdJqkLcHKy7xggvwL/nCfNLD8538fAOVnHmiXn6AfclfwbzgG+N7P85PGHSGpegfM451yl85VFnHO5VEPS1OT3ccC9QB7wpZm9k+w/EGgLvCUJYAdgPNAGmGlmnwFIeoiwHnZxRwJnAJhZIfB9smxitt7Jz3vJdi1CwbA28JSZ/ZicY1QF/k37Sfojofm5FvBS1rHHzWwt8JmkL5J/Q2+gfVb/wTrJuT+twLmcc65SeUHQOZdLy82sY/aOpLC3LHsXMMbMTil2uyL320wCbjCzu4ud45eb8Fj3AyeY2TRJg4EeWceKT9RqybkvNLPsAiOSmm3CuZ1zbrN407BzLm3eAQ6RtDeApJ0ltQL+CzST1DK53Sll3P8V4ILkvtUk1QGWEGr7Ml4Czs7qe9hY0u7AG8AJkmpIqk1ohi5PbWCupO2B04odGyBpuyRzC+CT5NwXJLdHUitJO1fgPM45V+m8RtA5lypmNj+pWRsuacdk91Vm9qmkocDzkn4kNC3XLuUhLgbukXQOUAhcYGbjJb2VTM/yQtJPcF9gfFIjuRQ43cymSHoMmAZ8C0ysQOTfAe8C85P/Z2f6CpgA7AKcb2YrJP2L0HdwisLJ5wMnVOzqOOdc5fIl5pxzzjnntlHeNOycc845t43ygqBzzjnn3DbKC4LOOeecc9soLwg655xzzm2jvCDonHPOObeN8oKgc84559w2yguCzjnnnHPbqP8HCaDBqa5+iCgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=False,\n",
    "                        title='Unnormalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense4(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 4))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 3, inputs1)\n",
    "    x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 4)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    x1 = dense_block_1(x1, 256, 5)\n",
    "    \n",
    "       \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    xx1 = Conv1Dme(64, 3, inputs2)\n",
    "    xx1=MaxPooling1D(pool_size=2, strides=2)(xx1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    xx1 = dense_block_1(xx1, 128, 4)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    xx1 = dense_block_1(xx1, 256, 5)\n",
    "    \n",
    "    ###############################################################\n",
    "    #           Concatenating\n",
    "    \n",
    "    xxx=keras.layers.concatenate([x1,xx1],axis=-1)\n",
    "    \n",
    "    xxx = dense_block_1(xxx, 256, 4)\n",
    "        \n",
    "    xf=Flatten()(xxx)\n",
    "\n",
    "    xf=Dense(256,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    \n",
    "    xf=Dense(64,)(xf)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "\n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9111538461538462, 0.9111538461538461, 0.917425762586435, 0.9119182995867678, 0.9041295532739397)\n",
      "(0.9292307692307692, 0.9292307692307691, 0.9348943475079114, 0.9301434954599762, 0.9237012587927157)\n",
      "(0.9180769230769231, 0.9180769230769231, 0.9211819878525715, 0.9183215776156375, 0.9114607996864601)\n",
      "(0.9161538461538462, 0.9161538461538462, 0.9201616456964785, 0.915932902556817, 0.9095633752858903)\n",
      "(0.9146153846153846, 0.9146153846153845, 0.9198762952148231, 0.9138403874174983, 0.9080413481908711)\n",
      "(0.9142307692307692, 0.9142307692307692, 0.924091598176918, 0.9156750783355933, 0.9078458658307987)\n",
      "(0.9211538461538461, 0.921153846153846, 0.9281043117258374, 0.9227985643128539, 0.9149741870404311)\n",
      "(0.9134615384615384, 0.9134615384615384, 0.9182809759082827, 0.9137408333424049, 0.9066362744688605)\n",
      "(0.9173076923076923, 0.9173076923076924, 0.9207077219943779, 0.9166390723171394, 0.9108539523365007)\n",
      "(0.9253846153846154, 0.9253846153846155, 0.92921491892905, 0.9246333633934172, 0.919650199656651)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    X_train_enac, X_test_enac = get_enac(i)\n",
    "    model = model_dense4()\n",
    "    #es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    #history = model.fit([X_train[:,:,0:4],X_train_enac], Y_train, validation_data=([X_test[:,:,0:4],X_test_enac], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.load_weights(\"Checkpoints/Dense_ENAC_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],enac_val])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9180769230769231\n",
      "0.9180769230769231\n",
      "0.9233939565592685\n",
      "0.9183643574338104\n",
      "0.9116856814563119\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9196153846153846, 0.9196153846153847, 0.9228937999547887, 0.9196492543422955, 0.9131875899047388)\n"
     ]
    }
   ],
   "source": [
    "#auc_mat_750c1 = []\n",
    "#history_750c1 = []\n",
    "#conf_mat_750c1 = []\n",
    "#history_750c1 = {}\n",
    "#class_report_750c1 = {}\n",
    "for i in range(1):\n",
    "    i=5\n",
    "    X_train, Y_train, X_test, Y_test = get_file8(i)\n",
    "    X_train_enac, X_test_enac = get_enac(i)\n",
    "    model = model_dense4()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],X_train_enac], Y_train, validation_data=([X_test[:,:,0:4],X_test_enac], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.save_weights(\"Checkpoints/Dense_ENAC_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],enac_val])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    #auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_750c1['fold%i'%i]=history.history\n",
    "    #conf_mat_750c1.append([conf_mat])\n",
    "    #class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 750, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enac_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "t27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
