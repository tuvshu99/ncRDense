{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers import  Dense, Flatten, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, merge, Input,Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import  Bidirectional\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Test_Data_750.h5',\n",
       " '/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot_Fold_0_Train_Data_750.h5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files8 = glob.glob(\"/home/chenming/ncrna/ncRDeep/Data_Processing/Ten_Fold_Onehot_Data_h5/Onehot*.h5\")\n",
    "my_files8.sort()\n",
    "my_files8[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_test_0.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/SS_test_1.h5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files7 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/RNAfold/*.h5\")\n",
    "my_files7.sort()\n",
    "my_files7[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath2 = '/home/chenming/ncrna/ncRDeep2/Data_Processing/NCP/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenming/ncrna/ncRDeep2/Data_Processing/NCP/Test_0_NCP.h5',\n",
       " '/home/chenming/ncrna/ncRDeep2/Data_Processing/NCP/Test_1_NCP.h5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files6 = glob.glob(\"/home/chenming/ncrna/ncRDeep2/Data_Processing/NCP/T*.h5\")\n",
    "my_files6.sort()\n",
    "my_files6[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file8(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files8[fold_no*2+1],'r')\n",
    "    hf_Test = h5.File(my_files8[fold_no*2],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file7(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files7[10+fold_no],'r')\n",
    "    hf_Test = h5.File(my_files7[fold_no],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file6(fold_no): #get train and test data from file by fold number\n",
    "    hf_Train = h5.File(my_files6[10+fold_no],'r')\n",
    "    hf_Test = h5.File(my_files6[fold_no],'r')\n",
    "    X_train = hf_Train['Train_Data'] # Get train set\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = hf_Train['Label']      # Get train label\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = hf_Test['Test_Data']     # Get test set\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = hf_Test['Label']       # Get test label\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_train = np_utils.to_categorical(Y_train, 13)  # Process the label of tain\n",
    "    Y_test = np_utils.to_categorical(Y_test, 13)    #  Process the label of test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_val = h5.File(my_files7[20],'r')\n",
    "X_val = hf_val['Test_Data']     # Get test set\n",
    "X_val = np.array(X_val)\n",
    "Y_val = hf_val['Label']       # Get test label\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np_utils.to_categorical(Y_val, 13)    #  Process the label of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 750, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_files6[20]\n",
    "hf_val2 = h5.File(my_files6[20],'r')\n",
    "X_val2 = hf_val2['Test_Data']     # Get test set\n",
    "X_val2 = np.array(X_val2)\n",
    "Y_val2 = hf_val2['Label']       # Get test label\n",
    "Y_val2 = np.array(Y_val2)\n",
    "Y_val2 = np_utils.to_categorical(Y_val2, 13)    #  Process the label of test\n",
    "X_val2[:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5S_rRNA',\n",
       " '5.8S_rRNA',\n",
       " 'tRNA',\n",
       " 'ribozymes',\n",
       " 'CD-box',\n",
       " 'miRNA',\n",
       " 'Intron_gpI',\n",
       " 'Intron_gpII',\n",
       " 'HACA-box',\n",
       " 'riboswitch',\n",
       " 'IRES',\n",
       " 'leader',\n",
       " 'scaRNA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['5S_rRNA', '5.8S_rRNA', 'tRNA', 'ribozymes', 'CD-box', 'miRNA', 'Intron_gpI', 'Intron_gpII', 'HACA-box', 'riboswitch', 'IRES', 'leader', 'scaRNA']\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    FONT_SIZE = 10\n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\\n============================\")\n",
    "    else:\n",
    "        #cm = np.asfarray(cm,float64)\n",
    "        print('Confusion matrix, without normalization\\n============================')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=(5*2, 4*2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=FONT_SIZE)\n",
    "    plt.yticks(tick_marks, classes, fontsize=FONT_SIZE)\n",
    "    fmt = '.3f' if normalize else '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    fontsize=FONT_SIZE,\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=FONT_SIZE)\n",
    "    plt.xlabel('Predicted label', fontsize=FONT_SIZE)\n",
    "    plt.savefig('Conf_mat_avg.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1Dme(f, k, x):\n",
    "    x1=Conv1D(filters=f,kernel_size=k,strides=1,padding=\"same\",kernel_initializer=initializers.random_uniform()) (x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1=Dropout(0.2)(x1)\n",
    "    x1=Activation('relu')(x1)\n",
    "    #x1=MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block_1(xin, f, k, p):\n",
    "    f1 = f\n",
    "    k1 = k\n",
    "    p1 = p\n",
    "\n",
    "    x1 = Conv1Dme(f1, k1, xin)\n",
    "    x11 = Conv1Dme(f1, k1, x1)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x11 = Concatenate(axis=-1)([x1,x11])\n",
    "    x11 = Conv1Dme(f1, k1, x11)\n",
    "    x1=MaxPooling1D(pool_size=p1, strides=p1)(x11)\n",
    "    \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block_1(xin, n):\n",
    "    xf=Dense(n,)(xin)\n",
    "    xf = BatchNormalization()(xf)\n",
    "    xf=Dropout(0.2)(xf)\n",
    "    xf=Activation('relu')(xf)\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dense5(): # Model\n",
    "    inputs1 = Input(shape=(750, 4))\n",
    "    inputs2 = Input(shape=(750, 3))\n",
    "    inputs3 = Input(shape=(750, 3))\n",
    "    \n",
    "    ###############################################################\n",
    "    #          1st dense block\n",
    "    \n",
    "    x1 = Conv1Dme(64, 5, inputs1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    x1 = dense_block_1(x1, 128, 5, 4)\n",
    "    x1 = dense_block_1(x1, 128, 5, 4)\n",
    "    x1 = Conv1Dme(64, 5, x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, strides=2)(x1)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #          2nd dense block\n",
    "    \n",
    "    x2 = Conv1Dme(64, 5, inputs2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    x2 = dense_block_1(x2, 128, 5, 4)\n",
    "    x2 = dense_block_1(x2, 128, 5, 4)\n",
    "    x2 = Conv1Dme(64, 5, x2)\n",
    "    x2 = MaxPooling1D(pool_size=2, strides=2)(x2)\n",
    "    \n",
    "    ###############################################################\n",
    "    #          3rd dense block\n",
    "    \n",
    "    x3 = Conv1Dme(64, 5, inputs3)\n",
    "    x3 = MaxPooling1D(pool_size=2, strides=2)(x3)\n",
    "    \n",
    "    x3 = dense_block_1(x3, 128, 5, 4)\n",
    "    x3 = dense_block_1(x3, 128, 5, 4)\n",
    "    x3 = Conv1Dme(64, 5, x3)\n",
    "    x3 = MaxPooling1D(pool_size=2, strides=2)(x3)\n",
    "    \n",
    "    xf=keras.layers.concatenate([x1,x2,x3],axis=-1)\n",
    "    \n",
    "    #xf = dense_block_1(xf, 256, 5, 2)\n",
    "    xf = Conv1Dme(64, 3, xf)\n",
    "    xf = MaxPooling1D(pool_size=2, strides=2)(xf)\n",
    "    xf = Flatten()(xf)\n",
    "\n",
    "    xf=fc_block_1(xf, 256)\n",
    "    xf=fc_block_1(xf, 64)\n",
    "    \n",
    "    xf=Dense(13, activation='softmax',  )(xf)\n",
    "\n",
    "    model = Model(inputs=[inputs1,inputs2,inputs3], outputs=xf)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.0005),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9476923076923077, 0.9476923076923077, 0.950298609671978, 0.9481314819296522, 0.9435073841626063)\n",
      "(0.9519230769230769, 0.9519230769230768, 0.9536131478711124, 0.9520318600664162, 0.9480417131073544)\n",
      "(0.9565384615384616, 0.9565384615384617, 0.9591452157338904, 0.9569680033423823, 0.953071553379659)\n",
      "(0.948076923076923, 0.9480769230769232, 0.9512847198846373, 0.9486627798460171, 0.9439276082176091)\n",
      "(0.948076923076923, 0.9480769230769233, 0.9513641625232228, 0.9484999379964905, 0.9439692260025901)\n",
      "(0.9588461538461538, 0.9588461538461537, 0.9604684133914501, 0.9588763205074358, 0.9555546501451645)\n",
      "(0.9569230769230769, 0.9569230769230769, 0.9581451302458852, 0.9571992566495184, 0.9533921582219641)\n",
      "(0.9484615384615385, 0.9484615384615385, 0.9504120582981458, 0.9486389720062459, 0.9443128648792751)\n",
      "(0.9511538461538461, 0.951153846153846, 0.9537899246348182, 0.9518307465597542, 0.9472131286404301)\n",
      "(0.9515384615384616, 0.9515384615384614, 0.9535873383469459, 0.9520633709025711, 0.9475867142262607)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in range(10):\n",
    "    X_train, Y_train, X_test, Y_test = get_file7(i)\n",
    "    X_train2, Y_train2, X_test2, Y_test2 = get_file6(i)\n",
    "    model = model_dense5()\n",
    "    #es = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "    #history = model.fit([X_train[:,:,0:4],X_train[:,:,4:7],X_train2[:,:,4:7]], Y_train, validation_data=([X_test[:,:,0:4],X_test[:,:,4:7],X_test2[:,:,4:7]], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.load_weights(\"Checkpoints/Dense_rnafoldncp_revise2_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],X_val[:,:,4:7],X_val2[:,:,4:7]])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    #history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val[:,:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.948076923076923, 0.9480769230769232, 0.9512847198846373, 0.9486627798460171, 0.9439276082176091)\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = []\n",
    "history_750c1 = []\n",
    "conf_mat_750c1 = []\n",
    "history_750c1 = {}\n",
    "class_report_750c1 = {}\n",
    "for i in [3]:\n",
    "    X_train, Y_train, X_test, Y_test = get_file7(i)\n",
    "    X_train2, Y_train2, X_test2, Y_test2 = get_file6(i)\n",
    "    model = model_dense5()\n",
    "    es = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:,:,0:4],X_train[:,:,4:7],X_train2[:,:,4:7]], Y_train, validation_data=([X_test[:,:,0:4],X_test[:,:,4:7],X_test2[:,:,4:7]], Y_test), epochs=500, verbose=0, callbacks=[es])\n",
    "    model.save_weights(\"Checkpoints/Dense_rnafoldncp_revise2_%i_fold.h5\" %i)\n",
    "    y = model.predict([X_val[:,:,0:4],X_val[:,:,4:7],X_val2[:,:,4:7]])\n",
    "    y_test_non_category = [ np.argmax(t) for t in Y_val ]\n",
    "    y_predict_non_category = [ np.argmax(t) for t in y ]\n",
    "    auc = accuracy_score(y_test_non_category, y_predict_non_category)\n",
    "    precision,recall,fscore,support=score(y_test_non_category, y_predict_non_category,average='macro')\n",
    "    mcc = matthews_corrcoef(y_test_non_category, y_predict_non_category)\n",
    "    conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n",
    "    classification_reports = classification_report(y_test_non_category, y_predict_non_category)\n",
    "    print(auc,recall,precision,fscore,mcc)\n",
    "    auc_mat_750c1.append([[auc],[recall],[precision],[fscore],[mcc]])\n",
    "    #history_5.append([history.history])\n",
    "    history_750c1['fold%i'%i]=history.history\n",
    "    conf_mat_750c1.append([conf_mat])\n",
    "    class_report_750c1['fold%i'%i]=classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9519230769230769\n",
      "0.951923076923077\n",
      "0.9542108720602087\n",
      "0.9522902729806484\n",
      "0.9480577000982914\n"
     ]
    }
   ],
   "source": [
    "auc_mat_750c1 = np.array(auc_mat_750c1)\n",
    "for i in range(5):    \n",
    "    print(np.average(auc_mat_750c1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "t27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
